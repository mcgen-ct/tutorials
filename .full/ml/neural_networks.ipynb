{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Computer Modern Roman\",\n",
    "    }\n",
    ")\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Neural_networks"
   },
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary: In this tutorial we will build and train a neural network***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be understood fundamentally as powerful tools for function approximation. They are structured to learn a mapping from inputs $x$ to outputs $y$, based on observed data. Whether the task is image classification, speech recognition, or solving differential equations, the central idea is the same: approximate an unknown function $f:\\mathbb{R}^n \\to \\mathbb{R}^m$ that relates inputs to desired outputs.\n",
    "\n",
    "A neural network achieves this by composing many simple parameterized functions (typically linear combinations of the input followed by nonlinear activations). With sufficient complexity (depth and width), neural networks are universal approximators: they can approximate any continuous function on a compact domain to arbitrary accuracy, assuming proper training. Neural networks then serve as flexible surrogates, learning from examples rather than requiring explicit modeling of the underlying system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "The_perceptron"
   },
   "source": [
    "### The perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will focus primarily neural networks built from ensembles of a basic atomic unit known as the ***perceptron***. A perceptron takes a vector of real-values inputs, calculates a linear combination of these inputs, and then outputs $1$ if the results is greater than some arbitrary threshold and $0$ otherwise. More precisely, given inputs $x_1, \\ldots, x_n$, the output of the perceptron $\\sigma(x_1, \\ldots, x_n)$ is:\n",
    "$$\n",
    "\\sigma(x_1, \\ldots, x_n; w_1, \\ldots , w_n, b) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } \\sum_i w_i x_i + b > 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $w_i$ are real-values constants or ***weights***, and $b$ represents an arbitrary threshold or ***bias*** which the sum of weighted inputs must surpass in order to return $1$. More compactly, defining $x_0 \\equiv b, w_0 = 1$ the perceptron can be written simply as\n",
    "$$\n",
    "\\sigma(x_1, \\ldots, x_n; w_1, \\ldots , w_n, b) = \\Theta \\left(\\sum_{i = 0}^n x_i w_i \\right)\n",
    "$$\n",
    "where $\\Theta$ is the heaviside theta function.\n",
    "\n",
    "We'll start by constructing a single perceptron with three inputs using our `Scalar` class built in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from engine import Scalar"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def perceptron(x1, x2, x3, w1, w2, w3, b):\n",
    "    \"\"\"\n",
    "    Assuming an input of three-scalar values\n",
    "    \"\"\"\n",
    "    # Build the computation graph\n",
    "    x1w1 = x1 * w1\n",
    "    x1w1.label = \"x_1*w_1\"\n",
    "    x2w2 = x2 * w2\n",
    "    x2w2.label = \"x_2*w_2\"\n",
    "    x3w3 = x3 * w3\n",
    "    x3w3.label = \"x_3*w_3\"\n",
    "\n",
    "    # Weighted sum of inputs\n",
    "    x1w1px2w2 = x1w1 + x2w2\n",
    "    x1w1px2w2.label = \"sum_i=1^2 (x_i*w_i)\"\n",
    "    x1w1px2w2px3w3 = x1w1px2w2 + x3w3\n",
    "    x1w1px2w2px3w3.label = \"sum_i=1^3 (w_i*x_i)\"\n",
    "\n",
    "    # Adding the bias to the weighted sum of inputs\n",
    "    x1w1px2w2px3w3pb = x1w1px2w2px3w3 + b\n",
    "    x1w1px2w2px3w3pb.label = \"sigma\"\n",
    "\n",
    "    # Activation function\n",
    "    sigma = x1w1px2w2px3w3pb.ifel(x1w1px2w2px3w3pb.data > 0.0, 1.0, 0.0)\n",
    "    sigma.label = \"Theta(sigma)\"\n",
    "\n",
    "    # Return the output of the perceptron\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Weights for each input\n",
    "w1 = Scalar(0.2, label=\"w_1\")\n",
    "w2 = Scalar(-0.5, label=\"w_2\")\n",
    "w3 = Scalar(-0.8, label=\"w_3\")\n",
    "\n",
    "x1 = Scalar(3.0, label=\"x_1\")\n",
    "x2 = Scalar(2.0, label=\"x_2\")\n",
    "x3 = Scalar(5.0, label=\"x_3\")\n",
    "\n",
    "b = Scalar(0.1, label=\"b\")\n",
    "\n",
    "# Construct the perceptron\n",
    "sigma = perceptron(x1, x2, x3, w1, w2, w3, b)\n",
    "# Perform the backwards pass\n",
    "sigma.backward()\n",
    "# Look at the computation graph\n",
    "sigma.visualize_graph(output_label=\"Theta(sigma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Classifying_<font_color=\"red\">red</font>_vs_<font_color=\"blue\">blue</font>_with_a_single_perceptron"
   },
   "source": [
    "## Classifying <font color=\"red\">red</font> vs <font color=\"blue\">blue</font> with a single perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to get some intuition for the capabilities of a single perceptron through a simple classification task:\n",
    "\n",
    "*Given a dataset of RGB (Red, Green, Blue) values (with G = 0 for simplicity), distinguish red RGB values from blue RGB values.*\n",
    "\n",
    "First we need to construct a dataset consisting of RGB values between (255,0,0) and (0,0,255)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create RGB values interpolating between red (255,0,0) and blue (0,0,255)\n",
    "num_samples = 20\n",
    "R = np.linspace(255, 0, num_samples, dtype=int)\n",
    "G = np.zeros(num_samples, dtype=int)\n",
    "B = np.linspace(0, 255, num_samples, dtype=int)\n",
    "\n",
    "# Label: 0 for red (R > B), 1 for blue (B >= R)\n",
    "labels = (B >= R).astype(int)\n",
    "\n",
    "# Stack into dataset\n",
    "dataset = np.column_stack((R, G, B, labels))\n",
    "\n",
    "# Plot the spectrum with labels\n",
    "fig, ax = plt.subplots(figsize=(8, 2))\n",
    "for i in range(num_samples):\n",
    "    color = (R[i] / 255, G[i] / 255, B[i] / 255)\n",
    "    ax.scatter(i, 0, color=color, s=200, edgecolor=\"k\")\n",
    "    ax.text(i, -0.013, f\"{labels[i]}\", ha=\"center\", va=\"top\", fontsize=10)\n",
    "\n",
    "# Clear axis ticks and labels\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# Set title\n",
    "ax.set_title(r\"RGB data w/ labels: Red (label = 0) to Blue (label = 1)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def RB_dataset(num_samples=20, seed=None):\n",
    "    \"\"\"\n",
    "    Create a random dataset of normalized RGB values and binary (red or blue) labels.\n",
    "    \"\"\"\n",
    "    # Initialize a seed\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    # Generate random RGB values\n",
    "    R = np.random.random(num_samples)\n",
    "    G = np.zeros(num_samples)  # Green channel is always 0\n",
    "    B = np.random.random(num_samples)\n",
    "\n",
    "    # Label: 0 for red (R > B), 1 for blue (B >= R)\n",
    "    labels = (B >= R).astype(int)\n",
    "\n",
    "    # Stack into dataset\n",
    "    RGB = np.column_stack((R, G, B, labels))\n",
    "\n",
    "    # Shuffle dataset\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    RGB = RGB[indices]\n",
    "\n",
    "    # Stack into dataset\n",
    "    return RGB"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a dataset with\n",
    "num_samples = 1000\n",
    "RB = RB_dataset(num_samples, seed=42)\n",
    "\n",
    "# Get a feel for the dataset\n",
    "print(\"Dataset shape:\", RB.shape)\n",
    "print(\"RGB value:\", RB[0, 0:3], \"\\nLabel:\", RB[0, 3])\n",
    "print(\"R:\", RB[0, 0], \"G:\", RB[0, 1], \"B:\", RB[0, 2])\n",
    "print(\"First 10 elements of dataset:\\n\", RB[:10])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We can also visualize the dataset\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "# Plot each point with its actual RGB color\n",
    "for row in RB:\n",
    "    R_val, G_val, B_val, label = row\n",
    "    color = (R_val, G_val, B_val)\n",
    "    marker = \"o\" if label == 0 else \"s\"\n",
    "    ax.scatter(R_val, B_val, c=[color], marker=marker, edgecolor=\"k\", s=80)\n",
    "\n",
    "# Plot the decision boundary: R = B\n",
    "min_val = min(RB[:, 0].min(), RB[:, 2].min())\n",
    "max_val = max(RB[:, 0].max(), RB[:, 2].max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], \"k--\", label=\"R = B\")\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel(\"R (Red value)\")\n",
    "ax.set_ylabel(\"B (Blue value)\")\n",
    "ax.set_title(\"G = 0\")\n",
    "ax.legend(framealpha=1.0, edgecolor=\"black\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Loss-ology"
   },
   "source": [
    "## Loss-ology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably the most important decision when implementing machine learning algorithms is the choice of ***loss function***. Many, if not all, machine learning problems rely on the ability to be framed as an optimization problem that minimizes the loss. This should clarify and emphasize the importance of autodiff engines in machine learning. There are many loss functions, each tailored for a specific class of problem.\n",
    "\n",
    "For the task at hand, we have a single perceptron with three input values $x_1, x_2, x_3$ corresponding to RGB values and four tuneable weight parameters $w_1, w_2, w_3, b$. Based on the input, we want our perceptron to correctly predict a binary decision between red (0) and blue (1).\n",
    "\n",
    "Given a dataset of $x_i$ consisting of $N$ RGB color values, each with a label $c_i$ classifying red or blue - a naive ansatz for the loss may look something like\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^N \\left(\\sigma(x_i) - c_i\\right)^2.\n",
    "$$\n",
    "What is the value of this function defined above? We can try to understand it based in two limits:\n",
    "1. when the classifier is perfectly imperfect.\n",
    "2. when the classifier is perfect.\n",
    "\n",
    "In the former, $\\mathcal{L} = N$, and in the latter, $\\mathcal{L} = 0$. In other words, when the classifier perfectly identifies all colors in the dataset, the quantity $\\mathcal{L}$ is at it's global minimum. As we discussed above, this is exactly the optimization function that we are looking for; minimizing $\\mathcal{L}$ by varying the weights $w_i$ (via the gradient information provided by our autodiff engine) will push us towards an optimal classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Training"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a loss, all that's left is to construct an optimization algorithm that will:\n",
    "\n",
    "1. intialize the weights,\n",
    "2. reset the gradients of the weights to zero,\n",
    "3. perform the forward-pass over the dataset (keeping track of the computation graph),\n",
    "4. perform the backward-pass,\n",
    "5. update the weights to minimize the loss and finally,\n",
    "6. repeat 1-4 many times over the full dataset a fixed number of times (or ***epochs***) or until the loss plateaus.\n",
    "\n",
    "The process outlined above is typically referred to as ***training*** the neural network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def train_perceptron(X, epochs=50, lr=1e-2, record_history=True):\n",
    "    \"\"\"\n",
    "    Train a perceptron on the dataset X.\n",
    "    X is expected to be an array of shape (N, 4) where the last column is the label.\n",
    "    \"\"\"\n",
    "    # Extract the total number of samples in the dataset\n",
    "    N = X.shape[0]\n",
    "    # Randomly initialize weights and bias between -1 and 1\n",
    "    weights = [Scalar(random.uniform(-1, 1), label=f\"w{i}\") for i in range(3)]\n",
    "    bias = Scalar(random.uniform(-1, 1), label=\"b\")\n",
    "\n",
    "    # We can also record some metrics for training analysis\n",
    "    if record_history:\n",
    "        loss_history = []\n",
    "        weight_history = [[w.data for w in weights]]\n",
    "        bias_history = [bias.data]\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize total loss for the epoch\n",
    "        total_loss = Scalar(0.0)\n",
    "        for i in range(N):\n",
    "            x1, x2, x3, label = X[i]\n",
    "            prediction = perceptron(\n",
    "                Scalar(x1),\n",
    "                Scalar(x2),\n",
    "                Scalar(x3),\n",
    "                weights[0],\n",
    "                weights[1],\n",
    "                weights[2],\n",
    "                bias,\n",
    "            )\n",
    "            label = Scalar(label, label=\"label\")\n",
    "            # Compute the loss for a single value\n",
    "            loss = (prediction - label) ** 2\n",
    "            # Accumulate the total loss\n",
    "            total_loss = total_loss + loss\n",
    "\n",
    "        # Reset gradients before backpropagation\n",
    "        for w in weights:\n",
    "            w.grad = 0.0\n",
    "        bias.grad = 0.0\n",
    "\n",
    "        # Perform the backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Update weights and bias\n",
    "        for w in weights:\n",
    "            w.data -= lr * w.grad\n",
    "        bias.data -= lr * bias.grad\n",
    "\n",
    "        # Record the loss and weights for analysis\n",
    "        if record_history:\n",
    "            loss_history.append(total_loss.data)\n",
    "            weight_history.append([w.data for w in weights])\n",
    "            bias_history.append(bias.data)\n",
    "\n",
    "        # Output the loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.data}\")\n",
    "\n",
    "    return weights, bias, loss_history, weight_history, bias_history"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define hyperparameters and train the perceptron\n",
    "n_epochs = 50\n",
    "learning_rate = 1e-2\n",
    "\n",
    "weights, bias, loss_hist, weight_hist, bias_hist = train_perceptron(\n",
    "    RB, epochs=n_epochs, lr=learning_rate\n",
    ")\n",
    "# weights, bias = train_perceptron(X, num_epochs=10, learning_rate=1e-2)\n",
    "print(\"Final Weights:\", [w.data for w in weights])\n",
    "print(\"Final Bias:\", bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Training_metrics_and_analysis"
   },
   "source": [
    "## Training metrics and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often a good idea to keep track of the loss over epochs - a good sign that things are working as expected is a gradual decrease in the loss as a function of epochs. A plateau in the loss may indicate that the optimization has reached or is bouncing around a global minimum.\n",
    "\n",
    "While less informative for larger networks, we can also gain some insight by investigating the evolution of the network parameters as a function of epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "# Initialize epoch axis for plotting\n",
    "epoch_axis = np.arange(1, n_epochs + 1)\n",
    "# Plot the loss as a function of epoch\n",
    "ax1.plot(epoch_axis, loss_hist, marker=\"o\", color=\"blue\")\n",
    "\n",
    "# Set labels for ax1\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(r\"$\\mathcal{L}$\")\n",
    "# Place hyperparameters as text in the top right corner\n",
    "ax1.text(\n",
    "    0.95,\n",
    "    0.95,\n",
    "    rf\"$\\mathtt{{n\\_epoch}} = {n_epochs}, \\quad \\mathtt{{lr}} = {learning_rate}$\",\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=12,\n",
    "    verticalalignment=\"top\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "\n",
    "# Plot the perceptron parameters as a function of epoch\n",
    "epoch_axis = np.arange(1, n_epochs + 2)\n",
    "ax2.plot(epoch_axis, bias_hist, marker=\"o\", color=\"orange\", label=\"$b$\")\n",
    "ax2.plot(\n",
    "    epoch_axis, [w[0] for w in weight_hist], marker=\"o\", color=\"red\", label=\"$w_1$\"\n",
    ")\n",
    "ax2.plot(\n",
    "    epoch_axis, [w[1] for w in weight_hist], marker=\"o\", color=\"green\", label=\"$w_2$\"\n",
    ")\n",
    "ax2.plot(\n",
    "    epoch_axis, [w[2] for w in weight_hist], marker=\"o\", color=\"purple\", label=\"$w_3$\"\n",
    ")\n",
    "\n",
    "# Set labels and legend for ax2\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Parameters\")\n",
    "ax2.legend(framealpha=1.0, edgecolor=\"black\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the performance on a new data sample\n",
    "validation = RB_dataset(1000, seed=142)\n",
    "\n",
    "# Run through the network and evaluate performance (track the performance for plotting)\n",
    "performance = []\n",
    "for i in range(validation.shape[0]):\n",
    "    x1, x2, x3, label = validation[i]\n",
    "    prediction = perceptron(\n",
    "        Scalar(x1), Scalar(x2), Scalar(x3), weights[0], weights[1], weights[2], bias\n",
    "    )\n",
    "\n",
    "    # Output 1 if correct, 0 if incorrect\n",
    "    performance.append(1 if prediction.data == label else 0)\n",
    "\n",
    "# Print the performance as a total percentage\n",
    "performance_percentage = np.mean(performance) * 100\n",
    "print(f\"Performance on validation set: {performance_percentage:.2f}% accuracy\")\n",
    "\n",
    "# Plot RB values and performance\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "for i in range(validation.shape[0]):\n",
    "    x1, x2, x3, label = validation[i]\n",
    "    color = (x1, x2, x3)\n",
    "    marker = \"o\" if performance[i] == 1 else \"x\"\n",
    "    if performance[i] == 1:\n",
    "        ax.scatter(x1, x3, c=[color], marker=marker, s=80, edgecolor=\"k\")\n",
    "    else:\n",
    "        ax.scatter(x1, x3, c=[color], marker=marker, s=80)\n",
    "\n",
    "# Include the performance percentage as a text annotation\n",
    "ax.text(\n",
    "    0.55,\n",
    "    0.075,\n",
    "    rf\"Performance: {performance_percentage:.2f}\\% correct\",\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=12,\n",
    "    verticalalignment=\"bottom\",\n",
    "    bbox=dict(facecolor=\"white\", alpha=1.0, edgecolor=\"black\"),\n",
    ")\n",
    "\n",
    "# Plot R = B boundary\n",
    "min_val = min(validation[:, 0].min(), validation[:, 2].min())\n",
    "max_val = max(validation[:, 0].max(), validation[:, 2].max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], \"k--\", label=\"R = B\")\n",
    "ax.set_xlabel(\"R (Red value)\")\n",
    "ax.set_ylabel(\"B (Blue value)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the network hyperparameters (learning rate and number of epochs) to get a feel for what optimal values may be. What learning rate parameter is too large? Too small? Describe an optimization procedure using the training metric and analysis techniques that can help you choose the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "###START_EXERCISE\n",
    "# Your answer here\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "###START_SOLUTION\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_more_structured_implementation"
   },
   "source": [
    "## A more structured implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build out a `Perceptron` class in a slightly more structured way such that we can recursively build up **layers** of perceptrons.\n",
    "\n",
    "We'll also replace the step final function by a `tanh` to add a smoother gradient."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size, activate=True):\n",
    "        self.activate = activate\n",
    "        self.weights = [\n",
    "            Scalar(random.uniform(-1, 1), label=f\"w{i}\")\n",
    "            for i in range(1, input_size + 1)\n",
    "        ]\n",
    "        self.bias = Scalar(random.uniform(-1, 1), label=\"b\")\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        weighted_sum = sum([w * x for w, x in zip(self.weights, inputs)], self.bias)\n",
    "        if self.activate:\n",
    "            return weighted_sum.tanh()\n",
    "        else:\n",
    "            return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize the perceptron with 2 inputs\n",
    "p = Perceptron(2)\n",
    "# Example input vector with 2 scalars\n",
    "x = [Scalar(1.0, label=\"x1\"), Scalar(-1.0, label=\"x2\")]\n",
    "# Forward pass through the perceptron\n",
    "p_forward = p(x)\n",
    "print(p_forward)\n",
    "# Backward pass\n",
    "p_forward.backward()\n",
    "# Visualize the computation graph\n",
    "p_forward.visualize_graph()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_perceptrons, input_size, activate=True):\n",
    "        self.perceptrons = [\n",
    "            Perceptron(input_size, activate) for _ in range(n_perceptrons)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return [p(inputs) for p in self.perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set layer parameters\n",
    "n_perceptrons = 2\n",
    "input_size = 2\n",
    "\n",
    "l = Layer(n_perceptrons=n_perceptrons, input_size=input_size)\n",
    "# Example input vector with 3 scalars\n",
    "x = [Scalar(1.0, label=\"x1\"), Scalar(-1.0, label=\"x2\")]\n",
    "# Forward pass through the layer\n",
    "l_forward = l(x)\n",
    "# Print the output of the layer\n",
    "print(l_forward)\n",
    "# Backward pass (for the first perceptron in the layer)\n",
    "l_forward[0].backward()\n",
    "# Visualize the computation graph for the first perceptron in the layer\n",
    "l_forward[0].visualize_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Multi-layer_perceptrons"
   },
   "source": [
    "## Multi-layer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a single perceptron can only model simple linear decision boundaries, the true power of neural networks emerges when we stack perceptrons together into layers, forming what is known as a **multi-layer perceptron** (MLP). By composing multiple layers of perceptrons we enable the network to learn and represent highly complex, nonlinear functions - this makes MLPs the foundational building block for most modern neural networks.\n",
    "\n",
    "In this case, we build the last layer without an activation function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(\n",
    "        self, n_inputs, n_outputs, n_hidden_layers=1, n_perceptrons_per_layer=5\n",
    "    ):\n",
    "        self.layers = []\n",
    "        # Input layer\n",
    "        self.layers.append(Layer(n_perceptrons_per_layer, n_inputs))\n",
    "        # Hidden layers\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            self.layers.append(Layer(n_perceptrons_per_layer, n_perceptrons_per_layer))\n",
    "        # Output layer (with no activation function)\n",
    "        self.layers.append(Layer(n_outputs, n_perceptrons_per_layer, activate=False))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        if len(inputs) == 1:\n",
    "            return inputs[0]\n",
    "        else:\n",
    "            return inputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set architecture parameters\n",
    "n_inputs = 4\n",
    "n_outputs = 5\n",
    "n_hidden_layers = 2\n",
    "n_perceptrons_per_layer = 5\n",
    "\n",
    "# Initialize the multi-layer perceptron\n",
    "mlp = MultiLayerPerceptron(\n",
    "    n_inputs=n_inputs,\n",
    "    n_outputs=n_outputs,\n",
    "    n_hidden_layers=n_hidden_layers,\n",
    "    n_perceptrons_per_layer=n_perceptrons_per_layer,\n",
    ")\n",
    "\n",
    "# Example input vector with 4 scalars\n",
    "y = [Scalar(1.0), Scalar(2.0), Scalar(-5.0), Scalar(-3.0)]\n",
    "# Forward pass through the multi-layer perceptron\n",
    "mlp_forward = mlp(y)\n",
    "print(mlp_forward)\n",
    "# Backward pass through the multi-layer perceptron\n",
    "if n_outputs == 1:\n",
    "    mlp_forward.backward()\n",
    "    # Print inputs after the backward pass\n",
    "    print(\"Input y:\", y)\n",
    "else:\n",
    "    for output in mlp_forward:\n",
    "        output.backward()\n",
    "    # Print inputs after the backward pass\n",
    "    print(\"inputs y:\", y)\n",
    "    # For visualization purposes\n",
    "    mlp_forward = mlp_forward[0]\n",
    "\n",
    "# Visualize the computation graph for one the multi-layer perceptron\n",
    "mlp_forward.visualize_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function in `MultiLayerPerceptron` that will compute and return the total number of parameters in the network.\n",
    "\n",
    "How many parameters does the following network have?\n",
    "\n",
    "```\n",
    "n_inputs                = 64\n",
    "n_outputs               = 1\n",
    "n_hidden_layers         = 23\n",
    "n_perceptrons_per_layer = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "###START_EXERCISE\n",
    "# Your answer here\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "###START_SOLUTION\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Classifying_a_<font_color=\"red\">c</font><font_color=\"magenta\">o</font><font_color=\"blue\">l</font><font_color=\"cyan\">o</font><font_color=\"green\">r</font>_<font_color=\"yellow\">w</font><font_color=\"black\">h</font><font_color=\"white\">e</font><font_color=\"red\">e</font><font_color=\"magenta\">l</font>"
   },
   "source": [
    "## Classifying a <font color=\"red\">c</font><font color=\"magenta\">o</font><font color=\"blue\">l</font><font color=\"cyan\">o</font><font color=\"green\">r</font> <font color=\"yellow\">w</font><font color=\"black\">h</font><font color=\"white\">e</font><font color=\"red\">e</font><font color=\"magenta\">l</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use the power of our newly built MLPs to learn a much harder task than the red-blue binary classification learned above.\n",
    "\n",
    "The goal is to train a neural network to distinguish between eight different color classes based on their RGB values: **<font color=\"red\">Red</font>, <font color=\"green\">Green</font>, <font color=\"blue\">Blue</font>, <font color=\"yellow\">Yellow</font>, <font color=\"cyan\">Cyan</font>, <font color=\"magenta\">Magenta</font>, <font color=\"white\">White</font>,** and **<font color=\"black\">Black</font>**. To do this, we generate a synthetic dataset of random RGB triplets, each normalized to the range $[0, 1]$. Each color sample is assigned a label according to simple rules (see below) that capture the essence of these color categories (for example, \"white\" if all channels are high, \"black\" if all are low, \"yellow\" if red and green are high but blue is low, etc.). This dataset provides a challenging multi-class problem for our MLP, requiring it to learn nonlinear boundaries in the RGB color cube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def RGB_dataset(num_samples=10000, seed=None):\n",
    "    \"\"\"\n",
    "    Create a dataset of normalized RGB values and labels assigned to 8 classes:\n",
    "    Red, Green, Blue, Yellow, Cyan, Magenta, White, Black.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Generate random RGB values\n",
    "    R = np.random.random(num_samples)\n",
    "    G = np.random.random(num_samples)\n",
    "    B = np.random.random(num_samples)\n",
    "\n",
    "    # Initialize labels\n",
    "    labels = np.zeros(num_samples, dtype=int)\n",
    "\n",
    "    # Class conditions (priority matters if overlapping)\n",
    "    for i in range(num_samples):\n",
    "        r, g, b = R[i], G[i], B[i]\n",
    "        if r > 0.7 and g > 0.7 and b > 0.7:\n",
    "            labels[i] = 6  # White\n",
    "        elif r < 0.3 and g < 0.3 and b < 0.3:\n",
    "            labels[i] = 7  # Black\n",
    "        elif r > 0.5 and g > 0.5 and b < 0.5:\n",
    "            labels[i] = 3  # Yellow\n",
    "        elif g > 0.5 and b > 0.5 and r < 0.5:\n",
    "            labels[i] = 4  # Cyan\n",
    "        elif r > 0.5 and b > 0.5 and g < 0.5:\n",
    "            labels[i] = 5  # Magenta\n",
    "        elif r > g and r > b and g < 0.5 and b < 0.5:\n",
    "            labels[i] = 0  # Red\n",
    "        elif g > r and g > b and r < 0.5 and b < 0.5:\n",
    "            labels[i] = 1  # Green\n",
    "        elif b > r and b > g and r < 0.5 and g < 0.5:\n",
    "            labels[i] = 2  # Blue\n",
    "        else:\n",
    "            labels[i] = np.argmax([r, g, b])  # fallback in case of ambiguity\n",
    "\n",
    "    # Stack into dataset\n",
    "    RGB = np.column_stack((R, G, B, labels))\n",
    "\n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    RGB = RGB[indices]\n",
    "\n",
    "    return RGB"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_samples = 10000\n",
    "RGB = RGB_dataset(num_samples, seed=42)\n",
    "\n",
    "# Get a feel for the dataset\n",
    "print(\"Dataset shape:\", RGB.shape)\n",
    "print(\"RGB value:\", RB[0, 0:3], \"\\nLabel:\", RGB[0, 3])\n",
    "print(\"R:\", RB[0, 0], \"G:\", RB[0, 1], \"B:\", RGB[0, 2])\n",
    "print(\"First 10 elements of dataset:\\n\", RGB[:10])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import colors\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Visualize the dataset on a color wheel instead\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "# Normalize RGB values to [0, 1]\n",
    "R = RGB[:, 0]\n",
    "G = RGB[:, 1]\n",
    "B = RGB[:, 2]\n",
    "\n",
    "# Convert RGB to HSV\n",
    "hsv = np.array([mcolors.rgb_to_hsv([r, g, b]) for r, g, b in zip(R, G, B)])\n",
    "\n",
    "# Plot the color wheel\n",
    "ax.scatter(hsv[:, 0], hsv[:, 1], c=RGB[:, :3], s=10)\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"Hue\")\n",
    "ax.set_ylabel(\"Saturation\")\n",
    "ax.set_title(\"Color wheel representation of RGB dataset\")\n",
    "\n",
    "# Add boundaries for each label in HSV space\n",
    "# We'll use the same logic as in RGB_dataset for the boundaries\n",
    "\n",
    "# For chromatic colors, boundaries are in hue\n",
    "# Red: hue near 0 or 1, S > 0.5, V > 0.3\n",
    "ax.axvspan(0.95, 1.0, color=\"red\", alpha=0.2, label=\"Red\")\n",
    "ax.axvspan(0.0, 0.05, color=\"red\", alpha=0.2)\n",
    "# Green: hue ~1/3\n",
    "ax.axvspan(0.28, 0.39, color=\"green\", alpha=0.2, label=\"Green\")\n",
    "# Blue: hue ~2/3\n",
    "ax.axvspan(0.61, 0.72, color=\"blue\", alpha=0.2, label=\"Blue\")\n",
    "# Yellow: hue ~1/6\n",
    "ax.axvspan(0.13, 0.19, color=\"gold\", alpha=0.2, label=\"Yellow\")\n",
    "# Cyan: hue ~0.5\n",
    "ax.axvspan(0.45, 0.55, color=\"cyan\", alpha=0.2, label=\"Cyan\")\n",
    "# Magenta: hue ~5/6\n",
    "ax.axvspan(0.83, 0.92, color=\"magenta\", alpha=0.2, label=\"Magenta\")\n",
    "\n",
    "ax.legend(loc=\"lower center\", fontsize=8, ncols=6)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"polar\"}, figsize=(6, 6))\n",
    "\n",
    "norm = mpl.colors.Normalize(0.0, 2 * np.pi)\n",
    "\n",
    "quant_steps = 2056\n",
    "cb = mpl.colorbar.ColorbarBase(\n",
    "    ax, cmap=mpl.colormaps[\"hsv\"], norm=norm, orientation=\"horizontal\"\n",
    ")\n",
    "\n",
    "cb.outline.set_visible(False)\n",
    "\n",
    "# ax.set_yticks([0, 0.5, 1.0])\n",
    "# ax.set_yticklabels(['0', '0.5', '1.0'])\n",
    "ax.set_xticks(np.linspace(0, 2 * np.pi, 7))\n",
    "ax.set_xticklabels([\"Red\", \"Yellow\", \"Green\", \"Cyan\", \"Blue\", \"Magenta\", \"Red\"])\n",
    "\n",
    "\n",
    "# White and Black are not hue-dependent, but can be shown as inner/outer rings\n",
    "# White: high radius (saturation low, value high)\n",
    "ax.fill_between(\n",
    "    np.linspace(0, 2 * np.pi, 200), 0.9, 1.0, color=\"white\", alpha=0.3, label=\"White\"\n",
    ")\n",
    "# Black: low radius (value low)\n",
    "ax.fill_between(\n",
    "    np.linspace(0, 2 * np.pi, 200), 0.0, 0.1, color=\"black\", alpha=0.3, label=\"Black\"\n",
    ")\n",
    "\n",
    "# Optional: add a legend for the boundaries\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.1), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the multi-class classification problem posed above by constructing and training an MLP (using the dataset produced above). Here are some useful checkpoints along this journey:\n",
    "\n",
    "1. Following the `classify.ipynb` tutorial, first implement a `SoftMax` method in `Scalar`: this function will take the raw **logit** output from the final MLP layer (notice how, by default, we defined the last layer of our MLP to not activate)\n",
    "   $$\\hat{p}_{i,y_i} \\equiv \\texttt{SoftMax}(\\textbf{z})_{y_i} = \\frac{\\exp(z_{i,y_i})}{\\sum_j \\exp(z_{i,j})}$$\n",
    "   where $z_{i,j}$ is $j$-th logit output from the MLP for the $i$-th data sample and $z_{i,y_i}$ corresponds to the logit in the position of the correct label $y_i$ of the $i$-th sample.\n",
    "2. Write up the cross-entropy loss\n",
    "   $$\\mathcal{L}_i = - \\log(\\hat{p}_{i,y_i}) = -z_{i,y_i} + \\log\\left(\\sum_j \\exp(z_{i,j})\\right)$$\n",
    "where $\\hat{p}$ is the predicted probability for the sample $i$ computed using the `SoftMax` function from above and $\\textbf{z}[\\texttt{id}(y_i)]$ is the output logit in the position corresponding to the true label of the $i$-th sample. Equivalently, in a slightly more unconventional notation:\n",
    "$$\\mathcal{L}_i = - \\log(\\hat{p}\\left({\\textbf{z}[\\texttt{id}(y_i)]}\\right) = -\\textbf{z}[\\texttt{id}(y_i)] + \\log\\left(\\sum_j \\exp(z_{i,j})\\right).$$\n",
    "3. Implement a training loop for the MLP.\n",
    "4. Train, validate, and assess the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "###START_EXERCISE\n",
    "# Your answer here\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "###START_SOLUTION\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**The_neural_network_zoo**"
   },
   "source": [
    "# **The neural network zoo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**Challenge**"
   },
   "source": [
    "### **Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at these websites:\n",
    "\n",
    "[https://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/](https://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/)\n",
    "\n",
    "[https://www.asimovinstitute.org/neural-network-zoo/](https://www.asimovinstitute.org/neural-network-zoo/)\n",
    "\n",
    "Choose a network and implement it from scratch using the `Scalar` engine."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "The_perceptron",
    "Classifying_<font_color=\"red\">red</font>_vs_<font_color=\"blue\">blue</font>_with_a_single_perceptron",
    "Loss-ology",
    "Training",
    "Training_metrics_and_analysis",
    "Exercise",
    "A_more_structured_implementation",
    "Multi-layer_perceptrons",
    "Exercise",
    "Classifying_a_<font_color=\"red\">c</font><font_color=\"magenta\">o</font><font_color=\"blue\">l</font><font_color=\"cyan\">o</font><font_color=\"green\">r</font>_<font_color=\"yellow\">w</font><font_color=\"black\">h</font><font_color=\"white\">e</font><font_color=\"red\">e</font><font_color=\"magenta\">l</font>",
    "Exercise",
    "**The_neural_network_zoo**",
    "**Challenge**"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}