{
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Simple_ML_introducion",
    "Introduction",
    "Classification",
    "Logistic_Regression",
    "Exercise",
    "Metrics",
    "Exercise",
    "Multiclass_(`sklearn`-based)"
   ],
   "provenance": []
  },
  "language_info": {
   "nbconvert_exporter": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.10.12",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "mimetype": "text/x-python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "cells": [
  {
   "source": [
    "# Simple ML introducion\n",
    "\n",
    "Written by:\n",
    "- Manuel Szewc (School of Physics, University of Cincinnati)\n",
    "- Philip Ilten (School of Physics, University of Cincinnati)\n",
    "$\\renewcommand{\\gtrsim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{>}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\lessim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{<}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\as}{\\alpha_{\\mathrm{s}}}\\renewcommand{\\aem}{\\alpha_{\\mathrm{em}}}\\renewcommand{\\kT}{k_{\\perp}}\\renewcommand{\\pT}{p_{\\perp}}\\renewcommand{\\pTs}{p^2_{\\perp}}\\renewcommand{\\pTe}{\\p_{\\perp\\mrm{evol}}}\\renewcommand{\\pTse}{\\p^2_{\\perp\\mrm{evol}}}\\renewcommand{\\pTmin}{p_{\\perp\\mathrm{min}}}\\renewcommand{\\pTsmim}{p^2_{\\perp\\mathrm{min}}}\\renewcommand{\\pTmax}{p_{\\perp\\mathrm{max}}}\\renewcommand{\\pTsmax}{p^2_{\\perp\\mathrm{max}}}\\renewcommand{\\pTL}{p_{\\perp\\mathrm{L}}}\\renewcommand{\\pTD}{p_{\\perp\\mathrm{D}}}\\renewcommand{\\pTA}{p_{\\perp\\mathrm{A}}}\\renewcommand{\\pTsL}{p^2_{\\perp\\mathrm{L}}}\\renewcommand{\\pTsD}{p^2_{\\perp\\mathrm{D}}}\\renewcommand{\\pTsA}{p^2_{\\perp\\mathrm{A}}}\\renewcommand{\\pTo}{p_{\\perp 0}}\\renewcommand{\\shat}{\\hat{s}}\\renewcommand{\\a}{{\\mathrm a}}\\renewcommand{\\b}{{\\mathrm b}}\\renewcommand{\\c}{{\\mathrm c}}\\renewcommand{\\d}{{\\mathrm d}}\\renewcommand{\\e}{{\\mathrm e}}\\renewcommand{\\f}{{\\mathrm f}}\\renewcommand{\\g}{{\\mathrm g}}\\renewcommand{\\hrm}{{\\mathrm h}}\\renewcommand{\\lrm}{{\\mathrm l}}\\renewcommand{\\n}{{\\mathrm n}}\\renewcommand{\\p}{{\\mathrm p}}\\renewcommand{\\q}{{\\mathrm q}}\\renewcommand{\\s}{{\\mathrm s}}\\renewcommand{\\t}{{\\mathrm t}}\\renewcommand{\\u}{{\\mathrm u}}\\renewcommand{\\A}{{\\mathrm A}}\\renewcommand{\\B}{{\\mathrm B}}\\renewcommand{\\D}{{\\mathrm D}}\\renewcommand{\\F}{{\\mathrm F}}\\renewcommand{\\H}{{\\mathrm H}}\\renewcommand{\\J}{{\\mathrm J}}\\renewcommand{\\K}{{\\mathrm K}}\\renewcommand{\\L}{{\\mathrm L}}\\renewcommand{\\Q}{{\\mathrm Q}}\\renewcommand{\\R}{{\\mathrm R}}\\renewcommand{\\T}{{\\mathrm T}}\\renewcommand{\\W}{{\\mathrm W}}\\renewcommand{\\Z}{{\\mathrm Z}}\\renewcommand{\\bbar}{\\overline{\\mathrm b}}\\renewcommand{\\cbar}{\\overline{\\mathrm c}}\\renewcommand{\\dbar}{\\overline{\\mathrm d}}\\renewcommand{\\fbar}{\\overline{\\mathrm f}}\\renewcommand{\\pbar}{\\overline{\\mathrm p}}\\renewcommand{\\qbar}{\\overline{\\mathrm q}}\\renewcommand{\\rbar}{\\overline{\\mathrm{r}}}\\renewcommand{\\sbar}{\\overline{\\mathrm s}}\\renewcommand{\\tbar}{\\overline{\\mathrm t}}\\renewcommand{\\ubar}{\\overline{\\mathrm u}}\\renewcommand{\\Bbar}{\\overline{\\mathrm B}}\\renewcommand{\\Fbar}{\\overline{\\mathrm F}}\\renewcommand{\\Qbar}{\\overline{\\mathrm Q}}\\renewcommand{\\tms}{{t_{\\mathrm{\\tiny MS}}}}\\renewcommand{\\Oas}[1]{{\\mathcal{O}\\left(\\as^{#1}\\right)}}$"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Simple_ML_introducion"
   }
  },
  {
   "source": [
    "## Introduction"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Introduction"
   }
  },
  {
   "source": [
    "This notebook wants to implement simple Machine Learning algorithms for classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import os\n",
    "\n",
    "# To generate data and handle arrays\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc(\"axes\", labelsize=14)\n",
    "mpl.rc(\"xtick\", labelsize=12)\n",
    "mpl.rc(\"ytick\", labelsize=12)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "## Classification"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Classification"
   }
  },
  {
   "source": [
    "In classification problems, we are interested in predicting a class asignment $\\mathcal{C}_{k}$ to a measurement $x$. The training data consists of paired measurements and class labels $x_{\\text{train}},t_{\\text{train}}$.\n",
    "\n",
    "There are three types of classification approaches:\n",
    "\n",
    "* Discriminant functions, where we learn a function $y(x,w)$ that defines the border between classes. For the binary case, and using $t \\in \\{-1,1\\}$ , this corresponds to definen a threshold $y_{0}$ (usually zero) such that if $y(x,w)>y_{0}$, $t = 1$ and $t=-1$ otherwise. This is the case for `Perceptron` and `Support Vector Machines` among others. `DecisionTrees` can be thought of in these terms as well, although they can also estimate probabilities.\n",
    "* Discriminative models, where we learn a function $y(x,w)$ that encodes the probability of a class asignment given $x$, $p(\\mathcal{C}_{k}|x)$. A class asignment can be made by selecting the class which maximizes the probability, although other criteria can be applied as well. `LogisticRegressors` and simple `Neural Network Classifiers` are examples of this.\n",
    "* Generative models, where we learn a function $y(x,w)$ that encodes the probability of $x$ per class, $p(x|\\mathcal{C}_{k})$. A class asignment can be made by estimating the per class probabilities using Bayes' rule and selecting the class which maximizes this probability. `Naive Bayes` classifiers or conditional density estimators are examples of this.\n",
    "\n",
    "\n",
    "Let's focus more on discriminative models. Here, we are interested in the per-class probability, which is a **posterior** over class asignments.  For the binary case, we only need to specifiy $p(\\mathcal{C}_{1}|x)$ since $p(\\mathcal{C}_{2}|x)=1-p(\\mathcal{C}_{1}|x)$. We deal with the multiclass problem further down the notebook.\n",
    "\n",
    "For the binary case, we compute\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=\\frac{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})+p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}=\\frac{1}{1+\\frac{p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}}$$\n",
    "\n",
    "Defining the **log-odds ratio**\n",
    "\n",
    "$$a=\\text{Ln }\\frac{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}{p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}=\\text{Ln }\\frac{p(\\mathcal{C}_{1}|x)}{p(\\mathcal{C}_{2}|x)}$$\n",
    "\n",
    "we have that\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=\\frac{1}{1+\\text{e}^{-a}}=\\sigma(a)$$\n",
    "\n",
    "where $\\sigma(a)$ is the  **sigmoid function**. Thus, the binary problem is reduced to computing the log-odds ratio between the two classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "prob = np.linspace(-10, 10, 200)\n",
    "\n",
    "a_vals = 1 / (1 + np.exp(-prob))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(prob, a_vals, color=\"black\")\n",
    "ax.scatter(0.0, 0.5, color=\"red\")\n",
    "ax.axvline(0.0, linestyle=\"dashed\", color=\"red\")\n",
    "ax.axhline(0.5, linestyle=\"dashed\", color=\"red\")\n",
    "ax.set_ylabel(r\"$p(\\mathcal{C}_{1}|x)$\", fontsize=16)\n",
    "ax.set_xlabel(\"Log odds\", fontsize=16)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "We see how the sigmoid function is a function lying in $[0,1]$, even as the log odds ratio takes values in $(-\\infty,\\infty)$. This is necessary since we're interpreting it as a probability.\n",
    "\n",
    "A particularly important point is where the two classes are equally likely. There, the log odds ratio is 1 and the per class probability is 0.5, as it should. In general, this is selected as the decision boundary.\n",
    "\n",
    "Two useful properties of the sigmoid function are:\n",
    "\n",
    "$$\\sigma(-a)=1-\\sigma(a)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{d\\sigma}{da}=\\sigma(1-\\sigma)$$\n",
    "\n",
    "For the $K$ class case, we can generalize the sigmoid to the normalized exponential or **softmax** function:\n",
    "\n",
    "$$p(\\mathcal{C}_{k}|x)=\\frac{p(x|\\mathcal{C}_{k})p(\\mathcal{C}_{k})}{\\sum_{l=1}^{K}p(x|\\mathcal{C}_{l})p(\\mathcal{C}_{l})}=\\frac{e^{a_{k}}}{\\sum_{l=1}^{K}e^{a_{l}}}$$\n",
    "\n",
    "where $a_{k}=\\text{Ln }p(x|\\mathcal{C}_{k})p(\\mathcal{C}_{k})$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Logistic_Regression"
   }
  },
  {
   "source": [
    "The simplest discriminative classifier is the `Logistic Regressor`. For the two class case, we model the per class probability posterior as\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|\\vec{x})=y(\\vec{x},\\vec{w})=\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$$\n",
    "\n",
    "$$p(\\mathcal{C}_{0}|\\vec{x})=1-y(\\vec{x},\\vec{w})=1-\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))=\\sigma(-\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$$\n",
    "\n",
    "We already have two features of note:\n",
    "\n",
    "- $\\vec{w}^{T}\\vec{\\phi}(\\vec{x})$ acts as the decision function in a discriminant approach. In particular, $\\vec{w}^{T}\\vec{\\phi}(\\vec{x})=0$ defines the equi-probability surface.\n",
    "- The sigmoid is a **non-linear activation function**. This feature, necessary to transform a linear model into a probability estimate, will be very present in `Neural Networks` even for regression for different reasons (the universal approximation is ensured by internal non-linear activation functions between neuron layers).\n",
    "\n",
    "As in the generalized linear models for regression,  $\\vec{\\phi}$ includes in principle the bias with $\\phi_{0}(\\vec{x})=1$.\n",
    "\n",
    "As with regression, we can group all training pairs into a design matrix\n",
    "\n",
    "$$\\Phi=\\begin{pmatrix}\\vec{\\phi}^{T}(\\vec{x}_{1}) \\\\ ... \\\\ \\vec{\\phi}^{T}(\\vec{x}_{N})\\end{pmatrix}$$\n",
    "\n",
    "obtaining\n",
    "\n",
    "$$\\sigma(\\Phi\\cdot \\vec{w}) = \\begin{pmatrix}\\sigma(\\vec{\\phi}^{T}(\\vec{x}_{1}) \\cdot\\vec{w})\\\\ ... \\\\ \\sigma(\\vec{\\phi}^{T}(\\vec{x}_{N})\\cdot\\vec{w})\\end{pmatrix}=\\begin{pmatrix}\\sigma((\\vec{w}^{T} \\cdot\\vec{\\phi}(\\vec{x}_{1}))^T)\\\\ ... \\\\ \\sigma((\\vec{w}^{T} \\cdot\\vec{\\phi}(\\vec{x}_{N}))^T)\\end{pmatrix}=\\begin{pmatrix}y(\\vec{x}_{1},\\vec{w})\\\\ ... \\\\ y(\\vec{x}_{N},\\vec{w})\\end{pmatrix}$$\n",
    "\n",
    "Now we need a criteria to obtain the best coefficients $\\vec{w}$. As in regression, a natural choice is to maximize the likelhiood (or minmize the negative log-likelihood). For a binary variable, we have only two possible results which we can label as success or failure or heads and tails. Assuming the points are independent and identically distributed, each label has a Bernoulli distribution\n",
    "\n",
    "$$p(\\text{t}|\\vec{x},\\vec{w})=p(t|\\mu)=\\mu^{t}(1-\\mu)^{1-t}$$\n",
    "\n",
    "where $\\mu=p(\\mathcal{C}_{1}|\\vec{x})$ is the success probability, which is exactly what we're trying to model!\n",
    "\n",
    "Thus, for a dataset $\\vec{x}_{n}$, with $n=1,..,N$ the likelihood is\n",
    "\n",
    "$$\\mathcal{L}(\\vec{w})=p(\\text{T}|X,\\vec{w})=\\prod_{n=1}^{N}y^{t_n}_{n}(1-y_{n})^{1-t_n}$$\n",
    "\n",
    "where $y_{n}=y(x_{n},\\vec{w})$. Again, the logarithm is easier to handle\n",
    "\n",
    "$$\\ln \\mathcal{L}(\\vec{w})=\\sum_{n=1}^{N}(t_{n}\\ln y_{n}+(1-t_{n})\\ln (1-y_{n}))$$\n",
    "\n",
    "Again we can define an error by taking the negative log-likelihood. This is called the **binary cross-entropy** (BCE) between $t$ and $y$\n",
    "\n",
    "$$E(\\vec{w})=-\\sum_{n=1}^{N}(t_{n}\\ln y_{n}+(1-t_{n})\\ln (1-y_{n}))$$\n",
    "\n",
    "We can see how BCE forces the right behaviour into the model:\n",
    "\n",
    "If $t_{n}=1$, we care about $\\ln y_{n}$. Thus, $y_{n}$ needs to be close to 1 to minimize the BCE.\n",
    "\n",
    "If $t_{n}=0$, we care about $\\ln (1-y_{n})$. Thus, $y_{n}$ needs to be close to 0 to minimize the BCE.\n",
    "\n",
    "Just a reminder, $y_{n}$ is not meant to match $t_{n}$ but $p(t_{n}|\\vec{x}_{n},\\vec{w})$. Thus, we also seek a degree of confidence to be embedded into the model.\n",
    ""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To obtain useful estimates, we need to be able to minimize the BCE with respect to the parameters $\\vec{w}$.\n",
    "\n",
    "For logistic regression, the problem is not as easy as it was for linear regression due to the presence of the logarithms and the sigmoid function. However, we can still solve this numerically. A particularly good choice is the Iterative Reweighted Least Squares or IRLS, which considers a Newton-Ralphson update for step $i$:\n",
    "\n",
    "$$\\vec{w}^{i}=\\vec{w}^{i-1}-\\mathrm{H}^{-1}\\nabla E(\\vec{w})|_{\\vec{w}^{i-1}}$$\n",
    "\n",
    "where $\\vec{w}^{0}$ is the initialized vector of parameters, $\\nabla E(\\vec{w})$ is the BCE gradient with respect to the parameters and $\\mathrm{H}$ is the Hessian matrix. For a logistic regressor where we combine a linear model with a sigmoide function, this simplifies to a set of **iterative normal equations**:\n",
    "\n",
    "$$\\vec{w}^{i}=\\Phi^{T}\\mathrm{R}\\Phi^{-1}\\Phi^{T}\\mathrm{R}\\mathrm{z}$$\n",
    "\n",
    "where $\\Phi$ is the design matrix, $\\mathrm{R}$ a diagonal matrix whose elements are $y_{n}(1-y_{n})$ and $\\mathrm{z}$ is\n",
    "\n",
    "$\\mathrm{z}=\\Phi\\vec{w}^{i-1}-\\mathrm{R}^{-1}(\\mathrm{Y}-\\mathrm{T})$\n",
    "\n",
    "where $\\mathrm{Y}$ e $\\mathrm{T}$ are the prediction and label vectors. One should remember that $\\mathrm{R}$, $\\mathrm{z}$ and $\\mathrm{Y}$ are all functions of $\\vec{w}^{-1}$.\n",
    "\n",
    "\n",
    "We could have applied such an algorithm to linear regression, and observed how it converge to the closed solution in 1 step. Additionally, be sure to notice that is an iterative algorithm but it is not sequential since it considers the full dataset at each step."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   }
  },
  {
   "source": [
    "Let's consider the following dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "size1 = 250\n",
    "mu1 = [0, 0]\n",
    "cov1 = [[1, 0.95], [0.95, 1]]\n",
    "\n",
    "size2 = 200\n",
    "mu2 = [-1, 0.5]\n",
    "cov2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "np.random.seed(20200922)\n",
    "# Sample classes\n",
    "xc1 = np.random.multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "xc2 = np.random.multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "print(xc1.shape, xc2.shape)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*xc1, \"ob\", mfc=\"None\", label=\"C1\")\n",
    "ax.plot(*xc2, \"or\", mfc=\"None\", label=\"C2\")\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_aspect(\"equal\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "X = np.hstack([xc1, xc2]).T\n",
    "\n",
    "tc1 = np.ones(xc1.shape[1])\n",
    "tc2 = np.zeros(xc2.shape[1])\n",
    "\n",
    "t = np.concatenate([tc1, tc2]).reshape(-1, 1)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Using this sigmoid function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def sigmoid(logoddsvec):\n",
    "    return 1 / (1 + np.exp(-logoddsvec))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Train a Logistic Regressor, print the best parameters, compute the predicted probability per measurement and plot it as a function of ($x_1,x_2$)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###START_SOLUTION\n",
    "\n",
    "Phi = np.hstack([np.ones(len(X)).reshape(-1, 1), X])  # design matrix\n",
    "# print(Phi.shape)\n",
    "w = [np.ones(3).reshape(-1, 1)]  # initialize weights\n",
    "n_iter = 13  # define the number of iterations to play with\n",
    "for i in range(n_iter):\n",
    "    try:\n",
    "        # obtain the initial ys using my weights, the design matrix and the activation function\n",
    "        y = sigmoid(np.dot(Phi, w[-1]))\n",
    "        # print(y.shape)\n",
    "        # R=np.diag(list(map(lambda yy: yy*(1-yy), y[:,0])))#matrix R\n",
    "        R = np.diag(y[:, 0] * (1 - y[:, 0]))  # matrix R\n",
    "        # print(R.shape)\n",
    "        aux_matrix = np.linalg.solve(R, (y - t))  # necessary auxiliar step\n",
    "        z = np.dot(Phi, w[-1]) - aux_matrix  # matrix z\n",
    "        aux_matrix_2 = np.dot(np.dot(Phi.T, R), Phi)\n",
    "        aux_matrix_3 = np.dot(np.dot(Phi.T, R), z)\n",
    "        w_aux = np.linalg.solve(aux_matrix_2, aux_matrix_3)\n",
    "        if 1 == 1:  # convergence criterion\n",
    "            w.append(w_aux)\n",
    "        else:\n",
    "            break\n",
    "    except:\n",
    "        break"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Veamos como evolucionan los coeficientes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "for w_val in w:\n",
    "    print(w_val[:, 0])"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "W = w[-1]\n",
    "y = sigmoid(np.dot(Phi, W))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.hist(\n",
    "    y[t[:, 0] > 0.5],\n",
    "    bins=np.linspace(0, 1, 10),\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    label=\"Predicted probabilities for $C_1$\",\n",
    ")\n",
    "plt.hist(\n",
    "    y[t[:, 0] <= 0.5],\n",
    "    bins=np.linspace(0, 1, 10),\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"red\",\n",
    "    label=\"Predicted probabilities for $C_2$\",\n",
    ")\n",
    "plt.axvline(\n",
    "    0.5, color=\"red\", linestyle=\"dashed\", linewidth=2, label=\"Decision boundary\"\n",
    ")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1 = np.linspace(-6.0, 3.0, 100)\n",
    "x2 = np.linspace(-3.0, 4.0, 100)\n",
    "X1toplot, X2toplot = np.meshgrid(x1, x2)\n",
    "# plt.xlim(0.0,0.2)\n",
    "# plt.ylim(0.0,0.2)\n",
    "Z = (\n",
    "    np.asarray(\n",
    "        [\n",
    "            sigmoid(np.dot(W.T, [1.0, el[0], el[1]]))\n",
    "            for el in np.c_[X1toplot.ravel(), X2toplot.ravel()]\n",
    "        ]\n",
    "    )\n",
    ").reshape(X1toplot.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*xc1, \"ob\", mfc=\"None\", label=\"C1\")\n",
    "ax.plot(*xc2, \"or\", mfc=\"None\", label=\"C2\")\n",
    "contour = ax.contourf(\n",
    "    X1toplot, X2toplot, Z, levels=[0.0, 0.001, 0.1, 0.5, 0.9, 0.999, 1.0], alpha=0.6\n",
    ")\n",
    "plt.colorbar(contour, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_aspect(\"equal\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "###STOP_SOLUTION"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "### Metrics\n",
    "\n",
    "Once we have a model, we need a criteria for class asignment. For the binary case, a natural choice is to assign $x\\in\\mathcal{C}_1$ if\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)\\geq p(\\mathcal{C}_{2}|x)$$\n",
    "\n",
    "Since the two probabilities add up to one, this criteria sets the **decision boundary** at\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=0.5$$\n",
    "\n",
    "This choice can be shown to maximize the **accuracy** given by\n",
    "\n",
    "$$\\mathrm{Accuracy = }\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{FP}+\\mathrm{TN}+\\mathrm{FN}}$$\n",
    "\n",
    "where TP, TN, FP and FN stand for True Positive, True Negative, False Positive and False Negative *ie* the fractions of correctly and incorrectly assigned measurements per class (positive is $C_{1}$, negative is $C_{2}$).\n",
    "\n",
    "That is, we're minimizing the **total misasignments**.\n",
    "\n",
    "Two other very useful metris are **precision** and **recall**:\n",
    "\n",
    "$$\\mathrm{Precision = }\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$$\n",
    "\n",
    "$$\\mathrm{Recall = }\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$$\n",
    "\n",
    "Precision captures how many positive samples are actually positive, and recall relates how many positive samples are we tagging as positives.\n",
    "\n",
    "All of these can be derived from the **confusion matrix**, which cointains at row, column $(i,j)$ the number of elements belonging to class $i$ that are labelled as belonging class $j$."
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Metrics"
   }
  },
  {
   "source": [
    "def accuracy_score(labels, predictions):\n",
    "    return np.mean([a == b for a, b in zip(labels, predictions)])\n",
    "\n",
    "\n",
    "def precision_score(labels, predictions):\n",
    "    return np.sum([a and b for a, b in zip(labels == 1, predictions == 1)]) / np.sum(\n",
    "        predictions == 1\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_score(labels, predictions):\n",
    "    return np.sum([a and b for a, b in zip(labels == 1, predictions == 1)]) / np.sum(\n",
    "        labels == 1\n",
    "    )"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "def confusion_matrix(labels, predictions):\n",
    "    #  pred0 pred1\n",
    "    # verdad0  VN  FP\n",
    "    # verdad1  FN  VP\n",
    "    TP = np.sum(labels[np.where(predictions[:, 0] == 1), 0])\n",
    "    FP = np.sum(1 - labels[np.where(predictions[:, 0] == 1), 0])\n",
    "    TN = np.sum(1 - labels[np.where(predictions[:, 0] == 0), 0])\n",
    "    FN = np.sum(labels[np.where(predictions[:, 0] == 0), 0])\n",
    "    return np.array([[TN, FP], [FN, TP]])"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Let's check them out"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Classify as 1 P(c1 | x) >= 0.5\n",
    "y_pred = np.where(y >= 0.5, 1, 0)\n",
    "\n",
    "print(t.shape)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "print(accuracy_score(t, y_pred))  # Accuracy\n",
    "print(precision_score(t, y_pred))  # Precision\n",
    "print(recall_score(t, y_pred))  # Recall"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "TP = np.sum(t[np.where(y_pred[:, 0] == 1), 0])\n",
    "FP = np.sum(1 - t[np.where(y_pred[:, 0] == 1), 0])\n",
    "TN = np.sum(1 - t[np.where(y_pred[:, 0] == 0), 0])\n",
    "FN = np.sum(t[np.where(y_pred[:, 0] == 0), 0])\n",
    "\n",
    "print(TP, FP, TN, FN)\n",
    "print((TP + TN) / (TP + FP + TN + FN))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "cf = confusion_matrix(t, y_pred)\n",
    "print(cf)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "TN, FP, FN, TP = cf.ravel()\n",
    "print(TP, FP, TN, FN)\n",
    "print((TP + TN) / (TP + FP + TN + FN))  # Accuracy from confusion matrix\n",
    "print((TP) / (TP + FP))  # Precision from confusion matrix\n",
    "print((TP) / (TP + FN))  # Recall from confusion matrix"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Introduce *varying metric *The ROC curve"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise\n",
    "\n",
    "Use `scikit-learn` for a more realistic problem, explore metrics and the ROC curve."
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   }
  },
  {
   "source": [
    "### Multiclass (`sklearn`-based)"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "Multiclass_(`sklearn`-based)"
   }
  },
  {
   "source": [
    "A lo largo de las \u00faltimas clases, nos encontramos varias veces con que hay ciertas ambig\u00fcedades al pasar de 2 clases a K clases, con K mayor que 2. Repasemos entonces un poco las diferencias entre el caso binario y el caso multiclase y veamos que podemos hacer al respecto.\n",
    "\n",
    "Vayamos primero al caso binario:\n",
    "- Tenemos las etiquetas, que pueden venir de varias formas: {0,1},{-1,1},{[1,0],[0,1]}. Para `sklearn`, con usar la primera estamos perfecto. Si codeamos a mano, hay que tener cuidado.\n",
    "- Tenemos las predicciones. Para el caso binario, s\u00f3lo necesito 1 n\u00famero para asignar la clase. Este n\u00famero puede ser una funci\u00f3n discriminante $y$ (por ejemplo el Perceptron o el SVC) o una probabilidad $p(C_{1}|x)$ (Regresi\u00f3n Log\u00edstica, Naive Bayes). Noten que s\u00f3lo necesitamos $p(C_{1}|x)$ porque, al tener \u00fanicamente dos clases, $p(C_{2}|x)=1-p(C_{1}|x)$.\n",
    "- Para el caso binario necesitamos entonces aprender **una sola funci\u00f3n**. Aparece entonces la frontera de decisi\u00f3n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ejemplifiquemos con un caso de juguete:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "gt_center = np.array([[2.0, 2.0], [-2.0, -2.0]])\n",
    "X, t = make_blobs(\n",
    "    1000,\n",
    "    n_features=2,\n",
    "    centers=gt_center,\n",
    "    cluster_std=1.5,\n",
    "    random_state=1234,\n",
    ")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Entrenemos una funci\u00f3n discriminante y un modelo discriminativo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "lr = LogisticRegression()\n",
    "percep = Perceptron()"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "lr.fit(X, t)\n",
    "percep.fit(X, t)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Para el caso del Perceptron, tendremos una \u00fanica funci\u00f3n de decisi\u00f3n. El criterio usual es que si $y\\geq0$, asigno a la clase $C_{1}$. Esto se puede modificar tal como vimos antes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(percep.decision_function(X[:1]))"
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   }
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = percep.decision_function(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, levels=[-1000, 0.0, 1000])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Mientras que para el Regresor Log\u00edstico tendremos dos probabilidades que suman uno, por lo que puedo quedarme \u00fanicamente con la segunda columna. El criterio por defecto es poner el umbral en $p(C_{1}|x)\\geq 0.5$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(lr.predict_proba(X[:1]))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = lr.predict_proba(Xvals)[:, 1].reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, levels=[0.0, 0.5, 1.0])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Vayamos ahora al caso multiclase:\n",
    "- Tenemos las etiquetas, que pueden venir esencialmente en dos formas: {0,1,...,K-1},{[1,0,...,0],[0,1,0,...,0],...}. Para `sklearn`, con usar la primera estamos perfecto. Si codeamos a mano, hay que tener cuidado.\n",
    "- Tenemos las predicciones. Ac\u00e1 es donde aparece la gran diferencia. \u00bfQu\u00e9 entrenamos?\n",
    "- El primer enfoque es el de _uno contra el resto_. Para cada clase $K$, entreno un clasificador que aprenda a distinguir esa clase del resto. Tenemos entonces K clasificadores. Por ejemplo, para 4 clases: (0,123), (1,023), (2,013), (3,012). Uno asigna entonces una clase viendo como se intersectan las distintas regiones de los clasificadores. Eso introduce la posibilidad de regiones ambiguas. Adem\u00e1s, el problema se puede volver muy desbalanceado.\n",
    "- El segundo enfoque es el de _uno contra uno_. En este enfoque, entreno un clasificador para cada combinaci\u00f3n de dos clases. Tenemos entonces $\\frac{K(K-1)}{2}$ clasificadores, uno por cada problema de dos clases. Por ejemplo, para 4 clases: (0,1), (0,2), (0,3), (1,2), (1,3), (2,3). Para asignar una clase, tomamos entonces el voto mayoritario. Es decir, cual es la clase que recibe m\u00e1s votos de cada uno de estos clasificadores. Esto tambi\u00e9n introduce ambig\u00fcedades.\n",
    "- El tercer enfoque, que no asigna ambiguedades. Es aprender un \u00fanico clasificador de $K$ clases. Para el caso de funci\u00f3n discriminante, esto se logra aprendiendo K funciones $y_{k}$ al mismo tiempo, y asignando la clase como la que da el maximo $y_{k}$ entre todos. Las fronteras de decisi\u00f3n son entonces dadas por las regiones donde coinciden dos funciones. Se puede mostrar que con este criterio ya no hay ambig\u00fcedades. Para el caso de modelos discriminativos y generativos, esto es todavia m\u00e1s f\u00e1cil. \u00a1Aprendemos $p(C_{k}|x)$ tal como antes! Ahora simplemente no simplificamos la clase redundante. Tampoco hay ambig\u00fcedades.\n",
    "\n",
    "Cual de los tres enfoques usamos depende del algoritmo. En particular, hay algunos que no pueden utilizar el tercer enfoque y entonces aparece el problema. Noten que ahora escribi el criterio, mientras que antes siempre aclare que podiamos jugar con el umbral. Para el caso multiclase, el an\u00e1lisis que hicimos se vuelve m\u00e1s d\u00edficil. Para poder hacerlo, vamos a binarizar el problema."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ejemplifiquemos con un caso de juguete:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "gt_center = np.array([[3.0, 3.0], [-3.0, -3.0], [-3.0, 3.0], [3.0, -3.0]])\n",
    "X, t = make_blobs(\n",
    "    1000,\n",
    "    n_features=2,\n",
    "    centers=gt_center,\n",
    "    cluster_std=1.0,\n",
    "    random_state=1234,\n",
    ")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Entrenemos una funci\u00f3n discriminante y un modelo discriminativo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lda = LinearDiscriminantAnalysis(solver=\"lsqr\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "lr.fit(X, t)\n",
    "lda.fit(X, t)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Para el caso del LinearDiscriminantAnalysis, tendremos ahora cuatro funciones de decisi\u00f3n."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(lda.decision_function(X[:1]))"
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   }
  },
  {
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(4 * 2 * 4, 2 * 3))\n",
    "for k in range(4):\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 0, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"red\",\n",
    "        label=\"$C_{0}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 1, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"blue\",\n",
    "        label=\"$C_{1}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 2, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"limegreen\",\n",
    "        label=\"$C_{2}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 3, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"gold\",\n",
    "        label=\"$C_{3}$\",\n",
    "    )\n",
    "    ax[k].set_xlabel(str(k) + \"-esima funci\u00f3n\")\n",
    "    ax[k].legend(loc=\"upper right\", framealpha=0.5)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = np.argmax(lda.decision_function(Xvals), axis=1).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Mientras que para el Regresor Log\u00edstico tendremos cuatro probabilidades que suman uno."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(lr.predict_proba(X[:1]))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(4 * 2 * 4, 2 * 3))\n",
    "for k in range(4):\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 0, k], histtype=\"step\", color=\"red\", label=\"$C_{0}$\"\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 1, k], histtype=\"step\", color=\"blue\", label=\"$C_{1}$\"\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 2, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"limegreen\",\n",
    "        label=\"$C_{2}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 3, k], histtype=\"step\", color=\"gold\", label=\"$C_{3}$\"\n",
    "    )\n",
    "    ax[k].set_xlabel(str(k) + \"-esima probabilidad\")\n",
    "    ax[k].legend(loc=\"upper center\", framealpha=0.5)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = np.argmax(lr.predict_proba(Xvals), axis=1).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "En general, vamos a utilizar algoritmos que tengan el tercer enfoque para el problema multiclase. En ese caso, nos quedamos tranquilos de que tenemos una asignaci\u00f3n clara basada en el criterio de maximizaci\u00f3n de funci\u00f3n discrimiante / probabilidad posterior. Podemos construir las matriz de confusi\u00f3n y calcular la accuracy:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(t, lr.predict(X)))\n",
    "print(accuracy_score(t, lr.predict(X)))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Si queremos ver otras m\u00e9tricas nos encontramos con que est\u00e1n dise\u00f1adas para el caso binario. Una posibilidad es binarizar de antemano y entrenar un clasificador binario en efecto reduciendo el problema a alguno de los otros dos enfoques (como hicimos el Martes pasado!). Otra es aprovechar este clasificador entrenado a full y tomar simplemente distintos positivos y negativos. El gran problema viene si queremos mover el umbral. Las asignaciones se toman en cuenta tomando el m\u00e1ximo, por lo que el valor relativo es importante, no el absoluto. Eso conspira para que el analogo a mover el umbral en el caso binario no sea trivial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "for k in range(4):\n",
    "    print(\"Clase \" + str(k) + \" contra todos\")\n",
    "    print(f1_score(np.where(t == k, 1.0, 0.0), np.where(lr.predict(X) == k, 1.0, 0.0)))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Sin embargo, hay algoritmos que, en `sklearn`, no tienen incorporado este tercer criterio. Por ejemplo, el `Perceptron`, utiliza el _Uno contra todos_.\n",
    "\n",
    "La clase pasada vimos las SVM. Para este algoritmo tampoco puede utilizarse f\u00e1cilmente el buen enfoque ya que esta muy pensado para el caso binario. SVC tiene que elegir entonces entre los dos enfoques imperfectos. Por lo tanto, hace lo siguiente:\n",
    "- `LinearSVC` resuelve el problema usando _uno contra todos_.  Se puede utilizar tambien el criterio `crammer_singer` que es parecido al tercer enfoque.\n",
    "- `SVC` resuelve el problema usando _uno contra uno_. Sin embargo, y por compatibilidad, reporta la funci\u00f3n de decisi\u00f3n como _uno contra todos_. Uno puede transformarla la funci\u00f3n de decisi\u00f3n a _uno contra uno_ sin prolbema."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "lsvc_1 = LinearSVC(loss=\"hinge\", penalty=\"l2\", C=10.0, max_iter=1000)\n",
    "svc_1 = SVC(kernel=\"linear\", C=10.0, decision_function_shape=\"ovr\")\n",
    "svc_2 = SVC(kernel=\"linear\", C=10.0, decision_function_shape=\"ovo\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "lsvc_1.fit(X, t)\n",
    "svc_1.fit(X, t)\n",
    "svc_2.fit(X, t)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "print(lsvc_1.decision_function(X[:1]).shape)\n",
    "print(svc_1.decision_function(X[:1]).shape)\n",
    "print(svc_2.decision_function(X[:1]).shape)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "Como estoy con `kernel=linear` puedo ver los coeficientes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(lsvc_1.coef_.shape)  # (nclasses, ncoefs)\n",
    "print(svc_1.coef_.shape)  # (nclasses*(nclasses-1)/2, ncoefs)\n",
    "print(svc_2.coef_.shape)  # (nclasses*(nclasses-1)/2, ncoefs)"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "En efecto, la forma de la funci\u00f3n de decisi\u00f3n no cambia como aprende. Eso es siempre 'ovo'. El orden es de 0 a K-1 con \u201c0 vs 1\u201d, \u201c0 vs 2\u201d , \u2026 \u201c0 vs K-1\u201d, \u201c1 vs 2\u201d, \u201c1 vs 3\u201d, \u201c1 vs K-1\u201d, . . . \u201cK-2 vs K-1\u201d"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(np.allclose(svc_1.coef_, svc_2.coef_))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "print(lsvc_1.decision_function(X[:1]), len(lsvc_1.decision_function(X[:1])[0]))\n",
    "print(lsvc_1.predict(X[:1]))\n",
    "print(svc_1.decision_function(X[:1]), len(svc_1.decision_function(X[:1])[0]))\n",
    "print(svc_1.predict(X[:1]))\n",
    "print(svc_2.decision_function(X[:1]), len(svc_2.decision_function(X[:1])[0]))\n",
    "print(svc_2.predict(X[:1]))"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = lsvc_1.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = svc_1.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.scatter(\n",
    "    svc_1.support_vectors_[:, 0],\n",
    "    svc_1.support_vectors_[:, 1],\n",
    "    marker=\"+\",\n",
    "    color=\"black\",\n",
    "    label=\"SV\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {}
  },
  {
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = svc_2.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.scatter(\n",
    "    svc_2.support_vectors_[:, 0],\n",
    "    svc_2.support_vectors_[:, 1],\n",
    "    marker=\"+\",\n",
    "    color=\"black\",\n",
    "    label=\"SV\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   }
  }
 ]
}