{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Introduction_to_Neural_Networks"
   },
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Rikab Gambhir (Center for Theoretical Physics, MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore Neural Networks, the fundamental building block of deep learning. We will go into the very basics of the theory of Neural Networks and Universal Function Approximation. Then, we will explore practical immplementations of Neural Networks and deep learning that are widely used both in physics applications and also are widespread in industry.\n",
    "\n",
    "This tutorial is divided into 4 parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Neural Network Basics**: Constructing multi-layer perceptrons and studying universal function approximation.\n",
    "2. **JAX**: An increasingly popular library used for machine learning. This library is extremeley similar to basic numpy, but has extra features like autodifferentiation and compilation that make it useful for machine learning.\n",
    "3. **PyTorch**: A commonly used ML library. Developed by Meta. Especially nice for implementing fancy modern ML models, since they're mostly developed in PyTorch anyways!\n",
    "4. **Tensorflow**: Less common in 2025, but many ML tools still use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**\n",
    "\n",
    "I will assume knowledge of the following:\n",
    "1. Basic python and numpy. You should be comfortable with matrix operations within numpy, dealing with lists and loops, defining functions, and classes.\n",
    "2. You are familiar with the previous tutorials on regression, classification, normalizing flows, and unsupervised learning. In particular you should appreciate the idea of finding parameters that minimize the log-likelhood (or other metrics) for function fitting, and the general importance of finding/optimizing for functions for statistical tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Prelude:_CPUs_vs_GPUs"
   },
   "source": [
    "# Prelude: CPUs vs GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorials here can be relatively computationally intensive, especially when we start training neural networks. The tutorials you have seen thus far have been relatively light, and you could run them on your laptop or on default Google Colab settings.\n",
    "\n",
    "However, for these tutorials, I recommend trying the GPU as well as the ordinary CPU. Google Colab provides free access to GPUs, which can significantly speed up the training of neural networks and other machine learning models. You can do this by going to the \"Runtime\" tab in Google Colab and selecting \"Change runtime type\" in the upper-right corner (next to the RAM icon). Then, select \"GPU\" as the hardware accelerator. It is possible to run these tutorials on a CPU, but it will be very slow, especially for the later tutorials.\n",
    "\n",
    "Most ML is designed to take advantage of highly-parallelizable tasks and linear algebra, where operations are simple and can be done in parallel. GPUS are designed with many smaller cores that can do many simple operations in parallel, which is why they are so useful for ML. Whereas CPUs have only a few cores that are designed to do more complex operations, but not as many in parallel.\n",
    "\n",
    "\n",
    "\n",
    "When running these tutorials, you may get an error telling you have run out of memory, especially on GPU. This is because Colab has a limited amount of memory available for each session. If you encounter this error, you can try restarting the runtime (Runtime -> Restart runtime) or clearing the output of the cells (Edit -> Clear all outputs), and then only re-running the cells of interest. If you still run into memory issues, you may need to reduce the size of the models or the batch size during training, or come up with some other clever solutions (like batching!). This is a very realistic issue that you will encounter in real ML applications, so consider running into this issue part of the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_1:_Neural_Network_Basics"
   },
   "source": [
    "# Chapter 1: Neural Network Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials, e.g. [regression](https://colab.research.google.com/github/mcgen-ct/tutorials/blob/main/ml/regression.ipynb), the goal was to model a fixed functional form $f(x)$ where $f$ depended on some parameters $\\theta$. For example, a linear fit of the form $f(x) = \\theta_0 + \\theta_1 x$.\n",
    "\n",
    "In Deep Learning, we want to be more ambitious. We do not want to assume a specific functional form: rather than only ``searching'' over a fixed set of basis functions, we want to search over *all* functions, or at least a very large class of functions. Our strategy for doing this is to take a functional form with an extremeley large set of parameters, such that in the infinite parameter limit all functions of a particular class fit within the parameterization. For example. the set of functions:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{i=0}^N \\theta_i x^i\n",
    "\\end{align}\n",
    "\n",
    "models all one-dimension analytic functions as $N \\to \\infty$. However, we would like a more general parameterization that can work for many dimensions and even model non-smooth (or even non-continuous) functions arbtirarily well.\n",
    "\n",
    "A **Neural Network (NN)**  (also known as a **Multilayer Perceptron (MLP)** a **feedforward network**, or a **Dense Neural Network (DNN)** depending on the context) parameterizes *all* peicewise-continuous functions from $\\mathbb{R}^{n} \\to \\mathbb{R}^m$ arbitrarily well with a very simple parameterization.\n",
    "\n",
    "\n",
    "To define a neural network, we first specify $L-2$ integers $N_1, ..., N_{L-1}$. Just for notation, choose $N_0 = n$ as the input dimension, and $N_L = m$ as the output dimension. $L$ is referred to as the *depth* of the network (or number of layers), and the $N$'s are the *width* of each layer. Unless you are doing something fancy (e.g. autoencoders), it is typical to choose $N$ to all be the same.\n",
    "\n",
    "Then, we define a set of *layer functions*, which are maps $f^{\\ell}:\\mathbb{R}^{N_{\\ell-1}}\\to\\mathbb{R}^{N_{\\ell}}$, as:\n",
    "\n",
    "$$ f^{\\ell}(x) = \\sigma(W^{(\\ell)}x + b^{(\\ell)})$$\n",
    "\n",
    "where $W^{(\\ell)} \\in \\mathbb{R}^{N_{\\ell} \\times N_{\\ell -1}}$ and $b^{(\\ell)} \\in \\mathbb{R}^{N_{\\ell}}$ are the parameters that define the layer, and $\\sigma$ is some pre-determined nonlinear transformation. This can differ between layers, but it is common to chose $\\sigma$ to be the same for every layer except the last, where $\\sigma$ is often instead chosen such that its image matches the desired output space. An extremeley common and simple chose for $\\sigma$ is the ReLU (Rectified Linear Unit) function, which we will use throuhout the rest of this tutorial:\n",
    "\n",
    "$$ \\sigma(x) = \\max(0, x)$$\n",
    "\n",
    "Then, the full neural network is defined by:\n",
    "\n",
    "$$ f = f^{L} \\cdot f^{L-1} \\cdot ... \\cdot f^{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an MLP from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Neural Network from Scratch #\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "L = 3\n",
    "N = 16  # We will use the same N throughout for simplicity\n",
    "\n",
    "\n",
    "# Function to initialize the W's and b's\n",
    "# For now, lets just pick random numbers!\n",
    "def init_params(input_dim, output_dim, L, N):\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            W = np.random.randn(N, input_dim) / np.sqrt(input_dim)\n",
    "            b = np.random.randn(N) / np.sqrt(input_dim)\n",
    "            # The sqrt(input_dim) normalization is not important for our toy examples, but it is common to do for stability reasons\n",
    "\n",
    "        elif l == L - 1:\n",
    "            W = np.random.randn(output_dim, N) / np.sqrt(N)\n",
    "            b = np.random.randn(output_dim) / np.sqrt(N)\n",
    "\n",
    "        else:\n",
    "            W = np.random.randn(N, N) / np.sqrt(N)\n",
    "            b = np.random.randn(N) / np.sqrt(N)\n",
    "\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "\n",
    "    return Ws, bs\n",
    "\n",
    "\n",
    "# Implement the ReLU function\n",
    "def sigma(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a neural network given x, the weights W, and the biases b\n",
    "\n",
    "\n",
    "def MLP(x, Ws, bs):\n",
    "    y = x.copy()\n",
    "\n",
    "    for l in range(L):\n",
    "        # Fun python fact: \"@\" implements matrix multiplication!\n",
    "        y = Ws[l] @ y + bs[l]\n",
    "\n",
    "        # Don't apply sigma to the final output so that our answer isn't forced positive\n",
    "        if l != L - 1:\n",
    "            y = sigma(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our MLP function by graphing the function f:R2 -> R1\n",
    "\n",
    "# Define some test points in R2\n",
    "xs1 = np.linspace(-1, 1, 100)\n",
    "xs2 = np.linspace(-1, 1, 100)\n",
    "\n",
    "xs1, xs2 = np.meshgrid(xs1, xs2)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "Ws, bs = init_params(input_dim, output_dim, L, N)\n",
    "\n",
    "ys = []\n",
    "for x in zip(xs1.flatten(), xs2.flatten()):\n",
    "    x = np.array(x)\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "\n",
    "ys = np.array(ys)\n",
    "ys = ys.reshape(xs1.shape)\n",
    "\n",
    "# 3d plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(xs1, xs2, ys)\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"y = MLP(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on functional vs. object-oriented programming**\n",
    "\n",
    "In the above code, we defined our MLP purely using python functions. There is no neural network \"object\" with an internal state keeping track of the parameters. Instead, the parameters $W$ and $b$ are also treated as inputs to functions. This is *functional programming*, in which there are no objects with internal states that get modified. This is the approach to ML used by JAX.\n",
    "\n",
    "It is also possible to define an MLP *class*, which is an object that contains the parameters as internal states that can potentially be modified, and methods that implement the model and evaluate $f(x)$. This is the approach to ML used by PyTorch and Tensorflow.\n",
    "\n",
    "It is largely a matter of programming taste which you prefer. Below, we will see a brief example of the above code, but written in an object-oriented style rather than functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MLP_Class:\n",
    "    def __init__(self, input_dim, output_dim, L, N):\n",
    "        # Initialize the network arguments\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.L = L\n",
    "        self.N = N\n",
    "\n",
    "        # Initialize the network internal state using the same initi function\n",
    "        self.Ws, self.bs = init_params(input_dim, output_dim, L, N)\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        # Just use the same exact function as above\n",
    "        return MLP(x, self.Ws, self.bs)\n",
    "\n",
    "    # \"Magic Method\" that lets us call the class as if it were a function (just syntatic magic)\n",
    "    def __call__(self, x):\n",
    "        return self.evaluate(x)\n",
    "\n",
    "\n",
    "my_MLP = My_MLP_Class(input_dim, output_dim, L, N)\n",
    "\n",
    "# Access the weights\n",
    "my_weights = my_MLP.Ws\n",
    "print(\"The number of layers is \", len(my_weights), \",Expected 3\")\n",
    "\n",
    "# Evaluate the function\n",
    "print(\"f(1,1) = \", my_MLP(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historical Notes and Semantics**\n",
    "\n",
    "The case where $L = 2$ (no ``hidden layers'' between the input and output) with the output dimensionality is $1$ is called a perceptron historically. These were introduced with $\\sigma$ not as ReLU, but rather:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "(the sigmoid function, hence the notation), and were used back in the day as a model of a biological neuron. The neuron \"activates\" (produces $1$) when $x$ is large, and \"deactivates\" (produces $0$ when $x$ is small, where $b$ is then a bias. For this reason, $\\sigma$ is called an activation function. This is also why our models are called \"Neural Networks\". The \"network\" is because the parameters of the weight matrix $w_{ij}$ are drawn as lines connecting a node $i$ in the previous layer to a node $j$ in the next layer. It's important to remember though, that these are just affine transformations interleaved by some simple nonlinear functions, and there isn't really anything magic here, just slightly-nonlinear algebra.\n",
    "\n",
    "The name \"feedforward\" network just refers to the function-compositional aspect of the model. It is to be contrasted with a \"backwards pass\", where derivatives with respect to the network are actually computed in reverse-order due to chain-rule simplifications. The name \"dense\" neural network is to emphasize that this is the simplest possible network one can build. There are many modern models with additional properties (such as gauranteeing symmetries, or working in spaces other than simple vector spaces, or deliberately constraining the function space), but many of these can be reduced to very large MLPs with constrained weights. When we say \"dense\" or \"fully-conencted\" MLPs, we typically mean there are no constraints on the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_1.1:_Universal_Function_Approximation"
   },
   "source": [
    "## Chapter 1.1: Universal Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of MLPs is that they are an efficient way to parameterize a large class of functions. This is captured by the **Universal Function Approximation Theorem(s) (UFAT)** (there are lots of variants, but at the level of rigor we are working at, we won't worry about this).\n",
    "\n",
    "**Emotionally**, the UFAT tells us that for sufficiently large $N$ and $L$, an MLP can approximate any (reasonable) $n$-to-$m$ dimensional function arbitrarily well.\n",
    "\n",
    "**Slightly more precisely**, a version of UFAT says: For any piecewise-continuous function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ defined on a compact domain $D \\subset \\mathbb{R}^n$, and for any \"error tolerance\" $\\epsilon  > 0$, there exists a large enough $N$ and $L$ such that one can define an MLP with specially-chosen parameters $W$ and $b$ such that:\n",
    "$$ \\int_D dx |f(x) - MLP(x)| < \\epsilon $$\n",
    "i.e. that we have approximated the function to within the specified error.\n",
    "\n",
    "[Side note: It is actually always possible to do this with just $L$ = 3 (meaning just one hidden layer with chosen $N$ in our defined $L$ counting), but typically this requires an exponentially large $N$ and isn't of practical use for what we will be doing].\n",
    "\n",
    "\n",
    "We will not prove the UFAT. However, we will explore a weaker-version of it that is easier to understand: If instead we explore continuous-and-piecewise-once-differentiable functions rather than just piecewise-continuous, then there is an easy construction using ReLU networks. If a function is piecewise-once-differentiable, then it can be well-approximated by a piecewise-linear function. We will see below (as exercises) how ReLU networks can exactly reproduce piecewise linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Modeling_|x|"
   },
   "source": [
    "#### Exercise: Modeling |x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x) = |x|$ in 1 dimension, design an MLP with a choice of N, L, weights W, and biases b that *exactly* match $f(x)$.\n",
    "\n",
    "HINT: It is possible to do this with $L = 2$ (one hidden layer) and $N = 2$.\n",
    "\n",
    "HINT 2: It is possible to do this with $b = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "def f(x):\n",
    "    return np.abs(x)\n",
    "\n",
    "\n",
    "L = 0  # YOUR SOLUTION HERE\n",
    "N = 0  # YOUR SOLUTION HERE\n",
    "\n",
    "Ws = []  # YOUR SOLUTION HERE\n",
    "bs = []  # YOUR SOLUTION HERE\n",
    "\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "\n",
    "# Evaluate the solution\n",
    "ys = []\n",
    "for x in xs:\n",
    "    x = np.array([x])\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "\n",
    "ys = np.array(ys)\n",
    "\n",
    "# Plot\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, ys, label=\"MLP(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_SOLUTION\n",
    "def f(x):\n",
    "    return np.abs(x)\n",
    "\n",
    "\n",
    "L = 2\n",
    "N = 2\n",
    "\n",
    "W0 = np.array([[1.0], [-1.0]])  # hidden unit 1:  +x  # hidden unit 2:  \u2013x\n",
    "b0 = np.array([0.0, 0.0])  # no shift\n",
    "\n",
    "W1 = np.array([[1.0, 1.0]])  # add the two ReLU outputs\n",
    "b1 = np.array(\n",
    "    [\n",
    "        0.0,\n",
    "    ]\n",
    ")  # no shift\n",
    "\n",
    "Ws = [W0, W1]\n",
    "bs = [b0, b1]\n",
    "\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "\n",
    "# Evaluate the solution\n",
    "ys = []\n",
    "for x in xs:\n",
    "    x = np.array([x])\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "\n",
    "ys = np.array(ys)\n",
    "\n",
    "# Plot\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, ys, label=\"MLP(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Approximating_a_smooth_1D_function."
   },
   "source": [
    "### Exercise: Approximating a smooth 1D function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x) = \\sin(10x) \\exp(-2x^2)$ on the interval $[-1,1]$, design an MLP with ReLU-activations that approximates the function to within an error of $\\epsilon < 0.01$ (where error is the mean-absolute error, as defined above). As a bonus, your implementation should be systematically improvable, e.g. it should be straightforward to make the MLP bigger to reduce the error further. Don't cheat and use minimization to get the parameters, explicitly construct them!\n",
    "\n",
    "\n",
    "HINT: First construct a continuous piecewise linear appoximation to the function, then implement this piecewise linear function as an MLP. It is possible to do this without knowledge of the actual form of $f$.\n",
    "\n",
    "HINT 2: This is possible to do systematically with $L = 2$ as before, but with a very large $N$. My personal solution requires $N$ between 100 and 150.\n",
    "\n",
    "HINT 3: A piecewise-linear continuous function can be written as $f(x) = c_0 + m_0x + + \\sum_{j = 1}^{n-1}(m_j - m_{j-1})\\sigma(x - x_j)$, where $\\sigma$ is ReLU, $x_{1}...x_{n-1}$ are the internal breakpoints, $m_j$ are the slopes to the right of each breakpoint, and $c_0$ is the $y$-coordinate at the leftmost point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "def f(x):\n",
    "    return np.sin(10 * x) * np.exp(-2 * x**2)\n",
    "\n",
    "\n",
    "L = 0  # YOUR SOLUTION HERE\n",
    "N = 0  # YOUR SOLUTION HERE\n",
    "Ws = (\n",
    "    []\n",
    ")  # YOUR SOLUTION HERE (hint, define a function to construct Ws, bs, systematically)\n",
    "bs = []  # YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "xs = np.linspace(-1, 1, 1000)\n",
    "\n",
    "ys = []\n",
    "for x in xs:\n",
    "    x = np.array([x])\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "ys = np.array(ys)\n",
    "\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, ys, label=\"MLP(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Approximate the mean absolute error\n",
    "print(\n",
    "    \"Mean absolute error: \", np.mean(np.abs(ys[:, 0] - f(xs)))\n",
    ")  # [:,0] due to annoying indexing\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_SOLUTION\n",
    "# For any function f, define a piecewise-linear approximation with n breakpoints.\n",
    "def piecewise_linear_params(n, f):\n",
    "    # Breakpoints of the piecewise linear approximation\n",
    "    xk = np.linspace(-1.0, 1.0, n + 1)  # x0, \u2026, xn\n",
    "    yk = f(xk)\n",
    "\n",
    "    # Approximate the slopes numerically (technically, possible exactly)\n",
    "    mk = np.diff(yk) / np.diff(xk)  # m0 \u2026 m_{n-1}\n",
    "\n",
    "    # slope jumps delta_m_j at each breakpoint\n",
    "    dm = mk[1:] - mk[:-1]\n",
    "\n",
    "    # constant term that glues the first segment to y(-1)\n",
    "    c0 = yk[0] - mk[0] * xk[0]\n",
    "\n",
    "    # Initialize weights\n",
    "    N = n + 1  # annoying indexing\n",
    "    W0 = np.ones((N, 1))\n",
    "    b0 = np.zeros(N)\n",
    "\n",
    "    # Add and subtract a ReLU, like the |x| example\n",
    "    W0[0, 0] = 1.0\n",
    "    b0[0] = 0.0\n",
    "    W0[1, 0] = -1.0\n",
    "    b0[1] = 0.0\n",
    "\n",
    "    # interior break-points\n",
    "    for j, t_j in enumerate(xk[1:-1], start=2):  # j = 2 \u2026 n\n",
    "        W0[j, 0] = 1.0\n",
    "        b0[j] = -t_j  # shift to next break point\n",
    "\n",
    "    # Second layer where everything is just 1 and -1 to add and subtract\n",
    "    W1 = np.zeros((1, N))\n",
    "    W1[0, 0] = mk[0]  #  +m0\u00b7ReLU(x)\n",
    "    W1[0, 1] = -mk[0]  #  \u2212m0\u00b7ReLU(\u2212x)\n",
    "\n",
    "    for j, d_m in enumerate(dm, start=2):\n",
    "        W1[0, j] = d_m\n",
    "\n",
    "    b1 = np.array([c0])  # constant offset for leftmost point\n",
    "\n",
    "    Ws = [W0, W1]\n",
    "    bs = [b0, b1]\n",
    "    return Ws, bs\n",
    "\n",
    "\n",
    "num_breakpoints = 100  # Increase for more accuracy!\n",
    "Ws, bs = piecewise_linear_params(num_breakpoints, f)\n",
    "xs = np.linspace(-1, 1, 1000)\n",
    "\n",
    "ys = []\n",
    "for x in xs:\n",
    "    x = np.array([x])\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "ys = np.array(ys)\n",
    "\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, ys, label=\"MLP(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Mean absolute error: \", np.mean(np.abs(ys[:, 0] - f(xs))))\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_1.2:_Functional_Optimization"
   },
   "source": [
    "## Chapter 1.2: Functional Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the ability to approximate function spaces with MLP's! The fun part of Machine Learning (the Learning) comes in when we can phrase problems as *functional optimization* problems:\n",
    "\n",
    "\"Out of all the (reasonably nice) functions from $\\mathbb{R}^n \\to \\mathbb{R}^m$ , which function $f$ minimizes the loss functional $L[f]$?\"\n",
    "\n",
    "Almost every interesting problem in life, statistics, and physics can be phrased this way. In fact, this is completely identical to Lagrangian mechanics, in the case that $L[f]$ can be written as the integral of a local Lagrangian. In simple cases (ordinary classical mechanics) this functional optimization can be performed using the Euler-Lagrange equations. But in many cases (e.g. where $L[f]$ is written as a sum rather than an integral so EL does not apply, or we can't solve the EL equations, etc), we must settle for numerics.\n",
    "\n",
    "You have seen in previous tutorials how many statistics problems (e.g. regression, classification, and density estimation) can be seen as functional optimization. In those examples, there were only a few parameters defining the function space: now there are *many* parameters and our function space is as close to the space of all possible functions as possible. We can no longer just use a simple parameter minimizer in this case.\n",
    "\n",
    "The strategy will be **gradient descent**. If we have an estimate of:\n",
    "$$\\nabla_{\\theta}L[f]$$\n",
    "then by simply moving $\\theta$ in the opposite direction of the gradient, we will move towards a local minimum. The process of iterating this is called **training**, and each iteration is called an **epoch**. In statistical settings, where $L[f]$ is some statistical measure (like in the regression examples), this training requires data to obtain statistical estimates of $\\nabla_{\\theta}L[f]$, hence the need for **training data**. There are many variants of gradient descent that work on the same principle but have varying numerical properties, like stochastic gradient descent and ADAM, but we will not dive deeper into these here.\n",
    "\n",
    "In principle, if we know $L$ (which we usually do, because it is typically part of the problem specification), we can explictly construct $\\nabla_{\\theta}L[f]$ exactly, since we know how $f$ depends on our parameters $\\theta = (W, b)$. However, it is still painful to manually construct. This is where libraries like **JAX**, **PyTorch**, and **Tensorflow** come in. These libraries are capable of **autodifferentiation**: computations are kept track of in a graph structure, so that gradients can be easily and exactly computed alongside the execution of the function. Exploring this further will be the subject of another tutorial, for now we wil take it for granted that these libraries can perform autodifferentiation.\n",
    "\n",
    "We have reached the limit of what we can practically do without the use of libraries in a reasonable amount of time. Now, we will explore how to use these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Interlude:_The_problems_we_will_solve:"
   },
   "source": [
    "# Interlude: The problems we will solve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be interested in using Neural Networks to solve classification problems. We have previously seen how to do this with logistic regression and cross-entropies in the [classification tutorial](https://colab.research.google.com/github/mcgen-ct/tutorials/blob/main/ml/classification.ipynb). We will now see how to do this with Neural Networks.\n",
    "\n",
    "We will have two problems: an easy problem, and a hard problem. The easy problem is \"Two Moons\", a classic ML test case in 2 dimensions. The hard problem is the \"MNIST\" dataset, a dataset of handwritten digits that is commonly used to test classification algorithms. This is a 28x28 pixel image dataset, so the input dimension is 784, so MLPs can really shine compared to classical fixed-form regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Moons Dataset\n",
    "\n",
    "X_moons, y_moons = make_moons(1024, noise=0.15, random_state=0)\n",
    "X_moons = StandardScaler().fit_transform(X_moons)  # Just normalizing the data\n",
    "\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], \"ro\", label=\"Class 0\")\n",
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], \"bo\", label=\"Class 1\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X_mnist = mnist.data / 255.0  # Normalize pixel values to [0, 1]\n",
    "y_mnist = mnist.target.astype(int)  # Convert target to integers\n",
    "\n",
    "# The MNIST input dimension is 784, but we can visualize it as 28x28 images\n",
    "plt.imshow(X_mnist[y_mnist == 7][0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Example MNIST Digit (7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2:_JAX"
   },
   "source": [
    "# Chapter 2: JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is a library developed by Google that is very similar to numpy, but with some extra features that make it useful for machine learning. It is based on the idea of functional programming, where functions are first-class citizens and can be passed around like any other object. JAX is particularly useful for machine learning because it has built-in support for autodifferentiation, which allows us to compute gradients of functions with respect to their inputs.\n",
    "\n",
    "Compared to PyTorch and Tensorflow, JAX is more low-level and requires more manual work to set up. However, it is also more flexible and allows for more control over computations. JAX is particularly useful for research and experimentation, where you want to try out new ideas quickly without having to worry about the details of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.1:_JAX_basics;_vmapping,_autodifferentiation,_and_compilation."
   },
   "source": [
    "## Chapter 2.1: JAX basics; vmapping, autodifferentiation, and compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has three useful features that we should be aquainted with:\n",
    "\n",
    "\n",
    "1. Vmapping: We can write a function acting on a single variable, and then execute that function on an entire list at once without using loops. In fact, this is much faster than looping (in Python), since Python loops must wait for the previous iteration to finish. Note that numpy can technically do this too, but it becomes especially important in JAX\n",
    "2. Just-In-Time compilation (JIT): Python is a scripted language, meaning lines of code are carried out as your computer sees them. In compiled languages, the computer looks at the entire program, translates to machine code (compilation), then executes. You pay an up-front time cost for the initial compilation, but every subsequent execution is much faster since the machine code is typically highly optimized. JIT allows us to pre-compile functions in Python. The cost is that we have to be a little be conscious of things like memory, and we cannot use things like ordinary if-statements or for-loops.\n",
    "3. Autodifferentiation: If we write a function in JAX, we can automatically compute its exact derivative. We don't have to manually compute it ourselves! This even works with multi-variate functions, functions that are highly-composed and require lots of chain-ruling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "def theta(x):\n",
    "    return x > 0\n",
    "\n",
    "\n",
    "def f(x, a):\n",
    "    return a * (1 - jnp.cos(x)) * theta(x)\n",
    "\n",
    "\n",
    "a = 3.0\n",
    "xs = jnp.linspace(-2 * jnp.pi, 2 * jnp.pi, 10000)\n",
    "\n",
    "# Vmapping time save test:\n",
    "start_time = time.time()\n",
    "ys = []\n",
    "for x in xs:\n",
    "    ys.append(f(x, a))\n",
    "ys = jnp.array(ys)\n",
    "end_time = time.time()\n",
    "print(\"Loop time: \", end_time - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# vmap(f) is a new function with the same signature as f.\n",
    "# vmap(f, in_axes = (0, None)) means we only want to vectorize over the first argument (x), not the second.\n",
    "\n",
    "ys = vmap(f, in_axes=(0, None))(xs, a)\n",
    "end_time = time.time()\n",
    "print(\"Vmap time: \", end_time - start_time)\n",
    "\n",
    "\n",
    "# Compile the function\n",
    "start_time = time.time()\n",
    "f_jit = jit(f)\n",
    "f_jit(\n",
    "    0, a\n",
    ").block_until_ready()  # Need to run the compiled function once to compile \"just in time\"\n",
    "end_time = time.time()\n",
    "print(\"Compilation time: \", end_time - start_time)\n",
    "# Note that compilation is NOT always faster, especially for only simple functions.\n",
    "# Also machine-dependent!\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "ys = []\n",
    "for x in xs:\n",
    "    ys.append(f_jit(x, a))\n",
    "ys = jnp.array(ys)\n",
    "end_time = time.time()\n",
    "print(\"JIT loop time: \", end_time - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "ys = vmap(f_jit, in_axes=(0, None))(xs, a)\n",
    "end_time = time.time()\n",
    "print(\"JIT vmap time: \", end_time - start_time)\n",
    "\n",
    "\n",
    "# Get the exact gradient with respect to x\n",
    "f_prime = jax.grad(f, argnums=0)  # Argnums is the argument we want the gradient of.\n",
    "f_prime_jit = jit(f_prime)\n",
    "f_prime_jit(\n",
    "    0.0, a\n",
    ").block_until_ready()  # Need to run the compiled function once to compile \"just in time\"\n",
    "\n",
    "# Get the exact gradient with respect to a\n",
    "f_prime_a = jax.grad(f, argnums=1)  # Argnums is the argument we want the gradient of.\n",
    "f_prime_a_jit = jit(f_prime_a)\n",
    "\n",
    "\n",
    "plt.plot(xs, vmap(f_jit, in_axes=(0, None))(xs, a), label=\"f(x)\")\n",
    "plt.plot(xs, vmap(f_prime_jit, in_axes=(0, None))(xs, a), label=\"d_x f(x)\")\n",
    "plt.plot(xs, vmap(f_prime_a_jit, in_axes=(0, None))(xs, a), label=\"d_a f(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Autodifferentiation_Practice"
   },
   "source": [
    "### Exercise: Autodifferentiation Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an *arbitrary* scalar-valued function $f(x)$, which you know is smooth, write a function that computes the Taylor expansion of $f$ around a point $x_0$ to order $n$.\n",
    "\n",
    "Specifically, your function should take as input $f$, $x_0$, and $n$, and return a new function, $f_n$, which is the Taylor expansion of $f$ around $x_0$ to order $n$. The function $f_n$ should take as input a single variable $x$, and return the value of the Taylor expansion at that point.\n",
    "\n",
    "HINT: Construct a list of functions $f_0, f_1, ..., f_n$ where $f_i$ is the $i$-th derivative of $f$, or equivalently, $f_i$ is the single derivative of $f_{i-1}$. Then evaluate this list of functions at $x_0$ to obtain the coefficients of the Taylor expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "def f(x):\n",
    "    # EXAMPLE FUNCTION\n",
    "    return jnp.sin(x)\n",
    "\n",
    "\n",
    "def build_taylor_series(f, x0, n):\n",
    "    # YOUR SOLUTION HERE\n",
    "\n",
    "    # Define a function representing the solution we want to return\n",
    "    def taylor_series(x):\n",
    "        # YOUR SOLUTION HERE\n",
    "        return 0\n",
    "\n",
    "    return taylor_series\n",
    "\n",
    "\n",
    "# Test the solution\n",
    "x0 = 0.0\n",
    "n = 5\n",
    "taylor_series = build_taylor_series(f, x0, n)\n",
    "xs = jnp.linspace(-2 * jnp.pi, 2 * jnp.pi, 10000)\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, vmap(taylor_series, in_axes=0)(xs), label=\"Taylor Series\")\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_SOLUTION\n",
    "def f(x):\n",
    "    # EXAMPLE FUNCTION\n",
    "    return jnp.sin(x)\n",
    "\n",
    "\n",
    "def build_taylor_series(f, x0, n):\n",
    "    # Compute the derivatives at x0\n",
    "    derivative_functions = [f]\n",
    "    derivatives_at_x0 = [f(x0)]\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        derivative_functions.append(grad(derivative_functions[-1], argnums=0))\n",
    "        derivatives_at_x0.append(derivative_functions[-1](x0))\n",
    "\n",
    "    # Define the Taylor series expansion\n",
    "    def taylor_series(x):\n",
    "        series = 0.0\n",
    "        for i in range(n + 1):\n",
    "            series += derivatives_at_x0[i] * (x - x0) ** i / math.factorial(i)\n",
    "        return series\n",
    "\n",
    "    return taylor_series\n",
    "\n",
    "\n",
    "# Test the solution\n",
    "x0 = 0.0\n",
    "n = 5\n",
    "xs = jnp.linspace(-2 * jnp.pi, 2 * jnp.pi, 10000)\n",
    "plt.plot(xs, f(xs), label=r\"f(x)\", color=\"black\", lw=2)\n",
    "\n",
    "for i in range(n + 1):\n",
    "    taylor_series = build_taylor_series(f, x0, i)\n",
    "    plt.plot(xs, vmap(taylor_series, in_axes=0)(xs), label=\"f_{}(x)\".format(i))\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.2:_End-to-End_MLP_and_Training_from_Scratch"
   },
   "source": [
    "## Chapter 2.2: End-to-End MLP and Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reproduce the MLP we defined above, but now using JAX. We will use the functional programming style, so we will define our MLP as a function that takes in the parameters $W$ and $b$ as inputs.\n",
    "\n",
    "A small difference is that rather than having W and b as separate inputs, we will combine them into a single input called `params`. This is just for convenience to make taking gradients cleaner. Also, since we are interested in classification, we will use a sigmoid activation for the last layer in the binary classification case to ensure the output is between 0 and 1, which is useful for classification tasks, and use a softmax for the multiclass case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX MLP. Basically identical to the above numpy code.\n",
    "def MLP_jax(x, params):\n",
    "    y = x\n",
    "    Ws, bs = params\n",
    "\n",
    "    for l in range(len(Ws) - 1):\n",
    "        y = jnp.dot(Ws[l], y) + bs[l]\n",
    "        y = relu(y)\n",
    "\n",
    "    y = jnp.dot(Ws[-1], y) + bs[-1]  # No activation on the last layer\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# Our classifier will be an MLP with a sigmoid at the end. Change to softmax for multi-class classification.\n",
    "def classifier(x, params):\n",
    "    # Its common to call the input to the sigmoid the \"logits\". But really its the log-likelihood ratio of the classes.\n",
    "    logits = MLP_jax(x, params)\n",
    "    return 1 / (1 + jnp.exp(-logits))\n",
    "\n",
    "\n",
    "# Gradient with respect to the parameters\n",
    "classifier_grad = grad(classifier, argnums=1)\n",
    "\n",
    "\n",
    "# Initialize the parameters for the MLP\n",
    "def init_params_jax(input_dim, output_dim, L, N):\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            # Basically the same as the numpy version, but using JAX's random module.\n",
    "            # Unlike numpy, JAX's random module is functional and requires a PRNG key.\n",
    "            W = random.normal(random.PRNGKey(l), (N, input_dim)) / jnp.sqrt(input_dim)\n",
    "            b = random.normal(random.PRNGKey(l + 1), (N,)) / jnp.sqrt(input_dim)\n",
    "        elif l == L - 1:\n",
    "            W = random.normal(random.PRNGKey(l), (output_dim, N)) / jnp.sqrt(N)\n",
    "            b = random.normal(random.PRNGKey(l + 1), (output_dim,))\n",
    "        else:\n",
    "            W = random.normal(random.PRNGKey(l), (N, N)) / jnp.sqrt(N)\n",
    "            b = random.normal(random.PRNGKey(l + 1), (N,))\n",
    "\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "\n",
    "    return Ws, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test just to make sure it works\n",
    "\n",
    "N = 16\n",
    "L = 3\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "xs1 = jnp.linspace(-1, 1, 100)\n",
    "xs2 = jnp.linspace(-1, 1, 100)\n",
    "\n",
    "xs1, xs2 = jnp.meshgrid(xs1, xs2)\n",
    "xs = jnp.array(list(zip(xs1.flatten(), xs2.flatten())))\n",
    "print(\"Shape of xs: \", xs.shape)  # Should be (10000, 2)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "Ws, bs = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "# VMAP our model. We can also JIT it, but its actually better to JIT the entire training step function later.\n",
    "vmapped_classifier = vmap(classifier, in_axes=(0, None))\n",
    "vmapped_classifier_grads = vmap(classifier_grad, in_axes=(0, None))\n",
    "\n",
    "ys = vmapped_classifier(xs, (Ws, bs))\n",
    "ys = ys.reshape(xs1.shape)\n",
    "\n",
    "# 3d plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(xs1, xs2, ys)\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"y = MLP(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the training. We want to minimize the cross-entropy loss function, which is defined as:\n",
    "$$ L[f] = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(f(x_i)) + (1 - y_i) \\log(1 - f(x_i)) $$\n",
    "\n",
    "where $y_i$ is the true label for the $i$-th data point, and $f(x_i)$ is the model score for the $i$-th data point.\n",
    "\n",
    "We will use JAX's autodifferentiation to compute the gradient of the loss function with respect to the parameters, and then use gradient descent to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BCE loss\n",
    "def bce_loss(x, y_true, params):\n",
    "    y_pred = vmapped_classifier(x, params).squeeze()\n",
    "    epsilon = 1e-10  # Small value to avoid log(0), just in case\n",
    "    return -jnp.mean(\n",
    "        y_true * jnp.log(y_pred + epsilon)\n",
    "        + (1 - y_true) * jnp.log(1 - y_pred + epsilon)\n",
    "    )\n",
    "\n",
    "\n",
    "grad_bce_loss = grad(bce_loss, argnums=2)  # Gradient with respect to the parameters\n",
    "\n",
    "\n",
    "# Gradient step function to update the parameters\n",
    "@jit  # <- Inline way to JIT the function\n",
    "def gradient_step(x, y_true, params, learning_rate=0.01):\n",
    "    # Compute the loss gradients (the loss itself is not needed, just the gradients)\n",
    "    loss = bce_loss(x, y_true, params)  # But we will compute it anyways for logging\n",
    "    grads = grad_bce_loss(x, y_true, params)\n",
    "\n",
    "    # Update the parameters using gradient descent\n",
    "    Ws, bs = params\n",
    "    dWs, dbs = grads\n",
    "    new_Ws = [W - learning_rate * dW for W, dW in zip(Ws, dWs)]\n",
    "    new_bs = [b - learning_rate * db for b, db in zip(bs, dbs)]\n",
    "    new_params = (new_Ws, new_bs)\n",
    "\n",
    "    return new_params, loss\n",
    "\n",
    "\n",
    "# ##### Train the model on the Two Moons dataset #####\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "train_fraction = 0.8\n",
    "test_fraction = 1 - train_fraction\n",
    "epochs = 2000\n",
    "learning_rate = 0.1\n",
    "N = 8  # Its nice to choose N ~ 2^n, since differences in width are only really important on log scale\n",
    "L = 3\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(train_fraction * len(X_moons))\n",
    "\n",
    "# Ensure the dataset is shuffled before splitting\n",
    "# [Just for fun, try removing this part and see how badly things get ruined]\n",
    "indices = np.random.permutation(len(X_moons))\n",
    "X_moons = X_moons[indices]\n",
    "y_moons = y_moons[indices]\n",
    "X_train, y_train = X_moons[:train_size], y_moons[:train_size]\n",
    "X_test, y_test = X_moons[train_size:], y_moons[train_size:]\n",
    "\n",
    "# Initialize the parameters\n",
    "params = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "# Arrays for saving training info\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    # Perform a gradient step\n",
    "    params, loss = gradient_step(X_train, y_train, params, learning_rate)\n",
    "    train_losses.append(loss)\n",
    "    # Evaluate on the test set\n",
    "    test_loss = bce_loss(X_test, y_test, params)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our answers!\n",
    "\n",
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot the decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = X_test[:, 0].min() - 0.1, X_test[:, 0].max() + 0.1\n",
    "y_min, y_max = X_test[:, 1].min() - 0.1, X_test[:, 1].max() + 0.1\n",
    "xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 100), jnp.linspace(y_min, y_max, 100))\n",
    "xs = jnp.array(list(zip(xx.ravel(), yy.ravel())))\n",
    "print(\"Shape of xs for contour plot: \", xs.shape)  # Should be (10000, 2)\n",
    "Z = vmap(classifier, in_axes=(0, None))(xs, params)\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, alpha=0.8, cmap=\"coolwarm\")\n",
    "ax.scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    c=y_test,\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have coded an MLP from scratch and trained it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise_2.1:_Ruining_the_model"
   },
   "source": [
    "### Exercise 2.1: Ruining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will deliberate try to break the model. The goal here is to get a little experience with failure modes.\n",
    "\n",
    "Try to deliberately overfit the model. You have a few knobs to play with: The depth $L$, the width $N$, the learning rate, the number of epochs, and the amount of training data. Try to find a combination of these that overfits the training data, i.e. the training loss is very low but the validation loss is high.\n",
    "\n",
    "Try to see what happens if you forget to shuffle the data! This is a very common mistake. I made this mistake when writing this tutorial, and it took me about 30 minutes to realize why the model was not learning anything.\n",
    "\n",
    "Try extreme learning rates, like $10^{-6}$ or $10^{2}$.\n",
    "\n",
    "When we loaded in the data a few cells ago, we normalized the data. What happens if the data is not O(1)? Try arranging the data so that the inputs are all $O(10^6)$ or $O(10^{-6})$. Technically, the optimal classifier should be coordinate-invariant, but in practice there can be issues! Try some other coordinate transforms of the data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "# your solution goes here\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###START_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else unchanged, choosing $N \\gtrapprox 32$ and $L \\gtrapprox 8$ will overfit the training data. There are many other options though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise_2.2:_MNIST_with_our_JAX_MLP"
   },
   "source": [
    "### Exercise 2.2: MNIST with our JAX MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the above code to classify MNIST. You will need to change the loss function and the output activation to account for the fact that MNIST is a multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "# Prepare the MNIST Dataset\n",
    "train_fraction = 0.8\n",
    "test_fraction = 1 - train_fraction\n",
    "\n",
    "# Shuffle the dataset before splitting\n",
    "indices = np.random.permutation(len(X_mnist))\n",
    "X_mnist = X_mnist[indices]\n",
    "y_mnist = y_mnist[indices]\n",
    "train_size = int(train_fraction * len(X_mnist))\n",
    "X_train_mnist, y_train_mnist = X_mnist[:train_size], y_mnist[:train_size]\n",
    "X_test_mnist, y_test_mnist = X_mnist[train_size:], y_mnist[train_size:]\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return jax.nn.one_hot(y, num_classes)\n",
    "\n",
    "\n",
    "input_dim = 784  # MNIST images are 28x28 pixels, flattened to 784\n",
    "output_dim = 10  # 10 classes for digits 0-9\n",
    "\n",
    "\n",
    "# --- The rest of your code here ---\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_SOLUTION\n",
    "# Prepare the MNIST Dataset\n",
    "train_fraction = 0.8\n",
    "test_fraction = 1 - train_fraction\n",
    "\n",
    "# Shuffle the dataset before splitting\n",
    "indices = np.random.permutation(len(X_mnist))\n",
    "X_mnist = X_mnist[indices]\n",
    "y_mnist = y_mnist[indices]\n",
    "train_size = int(train_fraction * len(X_mnist))\n",
    "X_train_mnist, y_train_mnist = X_mnist[:train_size], y_mnist[:train_size]\n",
    "X_test_mnist, y_test_mnist = X_mnist[train_size:], y_mnist[train_size:]\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return jax.nn.one_hot(y, num_classes)\n",
    "\n",
    "\n",
    "input_dim = 784  # MNIST images are 28x28 pixels, flattened to 784\n",
    "output_dim = 10  # 10 classes for digits 0-9\n",
    "\n",
    "\n",
    "# YOUR PARAMETERS HERE\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "N = 16\n",
    "L = 3\n",
    "\n",
    "# YOUR MODIFIED CLASSIFIER HERE\n",
    "classifier_mnist = lambda x, params: jax.nn.softmax(MLP_jax(x, params))\n",
    "vmapped_classifier = vmap(classifier_mnist, in_axes=(0, None))\n",
    "\n",
    "\n",
    "# YOUR MULTICLASS LOSS FUNCTION HERE\n",
    "def multiclass_cross_entropy_loss(x, y_true, params):\n",
    "    y_pred = vmapped_classifier(x, params)\n",
    "    epsilon = 1e-10  # Small value to avoid log(0)\n",
    "    return -jnp.mean(jnp.sum(y_true * jnp.log(y_pred + epsilon), axis=1))\n",
    "\n",
    "\n",
    "# YOUR TRAINING LOOP HERE\n",
    "params_mnist = init_params_jax(input_dim, output_dim, L, N)\n",
    "train_losses_mnist = []\n",
    "test_losses_mnist = []\n",
    "\n",
    "\n",
    "@jit\n",
    "def gradient_step_mnist(x, y_true, params, learning_rate=0.01):\n",
    "    loss = multiclass_cross_entropy_loss(x, y_true, params)\n",
    "    grads = grad(multiclass_cross_entropy_loss, argnums=2)(x, y_true, params)\n",
    "\n",
    "    Ws, bs = params\n",
    "    dWs, dbs = grads\n",
    "    new_Ws = [W - learning_rate * dW for W, dW in zip(Ws, dWs)]\n",
    "    new_bs = [b - learning_rate * db for b, db in zip(bs, dbs)]\n",
    "    new_params = (new_Ws, new_bs)\n",
    "\n",
    "    return new_params, loss\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    params_mnist, loss = gradient_step_mnist(\n",
    "        X_train_mnist,\n",
    "        one_hot_encode(y_train_mnist, output_dim),\n",
    "        params_mnist,\n",
    "        learning_rate,\n",
    "    )\n",
    "    train_losses_mnist.append(loss)\n",
    "    test_loss = multiclass_cross_entropy_loss(\n",
    "        X_test_mnist, one_hot_encode(y_test_mnist, output_dim), params_mnist\n",
    "    )\n",
    "    test_losses_mnist.append(test_loss)\n",
    "\n",
    "    # compute the accuracy\n",
    "    y_pred = jnp.argmax(\n",
    "        vmap(classifier_mnist, in_axes=(0, None))(X_test_mnist, params_mnist), axis=1\n",
    "    )\n",
    "    accuracy = jnp.mean(y_pred == y_test_mnist)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses_mnist, label=\"Train Loss\")\n",
    "plt.plot(test_losses_mnist, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot some predictions\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax[i // 5, i % 5].imshow(X_test_mnist[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax[i // 5, i % 5].set_title(\n",
    "        f\"Predicted: {jnp.argmax(classifier_mnist(X_test_mnist[i], params_mnist))}, True: {y_test_mnist[i]}\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "    ax[i // 5, i % 5].axis(\"off\")\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.3_(BONUS):_Solving_ODE's_with_MLPs_and_Autodiff"
   },
   "source": [
    "## Chapter 2.3 (BONUS): Solving ODE's with MLPs and Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore how we can use MLPs to solve ODEs. The idea is to represent our solution $f(x)$ as an MLP, and write the ODE as the minimum of a loss function. We can use autodiff to compute the derivatives of $f(x)$, and then use gradient descent to minimize the loss function. Unlike what we did above, there is no training (this is not a statistical problem). Also, we are now dealing with gradients of the function within respect to the input AND with respect to the model parameters.\n",
    "\n",
    "Suppose we have an ODE of the form $F(f, f', x) = 0$, where $f$ is the function we want to solve for, $f'$ is the derivative of $f$ with respect to $x$, and $F$ is some function that defines the ODE. We can define a loss function as:\n",
    "$$ L[f] = \\int dx |F(f, f', x)|^2 $$\n",
    "\n",
    "where the integral is over the domain of $x$ we are interested in. The goal is to minimize this loss function with respect to the parameters of the MLP that defines $f(x)$.\n",
    "\n",
    "\n",
    "HINT: If $f(x)$ is just an MLP, then your solution will likely just collapse to just $f(x) = 0$. Try to find a way to write $f(x) = $ something involving an MLP but also manifestly satisfies the initial condition.\n",
    "\n",
    "BONUS: If you try to do a second-order ODE using our MLP, the solution will fail miserably. Why? Hint: this relates to piecewise-linearity. Can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the rest of this code to build our approximate ODE solver! This requires only minor modifications to the code we have already written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "# We will only solve the ODE on a compact domain\n",
    "x_domain = jnp.linspace(-4, 4, 1000)\n",
    "x_domain = x_domain.reshape(-1, 1)  # Reshape to (1000, 1) for compatibility with MLP\n",
    "\n",
    "x_0 = 0\n",
    "f_0 = 1  # Initial condition: f(0) = 1\n",
    "\n",
    "\n",
    "def F(f_x, grad_f_x, x):\n",
    "    # Example ODE: f = exp(-0.25 * x)\n",
    "    ODE_term = 0.25 * f_x + grad_f_x\n",
    "\n",
    "    return ODE_term\n",
    "\n",
    "\n",
    "# HINT: If your solution f(x) is just an MLP, you will likely only find trivial solutions and fail to satisfy the initial condition.\n",
    "# Try to arrange f(x) to always satisfy the initial condition and then learn the rest of the solution!\n",
    "def my_solution(x, params):\n",
    "    g_x = MLP_jax(x, params)  # Get the output of the MLP\n",
    "\n",
    "    #  ... Rest of your solution\n",
    "    return 0\n",
    "\n",
    "\n",
    "vmapped_ODE_solution = vmap(\n",
    "    my_solution, in_axes=(0, None)\n",
    ")  # Vectorized solution function\n",
    "\n",
    "# lambda function to make the output of my_solution a scalar so that we can compute the gradient\n",
    "vmapped_grad_ODE = vmap(\n",
    "    grad(lambda x, params: my_solution(x, params).squeeze(), argnums=0),\n",
    "    in_axes=(0, None),\n",
    ")  # Vectorized gradient of MLP\n",
    "\n",
    "\n",
    "# [... REST OF YOUR SOLUTION]\n",
    "def gradient_step_ODE(params, learning_rate=0.01):\n",
    "    # YOUR SOLUTION HERE\n",
    "    return params, 0\n",
    "\n",
    "\n",
    "# Initialize the parameters for the ODE solver\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "L = 3\n",
    "N = 17\n",
    "params = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "\n",
    "# Train the ODE solver\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    params, loss = gradient_step_ODE(params, learning_rate=0.01)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot the solution\n",
    "y_vals = vmapped_ODE_solution(x_domain, params)\n",
    "plt.plot(\n",
    "    x_domain, jnp.exp(-0.25 * x_domain), label=\"True Solution: exp(-x)\", color=\"blue\"\n",
    ")\n",
    "plt.plot(x_domain, y_vals, label=\"MLP Solution\", color=\"orange\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.yscale(\"log\")\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_SOLUTION\n",
    "# We will only solve the ODE on a compact domain\n",
    "x_domain = jnp.linspace(-4, 4, 1000)\n",
    "x_domain = x_domain.reshape(-1, 1)  # Reshape to (1000, 1) for compatibility with MLP\n",
    "\n",
    "x_0 = 0\n",
    "f_0 = 1  # Initial condition: f(0) = 1\n",
    "\n",
    "\n",
    "def my_solution(x, params):\n",
    "    g_x = MLP_jax(x, params)  # Get the output of the MLP\n",
    "\n",
    "    # This solution will automatically satisfy the initial condition\n",
    "    f_x = f_0 + (x - x_0) * g_x\n",
    "\n",
    "    return f_x\n",
    "\n",
    "\n",
    "vmapped_ODE_solution = vmap(\n",
    "    my_solution, in_axes=(0, None)\n",
    ")  # Vectorized solution function\n",
    "\n",
    "# lambda function to make the output of my_solution a scalar so that we can compute the gradient\n",
    "vmapped_grad_ODE = vmap(\n",
    "    grad(lambda x, params: my_solution(x, params).squeeze(), argnums=0),\n",
    "    in_axes=(0, None),\n",
    ")  # Vectorized gradient of MLP\n",
    "\n",
    "\n",
    "def F(f_x, grad_f_x, x):\n",
    "    # Example ODE: f = exp(-0.25 * x)\n",
    "    ODE_term = 0.25 * f_x + grad_f_x\n",
    "\n",
    "    return ODE_term\n",
    "\n",
    "\n",
    "vmapped_grad_MLP = vmap(\n",
    "    grad(lambda x, params: MLP_jax(x, params).squeeze(), argnums=0), in_axes=(0, None)\n",
    ")  # Vectorized gradient of MLP\n",
    "\n",
    "\n",
    "def ODE_loss(params):\n",
    "    # Compute the loss as the mean squared error between the network output and the ODE solution\n",
    "    f_x = vmapped_ODE_solution(x_domain, params)  # params is a tuple of (Ws, bs)\n",
    "    grad_f_x = vmapped_grad_ODE(x_domain, params)\n",
    "\n",
    "    # Compute the ODE residual\n",
    "    residual = F(f_x, grad_f_x, x_domain)\n",
    "\n",
    "    # Mean squared error loss\n",
    "    return jnp.mean(residual**2)\n",
    "\n",
    "\n",
    "def gradient_step_ODE(params, learning_rate=0.01):\n",
    "    # Compute the loss and its gradient\n",
    "    loss = ODE_loss(params)\n",
    "    grads = grad(ODE_loss)(params)\n",
    "\n",
    "    # Update the network parameters using gradient descent\n",
    "    Ws, bs = params\n",
    "    dWs, dbs = grads\n",
    "    new_Ws = [W - learning_rate * dW for W, dW in zip(Ws, dWs)]\n",
    "    new_bs = [b - learning_rate * db for b, db in zip(bs, dbs)]\n",
    "    new_params = (new_Ws, new_bs)\n",
    "\n",
    "    return new_params, loss\n",
    "\n",
    "\n",
    "# Initialize the parameters for the ODE solver\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "L = 3\n",
    "N = 17\n",
    "params = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "\n",
    "# Train the ODE solver\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    params, loss = gradient_step_ODE(params, learning_rate=0.01)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot the solution\n",
    "y_vals = vmapped_ODE_solution(x_domain, params)\n",
    "plt.plot(\n",
    "    x_domain, jnp.exp(-0.25 * x_domain), label=\"True Solution: exp(-x)\", color=\"blue\"\n",
    ")\n",
    "plt.plot(x_domain, y_vals, label=\"MLP Solution\", color=\"orange\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.yscale(\"log\")\n",
    "###STOP_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.4:_Some_notes_on_JAX_Prebuilt_Libraries"
   },
   "source": [
    "## Chapter 2.4: Some notes on JAX Prebuilt Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has a number of prebuilt libraries that can be useful for machine learning. Some of the most popular ones are:\n",
    "1. **Flax**: A neural network library for JAX that provides a high-level interface for building and training neural networks. It is similar to PyTorch in terms of functionality, but uses JAX's functional programming style.\n",
    "2. **stax**: A library for probabilistic programming in JAX. It provides a high-level interface for building and training probabilistic models, and is similar to PyMC3 or TensorFlow Probability.\n",
    "\n",
    "However, we will not cover these in any great depth This is because if you are going to use prebuilt libraries, you are probably better off using PyTorch or TensorFlow, which have more mature libraries and a larger community. JAX is more useful for experimentation with explicitly defined models and getting into the guts of it all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for completeness, we will show how to use stax to build a simple MLP. This is not meant to be a comprehensive tutorial on these libraries, but rather a quick introduction to their usage so you can see the syntax. The syntax between stax and flax is virtually identical. Both are also directly meant to mimic PyTorch anyways, so after this example, we will go into an in-depth PyTorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import stax\n",
    "\n",
    "# prepare data\n",
    "indices = np.random.permutation(len(X_moons))\n",
    "X_moons = X_moons[indices]\n",
    "y_moons = y_moons[indices]\n",
    "X_moons = X_moons.astype(np.float32)  # Convert to float32 for JAX compatibility\n",
    "y_moons = y_moons.astype(np.float32)  # Convert to float32 for JAX compatibility\n",
    "\n",
    "\n",
    "# A model is a sequential list of layers\n",
    "init, apply = stax.serial(\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,  # Dense(N) is a fully connected layer with output dimension N. The input dimension is inferred from the input data.\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1),\n",
    ")\n",
    "\n",
    "\n",
    "# init is the function to initialize the parameters of the model.\n",
    "# apply is the function to apply the model to the input data.\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key1, key2 = random.split(key)\n",
    "_, params = init(key2, (-1, 2))\n",
    "\n",
    "\n",
    "# Define a loss function for binary classification\n",
    "def loss(params, x, y):\n",
    "    logits = jnp.squeeze(apply(params, x))\n",
    "    return -jnp.mean(\n",
    "        y * jax.nn.log_sigmoid(logits) + (1 - y) * jax.nn.log_sigmoid(-logits)\n",
    "    )\n",
    "\n",
    "\n",
    "@jit  # Same type of gradient step as before, but now using stax\n",
    "def step(params, x, y, lr=0.01):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "\n",
    "    # tree_map is a JAX function that applies a function to each leaf of a pytree (like a list of arrays)\n",
    "    # Makes it easy to update the parameters all at once\n",
    "    return jax.tree_util.tree_map(lambda a, b: a - lr * b, params, grads)\n",
    "\n",
    "\n",
    "for _ in range(10000):\n",
    "    params = step(params, X_moons, y_moons)\n",
    "\n",
    "print(\"stax loss:\", loss(params, X_moons, y_moons))\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_moons[:, 0].min() - 0.1, X_moons[:, 0].max() + 0.1\n",
    "y_min, y_max = X_moons[:, 1].min() - 0.1, X_moons[:, 1].max() + 0.1\n",
    "xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 100), jnp.linspace(y_min, y_max, 100))\n",
    "xs = jnp.array(list(zip(xx.ravel(), yy.ravel())))\n",
    "Z = jnp.squeeze(apply(params, xs))\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(xx, yy, jax.nn.sigmoid(Z), alpha=0.8, cmap=\"coolwarm\")\n",
    "ax.scatter(\n",
    "    X_moons[:, 0],\n",
    "    X_moons[:, 1],\n",
    "    c=y_moons,\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3:_PyTorch"
   },
   "source": [
    "# Chapter 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a popular ML library developed by Meta (formerly Facebook). It is widely used in industry and research. It is typically more high-level than JAX, and has a wider range of prebuilt modules and utilities for common ML tasks. It is also older and more widely supported with documentation and tutorials online.\n",
    "\n",
    "Unlike JAX, PyTorch is object-oriented, meaning that we will define an MLP as a class with an internal state that keeps track of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3.1:_Primer_on_PyTorch_tensors_and_autodiff"
   },
   "source": [
    "## Chapter 3.1: Primer on PyTorch tensors and autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has its own tensor class, which are similar to (but not the same as) numpy arrays. They can live on a \"device\" (e.g. `cuda` or `cpu`), and can be used to perform computations on that device. PyTorch tensors have many of the same methods as numpy arrays, but also have some additional methods for ML tasks, such as `backward()` for computing gradients.\n",
    "\n",
    "Autodiff in PyTorch is different than in JAX. In PyTorch, we define a computation graph by performing operations on tensors, and then call `backward()` on the output tensor to compute the gradients. This is different from JAX, where we define a function and then call `grad()` on that function to compute the gradients. Once a tensor is in the graph, it cannot be converted back to numpy without first `detach`ing it.\n",
    "\n",
    "To emphasize, in JAX, we take derivatives of functions, and the derivative of a function is another function. In PyTorch, we tell the graph to keep track of a tensor, we pass the tensor through a function, and then we ``backpropagate'' the resultant tensor. Derivatives are computed on the output tensor and the result is a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pytorch tensor defined from our numpy moons dataset\n",
    "x_pytorch = torch.tensor(X_moons, dtype=torch.float32, device=device)\n",
    "\n",
    "# Converting the pytorch tensor back to a numpy tensor\n",
    "x_numpy = x_pytorch.detach().cpu().numpy()\n",
    "# .detach() is used to remove the tensor from the computation graph, so it can be converted to a numpy array.\n",
    "# .cpu() is used to move the tensor to the CPU, since numpy only works with CPU tensors.\n",
    "\n",
    "\n",
    "##### DEFINING GRADIENTS AND COMPUTATION GRAPHS ######\n",
    "\n",
    "\n",
    "# Define a function and compute gradients\n",
    "def f_torch(x):\n",
    "    return torch.sin(10 * x) * torch.exp(-2 * x**2)\n",
    "\n",
    "\n",
    "# requires_grad=True to compute gradients later! Try setting to False and see what happens!\n",
    "\n",
    "x_single = torch.tensor([0.0], device=device, requires_grad=True)  # Single value tensor\n",
    "y_single = f_torch(x_single)  # Compute the function value\n",
    "\n",
    "\n",
    "# We want to compute the gradient of the function and plot it.\n",
    "# First, we call .backward on the output of the function (not the function itself!).\n",
    "# This tells Pytorch its time to compute the gradients in the computational graph.\n",
    "\n",
    "y_single.backward()  # Compute the gradient for the single value\n",
    "\n",
    "# Now the gradients are computed. To access it, we use .grad() on the input tensor.\n",
    "print(\"Gradient at x=0.0: \", x_single.grad.item())  # Should print the gradient at x=0.0\n",
    "\n",
    "\n",
    "##### MULTIPLE INPUTS ######\n",
    "\n",
    "# The same as above, but now on a vectorized input\n",
    "xs = torch.linspace(-1, 1, 1000, device=device, requires_grad=True)\n",
    "ys = f_torch(xs)\n",
    "\n",
    "# We cannot just use ys.backward() because ys is a vector, not a scalar.\n",
    "# ys has 1000 elements, and xs has 1000 elements, so PyTorch thinks there is a 1000 x 1000 jacobian!\n",
    "# We need to specify a gradient direction to sum the gradients over the output dimension. Since y_j is indepndent of x_i for i!=j, we can just use a vector of ones as the gradient direction.\n",
    "gradient_direction = torch.ones_like(ys, device=device)  #\n",
    "_ = ys.backward(gradient=gradient_direction)  # Compute the gradients\n",
    "# Now we can access the gradients\n",
    "xs_grad = (\n",
    "    xs.grad\n",
    ")  # This will be a tensor of the same shape as xs in the specified direction.\n",
    "\n",
    "\n",
    "# Plot the function and its gradient\n",
    "plt.plot(xs.detach().cpu().numpy(), ys.detach().cpu().numpy(), label=\"f(x)\")\n",
    "plt.plot(xs.detach().cpu().numpy(), xs_grad.detach().cpu().numpy(), label=\"f'(x)\")\n",
    "\n",
    "\n",
    "# Zero-ing out gradients\n",
    "# In PyTorch, gradients accumulate by default, so we need to zero them out before the next backward pass.\n",
    "x_single.grad.zero_()  # Zero out the gradients\n",
    "xs.grad.zero_()  # Zero out the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3.2:_Pre-built_PyTorch_Modules"
   },
   "source": [
    "## Chapter 3.2: Pre-built PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sequential` class\n",
    "\n",
    "There are many possible layers of abstraction in PyTorch. The highest-level, \"I don't care, I just want a neural network, don't bother me with details\" way to define an MLP is to use the `Sequential` class.\n",
    "\n",
    "Most models are simply a sequence of layers, where each layer is a function that takes in the output of the previous layer and produces an output. The `Sequential` class allows us to define a model as a sequence of layers, where each layer is applied in order. The ``nn`` module provides many prebuilt layers, such as `Linear`, `ReLU`, and `Softmax`, that can be used to define a model. Note that `Linear` actually means \"Affine\", i.e. it implements the affine transformation $W x + b$.\n",
    "\n",
    "Let's see how to implement an MLP. Compared to the above implementations, this will be much shorter and cleaner, since most of this is already implemented for us in PyTorch (but the cost is that we have less control over the details of the implementation, like if we wanted to mess with layer weights). Note that we don't even have to worry about the initialization of the weights, since PyTorch does this for us automatically.\n",
    "\n",
    "We also don't have to bother defining a loss function or gradient descent, sice this also already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),  # We just stack a bunch of layers\n",
    "    nn.Linear(\n",
    "        32, output_dim\n",
    "    ),  # No activation on the last layer. Instead, we'll put the sigmoid or softmax in the loss function..\n",
    ").to(\n",
    "    device\n",
    ")  # .to(device) moves the model to the GPU if available\n",
    "\n",
    "\n",
    "# Lots of optimizers to choose from! SGD is ordinary (stochastic) gradient descent, Adam is a more advanced optimizer.\n",
    "opt_sgd = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "opt_adam = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "# Prebuilt BCE. Already includes the sigmoid, so we don't need to apply it in the model.\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 300\n",
    "train_fraction = 0.8\n",
    "opt = opt_adam  # Choose the optimizer\n",
    "\n",
    "X_torch = torch.tensor(X_moons, dtype=torch.float32, device=device)\n",
    "y_torch = torch.tensor(y_moons, dtype=torch.float32, device=device).unsqueeze(\n",
    "    1\n",
    ")  # Unsqueeze to make it a column vector\n",
    "\n",
    "# Training/test split, shuffle the dataset\n",
    "train_size = int(train_fraction * len(X_torch))\n",
    "indices = torch.randperm(len(X_torch))  # Shuffle the dataset\n",
    "X_torch = X_torch[indices]\n",
    "y_torch = y_torch[indices]\n",
    "X_train, y_train = X_torch[:train_size], y_torch[:train_size]\n",
    "X_test, y_test = X_torch[train_size:], y_torch[train_size:]\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for _ in range(300):\n",
    "    opt.zero_grad()  # Zero out the gradients before the backward pass. VITAL!\n",
    "    loss = loss_fn(model(X_train), y_train)\n",
    "    loss.backward()  # Tell the graph its time to compute the gradients\n",
    "    opt.step()  # \"step\" automatically updates the parameters using the gradients computed in the backward pass\n",
    "    train_losses.append(loss.item())  # Save the training loss\n",
    "    test_loss = loss_fn(model(X_test), y_test)\n",
    "    test_losses.append(test_loss.item())  # Save the test loss\n",
    "    if _ % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {_}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"PyTorch loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot the decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = X_test[:, 0].min() - 0.1, X_test[:, 0].max() + 0.1\n",
    "y_min, y_max = X_test[:, 1].min() - 0.1, X_test[:, 1].max() + 0.1\n",
    "xx, yy = torch.meshgrid(\n",
    "    torch.linspace(x_min, x_max, 100), torch.linspace(y_min, y_max, 100)\n",
    ")\n",
    "xs = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(\n",
    "    device\n",
    ")  # Stack to create a grid of points\n",
    "Z = model(xs)  # Get the model predictions\n",
    "\n",
    "\n",
    "# Dont forget to apply the sigmoid to the logits!\n",
    "Z = torch.sigmoid(Z)  # Apply sigmoid to the logits\n",
    "\n",
    "Z = Z.detach().cpu().numpy()\n",
    "Z = Z.reshape(xx.shape)  # Reshape to match the grid shape\n",
    "ax.contourf(xx.cpu().numpy(), yy.cpu().numpy(), Z, alpha=0.8, cmap=\"coolwarm\")\n",
    "ax.scatter(\n",
    "    X_test[:, 0].cpu().numpy(),\n",
    "    X_test[:, 1].cpu().numpy(),\n",
    "    c=y_test.cpu().numpy(),\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise_3.1:"
   },
   "source": [
    "### Exercise 3.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we don't have to code anything, lets try some more advanced things.\n",
    "1) Compare the ADAM optimizer to the SGD optimizer.\n",
    "2) Trade out the ReLU activation for sigmoid (used in the original MLP's back in the day), selu (a variant of ReLU that doesn't go to 0), and others. How do they compare? The modern lore in 2025 is to use \"Swish\" (aka \"SiLU\") activations, which are supposedly better than ReLU. There are many more at https://docs.pytorch.org/docs/main/nn.functional.html#non-linear-activation-functions. Try them out!\n",
    "3) PyTorch MLP layers are called \"Linear\" despite being affine transformations. Let's see what happens if they are literally linear: the bias term can be removed with `bias=False`, e.g. `nn.Linear(32,32, bias = False)`. What happens to the decision boundary? As you do this, reflect on the exercise we did earlier where we constructed MLPs to exactly match piecewise linear functions, and the role the bias played there.\n",
    "4) It should be straightforward to change the model to work on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3.3:_Convolutional_Neural_Networks_(CNNs)"
   },
   "source": [
    "## Chapter 3.3: Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP's are already universal function approximators. But we can do better!\n",
    "\n",
    "Often, and especially in physics, our function has some structure we can exploit (such as symmetries, or locality). Perhaps we can use this symmetry to constrain the weights of the MLP, or even better (and equivalently), design a new MLP-like-model that has this symmetry built in and is a universal function approximator with respect to this symmetry.\n",
    "\n",
    "Image classification has an approximate translational symmetry: If an image of a handwritten \"5\" is shifted to the right, it is still a \"5\". We can exploit this symmetry by using a **Convolutional Neural Network (CNN)**, which is a type of neural network that is specifically designed for image classification tasks that has this translational symmetry built in. A CNN can achieve significantly better performance than an MLP with the same number or far fewer parameters.\n",
    "\n",
    "A 2D translation on our 784-dimensional input space is a violent operation, and a generic MLP will not be translationlly invariant (or covariant). One can attempt to solve for a special class of $W$'s that are covariant (related to Toeplitz matrices), but this will end up being related to convolutions anyways.\n",
    "\n",
    "Instead, if we represent an image as a distribution $I(x)$, where $I(x)$ is the pixel value at position $x$ (and there can be multiple channels, e.g. RGB labelled by $I^a(x)$), then the following operation (**convolution**) is equivalent to a translation-invariant operation:\n",
    "$$ (I * K)^b(x) = \\int dy \\sum_a I^a(y) K^b(x - y) $$\n",
    "where $K^b(x)$ is a kernel (or filter) that is applied to the image (there can be several kernels, labeled by $b$ and summed over $a$). The convolution operation is equivalent to sliding the kernel over the image and computing the dot product at each position.\n",
    "\n",
    "Then, we can construct a universal function approximator for translation-invariant functions:\n",
    "$$F(I) = \\Psi(\\int dx [\\text{Equivariant operations on I}])$$\n",
    "\n",
    "where $\\Psi$ is any function that is a universal function approximator (e.g. an MLP). This is the basic recipe for a CNN. We use (discrete) convolutions to extract features from an image in an equivariant way aggregate them (represented by the integral, but it could also any other invariant aggregation like max-pooling), then feed them into an MLP. In practice, we will interleave pooling with the convolutions (since it also helps introduce nonlinearity) and use max-pooling rather than integration (mean-pooling) for more nonlinearity.\n",
    "\n",
    "Images are not exactly translation invariant, but they are approximately so. In particular, they are discrete, and they have boundaries. So we will use a discrete convolution, and use pading on boundaries. We will also assume that our convolutions are local (the support of $K(x-y)$ is dominated by $x \\sim y$), so that we can write our kernels as small $3\\times 3$ or ($5\\times 5$) matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also make use of the ``nn.Module`` class. This is the base class for all neural network modules in PyTorch. It is one level of abstraction above the ``nn.Sequential`` class, and allows us to define the model with more contrl (e.g. we can define the parameters of the model directory (the layers in most models), and we can define how those paramrameters are used to define the function (the ``forward`` method).)\n",
    "\n",
    "We will also see our first example of a nontrivial layer beyond just the linear layer. The `nn.Conv2d` layer is a convolutional layer that applies a 2D convolution to the input. It takes as input the number of input channels, the number of output channels, and the kernel size (the size of the filter). It also has a stride and padding parameter, which control how the convolution is applied to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a PyTorch CNN\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1, 16, kernel_size=3, padding=1\n",
    "        )  # Input channels = 1 for grayscale images\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(\n",
    "            16 * 7 * 7, 16\n",
    "        )  # Assuming input images are 28x28, and we aggregate down to 7x7.\n",
    "        self.fc2 = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(\n",
    "            x, kernel_size=2\n",
    "        )  # aggregation: Take every 2x2 block and take the maximum value\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(\n",
    "            x, kernel_size=2\n",
    "        )  # After 2 aggregations, we go from 28x28 to 14x14 to 7x7.\n",
    "\n",
    "        # We havent aggregated all the way down to a single value, so we arent perfectly invariant to translation.\n",
    "        # But we are invariant to small translations, which is good enough!\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor:\n",
    "\n",
    "        # MLP part:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the CNN model\n",
    "model_cnn = SimpleCNN(num_classes=10).to(\n",
    "    device\n",
    ")  # Move the model to the GPU if available\n",
    "\n",
    "# Define the optimizer and loss function (IDENTICAL TO BEFORE)\n",
    "opt_cnn = torch.optim.Adam(model_cnn.parameters(), lr=1e-2)  # Adam optimizer\n",
    "loss_fn_cnn = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "\n",
    "# Data. We can treat the images as genuine 28x28x1 (1 = no color) images, rather than flattening them to 784.\n",
    "train_fraction = 0.8\n",
    "train_size = (\n",
    "    int(train_fraction * len(X_mnist)) // 5\n",
    ")  # Use a smaller training set for faster training, just for tutorial's sake\n",
    "indices = torch.randperm(len(X_mnist))  # Shuffle the dataset\n",
    "X_mnist_torch = (\n",
    "    torch.tensor(X_mnist, dtype=torch.float32, device=device)\n",
    "    .unsqueeze(1)\n",
    "    .reshape(-1, 1, 28, 28)\n",
    ")  # Reshape to (N, C, H, W)\n",
    "y_mnist_torch = torch.tensor(\n",
    "    y_mnist, dtype=torch.long, device=device\n",
    ")  # Long tensor for class labels\n",
    "X_train_mnist_torch, y_train_mnist_torch = (\n",
    "    X_mnist_torch[indices[:train_size]],\n",
    "    y_mnist_torch[indices[:train_size]],\n",
    ")\n",
    "X_test_mnist_torch, y_test_mnist_torch = (\n",
    "    X_mnist_torch[indices[train_size:]],\n",
    "    y_mnist_torch[indices[train_size:]],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the CNN model\n",
    "epochs = 100  # Far fewer epochs needed!\n",
    "train_losses_cnn = []\n",
    "test_losses_cnn = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    opt_cnn.zero_grad()  # Zero out the gradients before the backward pass\n",
    "    outputs = model_cnn(X_train_mnist_torch)  # Forward pass\n",
    "    loss = loss_fn_cnn(outputs, y_train_mnist_torch)  # Compute the loss\n",
    "    loss.backward()  # Backward pass to compute gradients\n",
    "    opt_cnn.step()  # Update the model parameters\n",
    "    train_losses_cnn.append(loss.item())  # Save the training loss\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        test_outputs = model_cnn(X_test_mnist_torch)\n",
    "        test_loss = loss_fn_cnn(test_outputs, y_test_mnist_torch)\n",
    "        test_losses_cnn.append(test_loss.item())  # Save the test loss\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        accuracy = (predicted == y_test_mnist_torch).float().mean().item()\n",
    "\n",
    "    if epoch % 1 == 0:  # Print every epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}, Accuracy: {accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses_cnn, label=\"Train Loss\")\n",
    "plt.plot(test_losses_cnn, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3.4_(BONUS):_Permutation_Invariance_and_Equivariance_with_Deep_Sets_and_Transformers"
   },
   "source": [
    "## Chapter 3.4 (BONUS): Permutation Invariance and Equivariance with Deep Sets and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw above how we can use convolutions to exploit translational symmetry. In this section, we will see how we can exploit permutation symmetry of inputs. Here, we are interested in functions of SETS to real numbers, rather than functions of vectors to real numbers. Sets are invariant to the order of the elements, meaning that the function should produce the same output regardless of the order of the points in the set. One immediate upside of this is that our model can allow for an arbitrary input size, since it's still a set regardless of how many points are in it!\n",
    "\n",
    "In physics, we often deal with sets of particles or measurements, where the particle labeling is completely arbitrary, so this symmetry is important. We also often do not have a fixed number of particles, so we want our model to be able to handle an arbitrary number of particles.\n",
    "\n",
    "Two models that are designed to exploit permutation symmetry are **Deep Sets** and **Transformers**.\n",
    "\n",
    "A Deep Set is a type of network that is a universal function approximator for functions of sets. It is defined as:\n",
    "$$ f(S) = \\Psi\\left(\\sum_{x \\in S} \\phi(x)\\right) $$\n",
    "where $\\phi$ is a function that maps each element of the set to a vector, and $\\Psi$ is a function that aggregates the vectors into a single vector. Both are MLPs! It is manifestly invariant to the order of the points in the set. Note that even though $\\phi$ only acts on a single element at a time, inter-point correlations are still captured by the aggregation function $\\Psi$ (though not necessarily efficiently).\n",
    "\n",
    "Transformers are a more general class of networks that include Deep Sets as a special case. Transformers are extremely common and have exploded in popularity in the last few years, especially in the context of natural language processing (NLP). Most of the most powerful models in high energy physics (such as Particle Transformer), as well as models outside physics (such as ChatGPT) are Transformers.\n",
    "\n",
    "Transformers are based on permutation equivariant layers:\n",
    "$$ f(x_i) = \\sigma(C_{ij}x_j + \\phi(x_i, x_j)D_{jk}x_k)$$\n",
    "where $C$ and $D$ are matrices that are shared across all points in the set, and $\\phi$ is some symmetric kernel. This is the simplest non-linear layer one can construct that is permutation invariant. Typically, $\\phi = \\text{softmax}(\\langle Qx_i, K x_j\\rangle)$.\n",
    "\n",
    "\n",
    "A lot of literature about transformers is based on natural language processing, so it's worth getting used to. Just for terminology's sake, it is common to call $Q_i$ the \"query vector\", $K_j$ the \"key vector\", and $V_j = D_{jk} x_k$ the \"value vector\", but these are just names and don't have any special meaning outside the very specific context of NLP. Emotionally, $Q$ is supposed to represent the \"question\" we are asking about the input $x_i$, $K$ is supposed to represent the \"context\" of the input $x_j$, and $V$ is supposed to represent the \"value\" of the input $x_k$. This  construction is called the **self-attention** layer, because the kernal $\\phi$ is a function from 0 to 1 that tells the model at $x_i$ how much to \"pay attention\" to $x_j$, but these are just non-rigorous words invented before the math was understood and don't get too caught up in them. The elements of the sets are called \"tokens\", because words are often encoded as tokens. Note that since self-attention is permutation invariant, the order of words in a sentence is usually encoded into the token itself to break the symmetry, or the kernel is forced to be 0 if the words are out of order.\n",
    "\n",
    "Both the deep-sets and the transformer were written as a scalar output. One can easily extend this to a vector of outputs by appropriately appending extra indcies to everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOY_MODEL:"
   },
   "source": [
    "### TOY MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct the following problem: Every point is a list of 10 random vectors. If the sum of all pairwise dot-products of the vectors in the set is positive, then the output is 1, otherwise it is 0. E.g. we are testing if the set of vectors are roughly pointing in the same direction or not. This problem is permnutation invariant (so Deep Sets or Transformers will be useful), but explicitly involves pairwise nonlinear correlations (so a small Deep Sets will struggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 25000\n",
    "SET_SIZE = 10  # number of vectors in each set\n",
    "FEATURES = 16  # dimension of vectors\n",
    "\n",
    "# Generate random sets\n",
    "X_full = torch.randn(NUM_SAMPLES, SET_SIZE, FEATURES, device=device)\n",
    "\n",
    "# Label is 1 if the sum of all pairwise dot products is positive\n",
    "\n",
    "\n",
    "def upper_triangle_dot_sum(x_set):  # x_set: (n, d)\n",
    "    total = 0.0\n",
    "    for i in range(SET_SIZE):\n",
    "        for j in range(i + 1, SET_SIZE):\n",
    "            total += (x_set[i] * x_set[j]).sum()  # simple dot\u00ad-product\n",
    "    return total\n",
    "\n",
    "\n",
    "y_full = torch.tensor(\n",
    "    [upper_triangle_dot_sum(x) > 0 for x in X_full], dtype=torch.float32, device=device\n",
    ").unsqueeze(\n",
    "    1\n",
    ")  # shape (N, 1)\n",
    "\n",
    "# Train / test split\n",
    "train_size = 20000\n",
    "X_train, y_train = X_full[:train_size], y_full[:train_size]\n",
    "X_test, y_test = X_full[train_size:], y_full[train_size:]\n",
    "\n",
    "\n",
    "# ##### DEFINE MODELS #####\n",
    "\n",
    "\n",
    "class MLP_torch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),  # (B, n*d)\n",
    "            nn.Linear(SET_SIZE * FEATURES, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DeepSets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(FEATURES, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B, n, d)\n",
    "        h = self.phi(x)  # (B, n, 64)\n",
    "        s = h.sum(dim=1)  # permutation-invariant\n",
    "        return self.rho(s)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=FEATURES, nhead=4, dim_feedforward=64, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Encoder layer: contains a stack of self-attention and feedforward layers as defined above\n",
    "        # Can code this yourself if you really want, but PyTorch has a built-in implementation\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.head = nn.Sequential(nn.Linear(FEATURES, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)  # equivariant map\n",
    "        g = h.mean(dim=1)  # invariant aggregation\n",
    "        return self.head(g)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLP_torch().to(device),\n",
    "    \"DeepSets\": DeepSets().to(device),\n",
    "    \"Transformer\": Transformer().to(device),\n",
    "}\n",
    "\n",
    "# ##### TRAINING #####\n",
    "\n",
    "\n",
    "def train_model(model, X_tr, y_tr, X_te, y_te, epochs=60, lr=1e-2, print_every=5):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # forward / backward on the entire training set\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(X_tr)\n",
    "        loss = bce_loss(logits, y_tr)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_te)\n",
    "            test_loss = bce_loss(test_logits, y_te)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            preds = (torch.sigmoid(test_logits) > 0.5).float()\n",
    "            accuracy = (preds == y_te).float().mean().item()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(\n",
    "                f\"epoch {epoch:02d}  \"\n",
    "                f\"train-loss {loss.item():.4f}  \"\n",
    "                f\"test-loss {test_loss.item():.4f}  \"\n",
    "                f\"acc {accuracy:.3f}\"\n",
    "            )\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "# ##### RESULTS #####\n",
    "plt.figure()\n",
    "\n",
    "colors = {\"MLP\": \"blue\", \"DeepSets\": \"orange\", \"Transformer\": \"green\"}\n",
    "for name, net in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    tr_loss, te_loss = train_model(net, X_train, y_train, X_test, y_test, epochs=60)\n",
    "    plt.plot(te_loss, label=f\"{name} (test)\", ls=\"--\", color=colors[name])\n",
    "    plt.plot(tr_loss, label=f\"{name} (train)\", color=colors[name])\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"BCE loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Linear_Permutation_Invariance"
   },
   "source": [
    "### Exercise: Linear Permutation Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you design a problem for which DeepSets will outperform a transformer? That is, a problem for which inter-particle correlations are expected to be less important, so that the problem can be more easily written as a sum of single-particle functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_4:_Tensorflow"
   },
   "source": [
    "# Chapter 4: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow (and the associated API, Keras) is another popular ML library developed by Google. It is similar to PyTorch in many ways. It used to be the most popular ML library years ago before being overtaken by PyTorch, but it is still widely used in industry and research. It is typically more high-level than JAX, and has a wider range of prebuilt modules and utilities for common ML tasks. It is also older and more widely supported with documentation and tutorials online.\n",
    "\n",
    "Keras is a high-level API for Tensorflow that allows us to define models in an intuitive way. It is similar to the `nn.Sequential` class in PyTorch, but with more features and flexibility. Keras allows us to define models as a sequence of layers, where each layer is applied in order. It also has a wide range of prebuilt layers and utilities for common ML tasks.\n",
    "\n",
    "We will not go into detail about Tensorflow and Keras, since they are similar to PyTorch and JAX, but we will provide a brief example of how to define an MLP in Keras, so that you are familiar with the syntax and can use it if you prefer.\n",
    "\n",
    "Tensorflow has even more pre-built modules that are abstracted away from the user than PyTorch, so it is even easier to define models. You do not even have to write your own traning loop. One difference is that Tensorflow uses a \"static graph\" approach, meaning that the computation graph is defined before the model is run. The model must be specified and then \"compiled\", not entirely unlike JAX's JIT compilation. This means that the model is optimized for performance before it is run, which can lead to better performance in some cases. However, it also means that the model is less flexible and harder to debug, since you cannot change the model on the fly like you can in PyTorch or JAX.\n",
    "\n",
    "A historical note: Tensorflow 1.0 is extremely different from Tensorflow 2.0 and Keras. We will only be using Tensorflow 2.0 and Keras, which is much more user-friendly and similar to PyTorch. It is rare to see Tensorflow 1.0 code these days, especially in physics, but it is worth being aware that it exists if you find yourself being confused by some old code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_4.1:_Defining_an_MLP_and_a_CNN_in_Keras"
   },
   "source": [
    "## Chapter 4.1: Defining an MLP and a CNN in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# Something nice about tensorflow: You only have to specify the output dimension of a layer. The input is inferred.\n",
    "mlp = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),  # logits\n",
    "    ]\n",
    ")\n",
    "\n",
    "mlp.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-2),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        \"accuracy\"\n",
    "    ],  # We can tell tensorflow what metrics we want to track on top of the loss!\n",
    ")\n",
    "\n",
    "\n",
    "# Tensorflow automatically has a training loop. We dont even need to split the dataset!\n",
    "history = mlp.fit(X_moons, y_moons, epochs=300, verbose=0, validation_split=0.2)\n",
    "\n",
    "# Training gives us a history object with the training and validation losses and accuracies.\n",
    "# Very convenient!\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"final val-acc:\", history.history[\"val_accuracy\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNNS_and_Callbacks"
   },
   "source": [
    "### CNNS and Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very cute feature of Keras/TF are \"Callbacks\", which are functions you can execute during the training loop. In JAX or PyTorch, you manually write training loops anyways and so can decorate them however you like, but Keras has a specialized interface for this.\n",
    "\n",
    "For example, you can define a callback that prints the training loss every 10 epochs, or a callback that saves the model every 100 epochs. This is very useful for debugging and monitoring the training process.\n",
    "\n",
    "The most useful callback is the `EarlyStopping` callback, which stops the training process if the validation loss does not improve for a certain number of epochs. This is useful to prevent overfitting and save time during training. This can also be done for PyTorch and JAX, of course, but we are introducing it here since it is especially convenient.\n",
    "\n",
    "Many callbacks are built-in, but for the sake of this tutorial, we will also define a custom callback to print the mean and spread of the weights of each layer. This is useful to monitor the training process and see if the weights are exploding or vanishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_mnist_np = X_mnist.astype(\"float32\").reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_np = y_mnist.astype(\"int32\")  # integer labels (0\u20129)\n",
    "\n",
    "idx = np.random.permutation(len(X_mnist_np))\n",
    "train_size = int(0.8 * len(idx))\n",
    "\n",
    "X_train = X_mnist_np[idx[:train_size]]\n",
    "y_train = y_mnist_np[idx[:train_size]]\n",
    "X_test = X_mnist_np[idx[train_size:]]\n",
    "y_test = y_mnist_np[idx[train_size:]]\n",
    "\n",
    "# Basic CNN model in Keras. Basicly the same as in PyTorch! Slightly more convient since no input dimensions.\n",
    "cnn = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            16, 3, padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10),  # logits\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "cnn.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# ########## CALLBACKS ##########\n",
    "\n",
    "# Pre-built callback: Stop training if the validation loss does not improve for 5 epochs\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Define a custom callback\n",
    "class LayerWeights(tf.keras.callbacks.Callback):\n",
    "    # Override the on_epoch_end method so the function executed at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # For each layer, print the mean and standard deviation of the weights\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, \"kernel\"):\n",
    "                weights = layer.kernel.numpy()\n",
    "                print(\n",
    "                    f\"Layer {layer.name} - Mean: {np.mean(weights):.4f}, Std: {np.std(weights):.4f}\"\n",
    "                )\n",
    "\n",
    "\n",
    "hist = cnn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, LayerWeights()],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ##### RESULTS #####\n",
    "test_loss, test_acc = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"MNIST  test-accuracy = {test_acc:.3f}\")\n",
    "\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cross-entropy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_4.2:_Custom_Training_Loops_and_GradientTape"
   },
   "source": [
    "## Chapter 4.2: Custom Training Loops and GradientTape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients in TF/Keras are a little unusual. Most operations in TF/Keras do not record gradients, unlike in JAX or PyTorch. Instead, you have to explicitly tell TF/Keras that it's time to record gradients using the `tf.GradientTape` context manager. Inside a GradientTape, all operations will record gradients, and you can then call `tape.gradient()` to compute the gradients of the output with respect to the inputs. This is not terribly dissimilar to PyTorch's `backward()` method, but it is more explicit and requires you to manage the context yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a toy problem to show off the manual training loop in TensorFlow\n",
    "\n",
    "# Toy data: 1 000 points, y = 1 if x\u2081 + x\u2082 > 0 else 0\n",
    "x_np = np.random.randn(1_000, 2).astype(\"float32\")\n",
    "y_np = ((x_np[:, 0] + x_np[:, 1]) > 0).astype(\"int32\")\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_np, y_np))  # batches of 128\n",
    "    .shuffle(1_000)\n",
    "    .batch(128)\n",
    ")\n",
    "\n",
    "# Toy MLP\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(8, activation=\"relu\", input_shape=(2,)),\n",
    "        tf.keras.layers.Dense(1),  # logits\n",
    "    ]\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(1e-3)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# ##### MANUAL TRAINING LOOP #####\n",
    "\n",
    "# This is more similar to the Pytorch and Jax Training Loops!\n",
    "for epoch in range(10):\n",
    "    running = 0.0\n",
    "    for xb, yb in train_ds:\n",
    "        # Open a \"Gradient Tape\" to tell Tensorflow its time to start recording the gradients of the following operations\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(xb, training=True)\n",
    "            loss = bce(yb, logits)\n",
    "\n",
    "        # Use the gradients\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        running += loss.numpy()\n",
    "\n",
    "    print(f\"epoch {epoch:02d}  mean-loss {running / len(train_ds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Regularization"
   },
   "source": [
    "### Exercise: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to prevent overfitting is to add a regularization term to the loss function. This can be done by adding a term that penalizes large weights, such as L1 or L2 regularization.\n",
    "\n",
    "There are two ways to do this in Keras:\n",
    "1. Add a regularization term to the loss function manually, e.g. `loss = loss + lambda * tf.reduce_sum(tf.square(model.trainable_weights))`, where `lambda` is the regularization strength. Do this in the custom training loop.\n",
    "2. Use a built-in regularization layer, such as `tf.keras.regularizers.l1` or `tf.keras.regularizers.l2`, and add it to the model when defining the layers, e.g. `tf.keras.layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01))`. This can be done without needing a custom loop.\n",
    "\n",
    "Method 2 is easier, but method 1 lets you control the form of the regularization directly.\n",
    "Try both methods and see how they affect the training process. Note how the spread of the weights (as recorded by our callback!) changes as a result of the regularization.\n",
    "\n",
    "A second type of regularization is **dropout**, which randomly sets a fraction of the inputs to zero during training. This can be done by adding a `tf.keras.layers.Dropout` layer to the model, e.g. `tf.keras.layers.Dropout(0.5)`. This is a very common regularization technique in deep learning, and can be used in conjunction with L1 or L2 regularization. It also has some nice interpretations in terms of Bayesian inference, but we will not go into that here.\n",
    "\n",
    "Try overfitting the model, but then adding dropout and/or L1/L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHAPTER_4.3_(BONUS):_GANS_and_Generative_Models"
   },
   "source": [
    "## CHAPTER 4.3 (BONUS): GANS and Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore the simplest model of generative models, the **Generative Adversarial Network (GAN)**. In a previous tutorial, you learned about Normalizing Flows, which are a more sophisticated type of generative model that can learn complex distributions. GANs are a simpler type of generative model that can be used to generate new data that is similar to the training data. GANs make for a nice tutorial because they involve the interplay of *two* neural networks in a fun way, and show how MLPs can be combined to create new types of functions.\n",
    "\n",
    "The basic premise is that we have two neural networks: a **generator** and a **discriminator**. Both of these can be MLPs, or any other type of model suited to the data. The generator takes in a random noise vector and generates a new data point, while the discriminator takes in a data point and outputs a probability that the data point is real (i.e. from the training data) or fake (i.e. generated by the generator). They are \"adversarial\" because they are trained in opposition to each other: the generator tries to generate data that is similar to the training data, while the discriminator tries to distinguish between real and fake data. The generator wins if it can fool the discriminator, and the discriminator wins if it can correctly classify the data. The loss functions are:\n",
    "\n",
    "$$ L_D = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] $$\n",
    "$$ L_G = -\\mathbb{E}_{z \\sim p_z}[\\log D(G(z))] $$\n",
    "\n",
    "(Technically, one can integrate out the discriminator and form the KL-divergence between the data distribution and the generator distribution, but this is harder to train. Normalizing flows achieve this though.)\n",
    "\n",
    "The generator and discriminator are trained in alternating steps. This is a \"minimax\" game, where the generator tries to minimize its loss while the discriminator tries to maximize its loss.\n",
    "\n",
    "In the end, we will have a generator that can generate new data points that are similar to the training data --- or at least similar enough that the discriminator cannot tell the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: G(z) to image, where z is a random noise vector\n",
    "\n",
    "noise_dim = 64\n",
    "generator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(noise_dim,)),\n",
    "        tf.keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "        tf.keras.layers.Reshape((28, 28, 1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Discriminator: D(x) to logits, where x is an image\n",
    "discriminator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),  # logits\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Losses and optimiziers\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "g_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "d_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# helper for label tensors\n",
    "ones = lambda n: tf.ones((n, 1))\n",
    "zeros = lambda n: tf.zeros((n, 1))\n",
    "\n",
    "# Training Loop\n",
    "batch = 256\n",
    "epochs = 50\n",
    "steps = len(X_train) // batch\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # shuffle once per epoch\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_shuffled = X_train[idx]\n",
    "\n",
    "    d_epoch, g_epoch = 0.0, 0.0\n",
    "\n",
    "    for i in range(steps):\n",
    "        # Get a batch of real images\n",
    "        real = X_shuffled[i * batch : (i + 1) * batch]\n",
    "\n",
    "        # Generate a batch of fake images\n",
    "        z = tf.random.normal((batch, noise_dim))\n",
    "        fake = generator(z, training=True)\n",
    "\n",
    "        # Update discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            d_real = discriminator(real, training=True)\n",
    "            d_fake = discriminator(fake, training=True)\n",
    "            d_loss = bce(ones(batch), d_real) + bce(zeros(batch), d_fake)\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        d_opt.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "        # Update generator\n",
    "        z = tf.random.normal((batch, noise_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake = generator(z, training=True)\n",
    "            d_fake = discriminator(fake, training=True)\n",
    "            g_loss = bce(ones(batch), d_fake)\n",
    "        grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "        d_epoch += d_loss.numpy()\n",
    "        g_epoch += g_loss.numpy()\n",
    "\n",
    "    d_losses.append(d_epoch / steps)\n",
    "    g_losses.append(g_epoch / steps)\n",
    "    print(\n",
    "        f\"epoch {epoch:02d}: D_loss =  {d_epoch/steps:.3f}  G_loss =  {g_epoch/steps:.3f}\"\n",
    "    )\n",
    "\n",
    "# Generate some samples from the generator\n",
    "z = tf.random.normal((16, noise_dim))\n",
    "samples = generator(z, training=False).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(4, 4))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(samples[i, ..., 0], cmap=\"gray\")\n",
    "    axi.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the losses\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(g_losses, label=\"Generator Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Upgrade_to_a_Convolutional_GAN"
   },
   "source": [
    "### Exercise: Upgrade to a Convolutional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your images probably aren't that great. Adding some convolutional structure might help. let's upgrade it to a Convolutional GAN (CGAN). This will allow us to generate images that are more realistic and similar to the training data.\n",
    "\n",
    "You already know how to make a CNN discriminator. You can also make a CNN generator, but it is a little more complicated. The generator will take in a random noise vector and output an image, so it will need to upsample the noise vector to the size of the image. This can be done using transposed convolutions (also known as deconvolutions) or upsampling layers.\n",
    "You can use the `tf.keras.layers.Conv2DTranspose` layer to do this. The generator will also need to use a non-linear activation function, such as ReLU or LeakyReLU, to introduce non-linearity into the model.\n",
    "It is also recommended to use \"batch normalization\", which is a technique that normalizes the inputs to each layer to have zero mean and unit variance. This can help stabilize the training process and improve the performance of the model. You can use the `tf.keras.layers.BatchNormalization` layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_EXERCISE\n",
    "# your solution goes here\n",
    "###STOP_EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START_SOLUTION\n",
    "noise_dim = 64\n",
    "\n",
    "# -- de-convolutional generator (lighter) --------------------\n",
    "generator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(7 * 7 * 16, input_shape=(noise_dim,)),\n",
    "        tf.keras.layers.Reshape((7, 7, 16)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(16, 4, 2, \"same\", use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            1, 4, 2, \"same\", activation=\"sigmoid\", use_bias=False\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -- convolutional discriminator (lighter) -------------------\n",
    "discriminator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, 4, 2, \"same\", input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Conv2D(16, 4, 2, \"same\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -- losses / optimisers -------------------------------------\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "g_opt = tf.keras.optimizers.Adam(2e-4)\n",
    "d_opt = tf.keras.optimizers.Adam(2e-4)\n",
    "\n",
    "ones = lambda n: tf.ones((n, 1))\n",
    "zeros = lambda n: tf.zeros((n, 1))\n",
    "\n",
    "\n",
    "batch = 128\n",
    "epochs = 10\n",
    "steps = len(X_train) // batch\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_shuffled = X_train[idx]\n",
    "    d_epoch = g_epoch = 0.0\n",
    "\n",
    "    for i in range(steps):\n",
    "        real = X_shuffled[i * batch : (i + 1) * batch]\n",
    "\n",
    "        z = tf.random.normal((batch, noise_dim))\n",
    "        fake = generator(z, training=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            d_real = discriminator(real, training=True)\n",
    "            d_fake = discriminator(fake, training=True)\n",
    "            d_loss = bce(ones(batch), tf.sigmoid(d_real)) + bce(\n",
    "                zeros(batch), tf.sigmoid(d_fake)\n",
    "            )\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        d_opt.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "        z = tf.random.normal((batch, noise_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake = generator(z, training=True)\n",
    "            d_fake = discriminator(fake, training=True)\n",
    "            g_loss = bce(ones(batch), tf.sigmoid(d_fake))\n",
    "        grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "        d_epoch += d_loss.numpy()\n",
    "        g_epoch += g_loss.numpy()\n",
    "\n",
    "    d_losses.append(d_epoch / steps)\n",
    "    g_losses.append(g_epoch / steps)\n",
    "    print(\n",
    "        f\"epoch {epoch:02d}: D_loss = {d_epoch/steps:.3f}  G_loss = {g_epoch/steps:.3f}\"\n",
    "    )\n",
    "\n",
    "# generate & plot samples\n",
    "z = tf.random.normal((16, noise_dim))\n",
    "samples = generator(z, training=False).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(4, 4))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(samples[i, ..., 0], cmap=\"gray\")\n",
    "    axi.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# loss curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(d_losses, label=\"Discriminator\")\n",
    "plt.plot(g_losses, label=\"Generator\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "###STOP_SOLUTION"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Prelude:_CPUs_vs_GPUs",
    "Chapter_1:_Neural_Network_Basics",
    "Chapter_1.1:_Universal_Function_Approximation",
    "Exercise:_Modeling_|x|",
    "Exercise:_Approximating_a_smooth_1D_function.",
    "Chapter_1.2:_Functional_Optimization",
    "Interlude:_The_problems_we_will_solve:",
    "Chapter_2:_JAX",
    "Chapter_2.1:_JAX_basics;_vmapping,_autodifferentiation,_and_compilation.",
    "Exercise:_Autodifferentiation_Practice",
    "Chapter_2.2:_End-to-End_MLP_and_Training_from_Scratch",
    "Exercise_2.1:_Ruining_the_model",
    "Exercise_2.2:_MNIST_with_our_JAX_MLP",
    "Chapter_2.3_(BONUS):_Solving_ODE's_with_MLPs_and_Autodiff",
    "Chapter_2.4:_Some_notes_on_JAX_Prebuilt_Libraries",
    "Chapter_3:_PyTorch",
    "Chapter_3.1:_Primer_on_PyTorch_tensors_and_autodiff",
    "Chapter_3.2:_Pre-built_PyTorch_Modules",
    "Exercise_3.1:",
    "Chapter_3.3:_Convolutional_Neural_Networks_(CNNs)",
    "Chapter_3.4_(BONUS):_Permutation_Invariance_and_Equivariance_with_Deep_Sets_and_Transformers",
    "TOY_MODEL:",
    "Exercise:_Linear_Permutation_Invariance",
    "Chapter_4:_Tensorflow",
    "Chapter_4.1:_Defining_an_MLP_and_a_CNN_in_Keras",
    "CNNS_and_Callbacks",
    "Chapter_4.2:_Custom_Training_Loops_and_GradientTape",
    "Exercise:_Regularization",
    "CHAPTER_4.3_(BONUS):_GANS_and_Generative_Models",
    "Exercise:_Upgrade_to_a_Convolutional_GAN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}