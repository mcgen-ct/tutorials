{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Example_of_ML_applications"
   },
   "source": [
    "# Example of ML applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will work with the public [Top Quark Tagging Reference Dataset](https://zenodo.org/records/2603256), which provides the following description:\n",
    "\n",
    "A set of MC simulated training/testing events for the evaluation of top quark tagging architectures.\n",
    "\n",
    "In total 1.2M training events, 400k validation events and 400k test events. Use \u201ctrain\u201d for training, \u201cval\u201d for validation during the training and \u201ctest\u201d for final testing and reporting results.\n",
    "\n",
    "Description\n",
    "\n",
    "* 14 TeV, hadronic tops for signal, qcd diets background, Delphes ATLAS detector card with Pythia8\n",
    "\n",
    "* No MPI/pile-up included\n",
    "\n",
    "* Clustering of  particle-flow entries (produced by Delphes E-flow) into anti-kT 0.8 jets in the pT range [550,650] GeV\n",
    "\n",
    "* All top jets are matched to a parton-level top within \u2206R = 0.8, and to all top decay partons within 0.8\n",
    "\n",
    "* Jets are required to have |eta| < 2\n",
    "\n",
    "* The leading 200 jet constituent four-momenta are stored, with zero-padding for jets with fewer than 200\n",
    "\n",
    "* Constituents are sorted by pT, with the highest pT one first\n",
    "\n",
    "* The truth top four-momentum is stored as truth_px etc.\n",
    "\n",
    "* A flag (1 for top, 0 for QCD) is kept for each jet. It is called is_signal_new\n",
    "\n",
    "* The variable \"ttv\" (= test/train/validation) is kept for each jet. It indicates to which dataset the jet belongs. It is redundant as the different sets are already distributed as different files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataset, which you can download from the provided link, we will exemplify one  Machine Learning tasks: **Classification**.\n",
    "\n",
    "To do so we will need to install different packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, download and extract the Top_Quark_Tagging_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"train.h5\"\n",
    "path_val = \"val.h5\"\n",
    "path_test = \"test.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -N https://zenodo.org/records/2603256/files/train.h5\n",
    "!wget -q -N https://zenodo.org/records/2603256/files/val.h5\n",
    "!wget -q -N https://zenodo.org/records/2603256/files/test.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_path(path):\n",
    "    return os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    check_dataset_path(path_train)\n",
    "    * check_dataset_path(path_val)\n",
    "    * check_dataset_path(path_test)\n",
    "    == 0\n",
    "):\n",
    "    print(\"Path not properly set\")\n",
    "else:\n",
    "    print(\"All good here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Data_preprocessing"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the datasets. These are a bit heavy, but things should be okay.\n",
    "\n",
    "To be safe against overfitting, every operation should be decided on the training dataset and applied without changes to the validation and testing datasets.\n",
    "\n",
    "Additionally, to save some memory we won't load the full training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf(path_train, key=\"table\", start=0, stop=500000)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_hdf(path_val, key=\"table\", start=0, stop=100000)\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_hdf(path_test,key='table')\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what the features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how we have a **balanced dataset** (the mean of \"is_signal_new\" is 0.5. Let's distinguish between features and labels. Also, truthE, truthpx, truthpy, truthpz should be apart since it's only valid for tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.drop(\n",
    "    columns=[\"truthE\", \"truthPX\", \"truthPY\", \"truthPZ\", \"ttv\", \"is_signal_new\"]\n",
    ")\n",
    "train_labels = train[\"is_signal_new\"]\n",
    "top_momenta = train[[\"truthE\", \"truthPX\", \"truthPY\", \"truthPZ\"]][train_labels == 0]\n",
    "print(train_features.shape, train_labels.shape, top_momenta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = val.drop(\n",
    "    columns=[\"truthE\", \"truthPX\", \"truthPY\", \"truthPZ\", \"ttv\", \"is_signal_new\"]\n",
    ")\n",
    "val_labels = val[\"is_signal_new\"]\n",
    "val_top_momenta = val[[\"truthE\", \"truthPX\", \"truthPY\", \"truthPZ\"]][val_labels == 0]\n",
    "print(val_features.shape, val_labels.shape, val_top_momenta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(train_labels == 0), train_features[train_labels == 0].shape, np.sum(\n",
    "    train_labels == 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[train_labels == 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start exploring the distributions to see how things look like. For instance, the $p_T$ distribution of the leading constituent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leading_pT = np.sqrt(train_features[\"PX_0\"] ** 2 + train_features[\"PY_0\"] ** 2)\n",
    "plt.hist(\n",
    "    leading_pT[train_labels == 0],\n",
    "    bins=np.arange(0, 600, 10),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    leading_pT[train_labels == 1],\n",
    "    bins=np.arange(0, 600, 10),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Leading constituent $p_T$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see a nice difference! The Top is softer than the background for the leading constituent. We can explain this by looking at the jet mass distribution. Because the cosntituents are zero-padded, we can obtain the total momentum by summing carefully. I prefer to work in numpy, so I'll transform everything to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "top_momenta = top_momenta.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = val_features.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "val_top_momenta = val_top_momenta.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalE = np.sum(train_features[:, 0:801:4], axis=1)\n",
    "totalPX = np.sum(train_features[:, 1:801:4], axis=1)\n",
    "totalPY = np.sum(train_features[:, 2:801:4], axis=1)\n",
    "totalPZ = np.sum(train_features[:, 3:801:4], axis=1)\n",
    "print(totalE.shape, totalPX.shape, totalPY.shape, totalPZ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mass = totalE**2 - totalPX**2 - totalPY**2 - totalPZ**2\n",
    "total_mass = np.where(total_mass > 0, np.sqrt(total_mass), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    total_mass[train_labels == 0],\n",
    "    bins=np.arange(0, 350, 10),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    total_mass[train_labels == 1],\n",
    "    bins=np.arange(0, 350, 10),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Jet mass\")\n",
    "plt.savefig(\"mass.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jet mass is centered around the top mass for Top jets, while it's softer for QCD jets. Is there any relationship between mass and the leading constituent $p_T$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(total_mass, leading_pT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(total_mass[train_labels == 0], leading_pT[train_labels == 0], c=\"blue\")\n",
    "plt.scatter(total_mass[train_labels == 1], leading_pT[train_labels == 1], c=\"red\")\n",
    "plt.xlabel(\"Jet mass\")\n",
    "plt.ylabel(\"Leading $p_T$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, yes, but this doesn't look particularly useful. We can also look at other **global observables** . To do everything, its useful to compute the multiplicity. Because the energy is positive, we just need to check what's the first zero energy. We define a mask that can be used later on when using graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = train_features[:, 0:801:4] > 0\n",
    "multiplicity = np.argmin(train_mask, axis=1)\n",
    "print(multiplicity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask = val_features[:, 0:801:4] > 0\n",
    "val_multiplicity = np.argmin(val_mask, axis=1)\n",
    "print(val_multiplicity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    multiplicity[train_labels == 0],\n",
    "    bins=np.arange(-0.5, 200.5, 1),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    multiplicity[train_labels == 1],\n",
    "    bins=np.arange(-0.5, 200.5, 1),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Multiplicity\")\n",
    "plt.savefig(\"multiplicity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(total_mass[train_labels == 0], multiplicity[train_labels == 0], c=\"blue\")\n",
    "plt.scatter(total_mass[train_labels == 1], multiplicity[train_labels == 1], c=\"red\")\n",
    "plt.xlabel(\"Jet mass\")\n",
    "plt.ylabel(\"Multiplicity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the average $p_T$ (it's not the best observable but it's faster to compute). To do this, we need to move from (E,px,py,pz) to (pT,eta,phi,mass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Epxpypz_to_pTetaphimass(vec):\n",
    "    E, px, py, pz = vec[0], vec[1], vec[2], vec[3]\n",
    "    pT = np.sqrt(vec[1] ** 2 + vec[2] ** 2)\n",
    "    phi = np.arctan2(vec[2], vec[2])\n",
    "    mass = (\n",
    "        np.sqrt(E**2 - px**2 - py**2 - pz**2)\n",
    "        if E**2 - px**2 - py**2 - pz**2 > 0\n",
    "        else 0\n",
    "    )\n",
    "    eta = np.arctanh(pz / np.sqrt(px**2 + py**2 + pz**2))\n",
    "    return np.array([pT, eta, phi, mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_pT = np.zeros(len(train_features))\n",
    "for nevent in range(len(train_features)):\n",
    "    av_pT[nevent] = np.mean(\n",
    "        np.sqrt(\n",
    "            train_features[nevent, 1 : 4 * multiplicity[nevent] : 4] ** 2\n",
    "            + train_features[nevent, 2 : 4 * multiplicity[nevent] : 4] ** 2\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevent, len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    av_pT[train_labels == 0],\n",
    "    bins=np.arange(-0.5, 200.5, 1),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    av_pT[train_labels == 1],\n",
    "    bins=np.arange(-0.5, 200.5, 1),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(r\"$\\langle p_{T} \\rangle$\")\n",
    "plt.savefig(\"av_pT.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    total_mass[train_labels == 0], av_pT[train_labels == 0], c=\"blue\", label=\"QCD\"\n",
    ")\n",
    "plt.scatter(\n",
    "    total_mass[train_labels == 1], av_pT[train_labels == 1], c=\"red\", label=\"Top\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Jet mass\")\n",
    "plt.ylabel(r\"$\\langle p_{T} \\rangle$\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(\"mass_av_pT.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Baseline_Classifier"
   },
   "source": [
    "# Baseline Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these two features to train a baseline classifier, let's use the simplest possible: a logistic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack([total_mass, av_pT]).T\n",
    "print(X_train.shape)\n",
    "lr.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_lr = lr.predict_proba(X_train)[:, 1]\n",
    "assignment_lr = lr.predict(X_train)  # it's just 0 if proba < 0.5 and 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the probability region looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    proba_lr[train_labels == 0],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    proba_lr[train_labels == 1],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.savefig(\"proba_lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 350, 100)\n",
    "x2 = np.linspace(0, 200, 100)\n",
    "X1toplot, X2toplot = np.meshgrid(x1, x2)\n",
    "# plt.xlim(0.0,0.2)\n",
    "# plt.ylim(0.0,0.2)\n",
    "plt.scatter(\n",
    "    total_mass[train_labels == 0],\n",
    "    av_pT[train_labels == 0],\n",
    "    c=\"blue\",\n",
    "    alpha=0.1,\n",
    "    label=\"QCD\",\n",
    ")\n",
    "plt.scatter(\n",
    "    total_mass[train_labels == 1],\n",
    "    av_pT[train_labels == 1],\n",
    "    c=\"red\",\n",
    "    alpha=0.1,\n",
    "    label=\"Top\",\n",
    ")\n",
    "Z = (\n",
    "    np.asarray(\n",
    "        [\n",
    "            lr.predict_proba(np.asarray([el[0], el[1]]).reshape(1, -1))[0, 1]\n",
    "            for el in np.c_[X1toplot.ravel(), X2toplot.ravel()]\n",
    "        ]\n",
    "    )\n",
    ").reshape(X1toplot.shape)\n",
    "\n",
    "contour = plt.contourf(X1toplot, X2toplot, Z, levels=[0, 0.5, 1], alpha=0.6)\n",
    "plt.colorbar(contour)\n",
    "\n",
    "\n",
    "plt.xlim(0, 350)\n",
    "plt.ylim(0, 200)\n",
    "plt.xlabel(\"Jet mass\")\n",
    "plt.ylabel(r\"$\\langle p_{T} \\rangle$\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(\"classifier_space_lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we need metrics. The logistic regressor outputs a probability and a hard assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(\n",
    "    accuracy_score(train_labels, assignment_lr), roc_auc_score(train_labels, proba_lr)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the confusion matrix: row is true class, column predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels, assignment_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how many more QCD are tagged as tops than QCD as tops. This can be seen from the 2d plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ROC Curve. There are many ways to plot, we'll do (FPR,TPR) so that up and left is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_lr, tpr_lr, thr_lr = roc_curve(train_labels, proba_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    fpr_lr,\n",
    "    tpr_lr,\n",
    "    label=\"Logistic Regression \" + str(round(roc_auc_score(train_labels, proba_lr), 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, 10),\n",
    "    np.linspace(0, 1, 10),\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"black\",\n",
    "    label=\"Random\",\n",
    ")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_total_mass = (\n",
    "    np.sum(val_features[:, 0:801:4], axis=1) ** 2\n",
    "    - np.sum(val_features[:, 1:801:4], axis=1) ** 2\n",
    "    - np.sum(val_features[:, 2:801:4], axis=1) ** 2\n",
    "    - np.sum(val_features[:, 3:801:4], axis=1) ** 2\n",
    ")\n",
    "val_total_mass = np.where(val_total_mass > 0, np.sqrt(val_total_mass), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_total_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_av_pT = np.zeros(len(val_features))\n",
    "for nevent in range(len(val_features)):\n",
    "    val_av_pT[nevent] = np.mean(\n",
    "        np.sqrt(\n",
    "            val_features[nevent, 1 : 4 * val_multiplicity[nevent] : 4] ** 2\n",
    "            + val_features[nevent, 2 : 4 * val_multiplicity[nevent] : 4] ** 2\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.vstack([val_total_mass, val_av_pT]).T\n",
    "print(X_val.shape)\n",
    "val_proba_lr = lr.predict_proba(X_val)[:, 1]\n",
    "val_assignmnet_lr = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    val_proba_lr[val_labels == 0],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    val_proba_lr[val_labels == 1],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    accuracy_score(val_labels, val_assignmnet_lr),\n",
    "    roc_auc_score(val_labels, val_proba_lr),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fpr_lr, val_tpr_lr, val_thr_lr = roc_curve(val_labels, val_proba_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    val_fpr_lr,\n",
    "    val_tpr_lr,\n",
    "    label=\"Logistic Regression \"\n",
    "    + str(round(roc_auc_score(val_labels, val_proba_lr), 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, 10),\n",
    "    np.linspace(0, 1, 10),\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"black\",\n",
    "    label=\"Random\",\n",
    ")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:"
   },
   "source": [
    "## Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo this using the $C_{3}$ coefficient defined [here](https://arxiv.org/pdf/1305.0007) instead of the average $p_T$. It's much slower to compute, that's why we do not use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_home-made_DeepSets_classifier:_[ParticleFlow_Network](http://arxiv.org/abs/1810.05165)"
   },
   "source": [
    "# A home-made DeepSets classifier: [ParticleFlow Network](http://arxiv.org/abs/1810.05165)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a ParticleFlow network but in 4-momenta space, without using the ($p_{T}$,$\\eta$,$\\phi$) representation. This will probably cost some performance, but it takes a bit longer to preprocess 1M events. You probably want to do that (and maybe try a CNN there as well!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom Dataset and a custom Neural Network class in Pytorch.\n",
    "\n",
    "Our dataset should have features, label and mask to indicate which particles are zero-padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Joins the x and y into a dataset, so that it can be used by the pythorch syntax.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, m):\n",
    "        self.x = torch.tensor(x).to(torch.float)\n",
    "        self.y = torch.tensor(y).to(torch.float)\n",
    "        self.m = torch.tensor(m).to(torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [self.x[idx], self.y[idx], self.m[idx]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standarize the dataset,otherwise the training gets really hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxE = np.max(train_features[:, 0:801:4])\n",
    "maxPX = np.max(np.abs(train_features[:, 1:801:4]))\n",
    "maxPY = np.max(np.abs(train_features[:, 2:801:4]))\n",
    "maxPZ = np.max(np.abs(train_features[:, 3:801:4]))\n",
    "\n",
    "meanE = np.mean(train_features[:, 0:801:4].flatten()[train_mask.flatten()])\n",
    "meanPX = np.mean(train_features[:, 1:801:4].flatten()[train_mask.flatten()])\n",
    "meanPY = np.mean(train_features[:, 2:801:4].flatten()[train_mask.flatten()])\n",
    "meanPZ = np.mean(train_features[:, 3:801:4].flatten()[train_mask.flatten()])\n",
    "\n",
    "stdE = np.std(train_features[:, 0:801:4].flatten()[train_mask.flatten()])\n",
    "stdPX = np.std(train_features[:, 1:801:4].flatten()[train_mask.flatten()])\n",
    "stdPY = np.std(train_features[:, 2:801:4].flatten()[train_mask.flatten()])\n",
    "stdPZ = np.std(train_features[:, 3:801:4].flatten()[train_mask.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maxE, maxPX, maxPY, maxPZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meanE, meanPX, meanPY, meanPZ)\n",
    "print(stdE, stdPX, stdPY, stdPZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features[:,0:801:4]*=1/maxE\n",
    "train_features[:,1:801:4]*=1/maxPX\n",
    "train_features[:,2:801:4]*=1/maxPY\n",
    "train_features[:,3:801:4]*=1/maxPZ\n",
    "\"\"\"\n",
    "train_features[:, 0:801:4] = (train_features[:, 0:801:4] - meanE) / stdE\n",
    "train_features[:, 1:801:4] = (train_features[:, 1:801:4] - meanPX) / stdPX\n",
    "train_features[:, 2:801:4] = (train_features[:, 2:801:4] - meanPY) / stdPY\n",
    "train_features[:, 3:801:4] = (train_features[:, 3:801:4] - meanPZ) / stdPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_features[:, 0:801:4].flatten()[train_mask.flatten()], bins=100)\n",
    "plt.xlabel(\"Standarized Energy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_features[:, 1:801:4].flatten()[train_mask.flatten()], bins=100)\n",
    "plt.xlabel(\"Standarized $p_x$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standarize the validation dataset using the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features[:, 0:801:4] = (val_features[:, 0:801:4] - meanE) / stdE\n",
    "val_features[:, 1:801:4] = (val_features[:, 1:801:4] - meanPX) / stdPX\n",
    "val_features[:, 2:801:4] = (val_features[:, 2:801:4] - meanPY) / stdPY\n",
    "val_features[:, 3:801:4] = (val_features[:, 3:801:4] - meanPZ) / stdPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch = CustomDataset(\n",
    "    train_features.reshape((len(train_features), 200, 4)), train_labels, train_mask\n",
    ")\n",
    "val_torch = CustomDataset(\n",
    "    val_features.reshape((len(val_features), 200, 4)), val_labels, val_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "del train\n",
    "del X_train\n",
    "del val\n",
    "del X_val\n",
    "del train_features\n",
    "del val_features\n",
    "del train_mask\n",
    "del val_mask\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model consists of two neural networks, one evaluated per particle and one evaluated over all particles in an event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class PseudoParticleFlow(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input=4,\n",
    "        inner_dim=64,\n",
    "        layers_Phi=[(64, nn.ReLU()), (64, nn.ReLU()), (64, nn.ReLU())],\n",
    "        layers_F=[(64, nn.ReLU()), (64, nn.ReLU()), (64, nn.ReLU())],\n",
    "    ):  # example of layers_data=[(layer1, nn.ReLU()), (layer2, nn.ReLU()), (output_size, nn.Sigmoid())]\n",
    "        super(PseudoParticleFlow, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Phi = nn.ModuleList()\n",
    "        self.F = nn.ModuleList()\n",
    "        self.input_size = dim_input  # Can be useful later ...\n",
    "        self.inner_dim = inner_dim  # Can be useful later ...\n",
    "        input_size = dim_input\n",
    "        for size, activation in layers_Phi:\n",
    "            self.Phi.append(nn.Linear(input_size, size))\n",
    "            input_size = size  # For the next layer\n",
    "            if activation is not None:\n",
    "                assert isinstance(\n",
    "                    activation, Module\n",
    "                ), \"Each tuples should contain a size (int) and a torch.nn.modules.Module.\"\n",
    "                self.Phi.append(activation)\n",
    "        self.Phi.append(nn.Linear(input_size, inner_dim))\n",
    "\n",
    "        input_size = inner_dim\n",
    "        for size, activation in layers_F:\n",
    "            self.F.append(nn.Linear(input_size, size))\n",
    "            input_size = size  # For the next layer\n",
    "            if activation is not None:\n",
    "                assert isinstance(\n",
    "                    activation, Module\n",
    "                ), \"Each tuples should contain a size (int) and a torch.nn.modules.Module.\"\n",
    "                self.F.append(activation)\n",
    "        self.F.append(nn.Linear(input_size, 1))\n",
    "        self.F.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        output = x\n",
    "        for layer in self.Phi:\n",
    "            output = layer(output)\n",
    "            # print(output.shape)\n",
    "        ### this uses the mask to remove zero-padded particles when performing the averaging\n",
    "        output = torch.einsum(\"ijk,ij->ik\", output, m) / (\n",
    "            torch.einsum(\"ik,ik->i\", m, m).unsqueeze(1)\n",
    "        )\n",
    "        # output = torch.stack((torch.masked.masked_tensor(output[:,:,0], m),torch.masked.masked_tensor(output[:,:,0], m)),dim=-1)\n",
    "        # print(output.shape)\n",
    "        # output = torch.mean(output,1)\n",
    "        # print(output.shape)\n",
    "        for layer in self.F:\n",
    "            output = layer(output)\n",
    "            # print(output.shape)\n",
    "        return output\n",
    "\n",
    "    def latent(self, x, m):\n",
    "        output = x\n",
    "        for layer in self.Phi:\n",
    "            output = layer(output)\n",
    "            # print(output.shape)\n",
    "        ### this uses the mask to remove zero-padded particles when performing the averaging\n",
    "        output = torch.einsum(\"ijk,ij->ik\", output, m) / (\n",
    "            torch.einsum(\"ik,ik->i\", m, m).unsqueeze(1)\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def proba_from_latent(self, ell):\n",
    "        output = ell\n",
    "        for layer in self.F:\n",
    "            output = layer(output)\n",
    "            # print(output.shape)\n",
    "        return output\n",
    "\n",
    "    def loss_function(self, y, t):\n",
    "        # loss_fn = nn.MSELoss(reduction='mean')\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        return loss_fn(y, t)\n",
    "\n",
    "    def reset_weights(self):\n",
    "        for m in self.Phi:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()\n",
    "        for m in self.F:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define training, testing and predicting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, batch_size=1024):\n",
    "    model.train()\n",
    "    device = torch.device(\"cpu\")  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_dataset_batched = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    total_loss = 0\n",
    "    for batch, (X, y, M) in enumerate(train_dataset_batched):\n",
    "        X, y, M = X.to(device), y.to(device), M.to(device)\n",
    "        # print(torch.mean(y))\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        pred = model(X, M)[:, 0]  # Forward pass.\n",
    "        # print(pred.shape,y.shape)\n",
    "        loss = model.loss_function(pred, y)  # Loss computation.\n",
    "        loss.backward()  # Backward pass.\n",
    "        optimizer.step()  # Update model parameters.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_dataset_batched.dataset)\n",
    "\n",
    "\n",
    "def test(model, test_dataset, batch_size=1024):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cpu\")  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_dataset_batched = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    total_loss = 0\n",
    "    for batch, (X, y, M) in enumerate(test_dataset_batched):\n",
    "        X, y, M = X.to(device), y.to(device), M.to(device)\n",
    "        pred = model(X, M)[:, 0]  # Forward pass.\n",
    "        loss = model.loss_function(pred, y)  # Loss computation.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_dataset_batched.dataset)\n",
    "\n",
    "\n",
    "def predict_proba(model, test_dataset, batch_size=1024):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cpu\")  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_dataset_batched = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    truths = np.zeros(len(test_dataset))\n",
    "    preds = np.zeros(len(test_dataset))\n",
    "    initial_index = 0\n",
    "    final_index = 0\n",
    "    for batch, (X, y, M) in enumerate(test_dataset_batched):\n",
    "        final_index += len(y)\n",
    "        truths[initial_index:final_index] = y.detach().cpu().numpy()\n",
    "        X, y, M = X.to(device), y.to(device), M.to(device)\n",
    "        pred = model(X, M)[:, 0]  # Forward pass.\n",
    "        preds[initial_index:final_index] = pred.detach().cpu().numpy()\n",
    "        initial_index += len(y)\n",
    "    return truths, preds\n",
    "\n",
    "\n",
    "def predict(model, test_dataset, batch_size=1024):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cpu\")  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_dataset_batched = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    truths = np.zeros(len(test_dataset))\n",
    "    preds = np.zeros(len(test_dataset))\n",
    "    initial_index = 0\n",
    "    final_index = 0\n",
    "    for batch, (X, y, M) in enumerate(test_dataset_batched):\n",
    "        final_index += len(y)\n",
    "        truths[initial_index:final_index] = y.detach().cpu().numpy()\n",
    "        X, y, M = X.to(device), y.to(device), M.to(device)\n",
    "        pred = model(X, M)[:, 0]  # Forward pass.\n",
    "        preds[initial_index:final_index] = pred.detach().cpu().numpy()\n",
    "        preds[initial_index:final_index] = np.where(\n",
    "            preds[initial_index:final_index] > 0.5, 1, 0\n",
    "        )\n",
    "        initial_index += len(y)\n",
    "    return truths, preds\n",
    "\n",
    "\n",
    "def latent(model, test_dataset, batch_size=1024):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cpu\")  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_dataset_batched = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    latent_variables = np.zeros((len(test_dataset), model.inner_dim))\n",
    "    initial_index = 0\n",
    "    final_index = 0\n",
    "    for batch, (X, y, M) in enumerate(test_dataset_batched):\n",
    "        final_index += len(y)\n",
    "        X, y, M = X.to(device), y.to(device), M.to(device)\n",
    "        pred = model.latent(X, M)  # Forward pass.\n",
    "        latent_variables[initial_index:final_index] = pred.detach().cpu().numpy()\n",
    "        initial_index += len(y)\n",
    "    return latent_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we initialize our model. We can save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PseudoParticleFlow()\n",
    "model_path = \"saved_model.pth\"\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded existing model\")\n",
    "except:\n",
    "    print(\"New model\")\n",
    "device = torch.device(\"cpu\")  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always start over by resetting the weights as commented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.reset_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the optimizer that we'll use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 30\n",
    "npatience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.einsum(\"ik,ik->i\", train_torch[:2][2], train_torch[:2][2]))\n",
    "print(\n",
    "    torch.einsum(\"ijk,ij->ik\", train_torch[:2][0], train_torch[:2][2])\n",
    "    / (torch.einsum(\"ik,ik->i\", train_torch[:2][2], train_torch[:2][2]).unsqueeze(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalE[0]/(maxE*multiplicity[0]),totalE[1]/(maxE*multiplicity[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's train. We'll use the validation to test to avoid overfitting (this you shouldn't do if you're actually using the validation dataset to select the best model, you should either cross-validate or separate the training into actual training and evaluation datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_aux = 1000\n",
    "times = 0\n",
    "for epoch in range(1, nepochs + 1):\n",
    "    loss = train(model, optimizer, train_torch, batch_size=1024)\n",
    "    val_loss = test(model, val_torch, batch_size=1024)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:02d}, Loss: {loss:.8f},Val Loss: {val_loss:.8f}, Learning Rate: {learning_rate:.6f}\"\n",
    "    )\n",
    "    if epoch == 0:\n",
    "        val_loss_aux = val_loss\n",
    "        times = 0\n",
    "        continue\n",
    "\n",
    "    if loss < val_loss_aux:\n",
    "        val_loss_aux = val_loss\n",
    "        times = 0\n",
    "    else:\n",
    "        times += 1\n",
    "\n",
    "    if times == npatience:\n",
    "        learning_rate *= 0.1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if times == 2 * npatience:\n",
    "        break\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is slightly trained, we can obtain the predictions and do the same analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_NN, val_proba_NN = predict_proba(model, val_torch, batch_size=128)\n",
    "val_assignmnet_NN = np.where(val_proba_NN > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(val_labels_NN, val_assignmnet_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    val_proba_NN[val_labels_NN == 0],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    val_proba_NN[val_labels_NN == 1],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.savefig(\"proba_NN.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    accuracy_score(val_labels_NN, val_assignmnet_NN),\n",
    "    roc_auc_score(val_labels_NN, val_proba_NN),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fpr_NN, val_tpr_NN, val_thr_NN = roc_curve(val_labels_NN, val_proba_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    val_fpr_lr,\n",
    "    val_tpr_lr,\n",
    "    label=\"Logistic Regression \"\n",
    "    + str(round(roc_auc_score(val_labels, val_proba_lr), 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    val_fpr_NN,\n",
    "    val_tpr_NN,\n",
    "    label=\"Deep Sets \" + str(round(roc_auc_score(val_labels_NN, val_proba_NN), 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, 10),\n",
    "    np.linspace(0, 1, 10),\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"black\",\n",
    "    label=\"Random\",\n",
    ")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"roc_NN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the performance is not that impressive. This is because the architecture is not huge and we are not using the full dataset and training for a long time.\n",
    "\n",
    "Now the interpretation is more difficult of course...\n",
    "\n",
    "We can try to interpret the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_latent_variables_NN = latent(model, val_torch, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_latent_variables_NN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    val_latent_variables_NN[val_labels_NN == 0, 0],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    val_latent_variables_NN[val_labels_NN == 1, 0],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Latent variable 1\")\n",
    "plt.savefig(\"latent_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    val_latent_variables_NN[val_labels_NN == 0, 1],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"blue\",\n",
    "    label=\"QCD\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    val_latent_variables_NN[val_labels_NN == 1, 1],\n",
    "    bins=np.linspace(0, 1, 50),\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    label=\"Top\",\n",
    "    density=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Latent variable 2\")\n",
    "plt.savefig(\"latent_2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nlatent_dim in range(model.inner_dim):\n",
    "    plt.hist(\n",
    "        val_latent_variables_NN[val_labels_NN == 0, nlatent_dim],\n",
    "        bins=np.linspace(0, 1, 50),\n",
    "        histtype=\"step\",\n",
    "        color=\"blue\",\n",
    "        label=\"QCD\",\n",
    "        density=True,\n",
    "    )\n",
    "    plt.hist(\n",
    "        val_latent_variables_NN[val_labels_NN == 1, nlatent_dim],\n",
    "        bins=np.linspace(0, 1, 50),\n",
    "        histtype=\"step\",\n",
    "        color=\"red\",\n",
    "        label=\"Top\",\n",
    "        density=True,\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.xlabel(\"Latent variable \" + str(nlatent_dim + 1))\n",
    "    # plt.savefig('latent_'+str(nlatent_dim+1)+'.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nlatent_dim1 in range(model.inner_dim):\n",
    "    for nlatent_dim2 in range(nlatent_dim1 + 1, model.inner_dim):\n",
    "        if nlatent_dim1 > 2 or nlatent_dim2 > 3:\n",
    "            break\n",
    "        plt.scatter(\n",
    "            val_latent_variables_NN[val_labels_NN == 0, nlatent_dim1],\n",
    "            val_latent_variables_NN[val_labels_NN == 0, nlatent_dim2],\n",
    "            c=\"blue\",\n",
    "            alpha=0.1,\n",
    "            label=\"QCD\",\n",
    "        )\n",
    "        plt.scatter(\n",
    "            val_latent_variables_NN[val_labels_NN == 1, nlatent_dim1],\n",
    "            val_latent_variables_NN[val_labels_NN == 1, nlatent_dim2],\n",
    "            c=\"red\",\n",
    "            alpha=0.1,\n",
    "            label=\"Top\",\n",
    "        )\n",
    "\n",
    "        # plt.xlim(0,1)\n",
    "        # plt.ylim(0,1)\n",
    "        plt.xlabel(\"Latent variable \" + str(nlatent_dim1 + 1))\n",
    "        plt.ylabel(\"Latent variable \" + str(nlatent_dim2 + 1))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        # plt.savefig('classifier_space_NN'+str(nlatent_dim1+1)+'_'+str(nlatent_dim2+1)+'.png')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the latent dimension is equal to 2, we can do the same plots as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.inner_dim == 2:\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "    X1toplot, X2toplot = np.meshgrid(x1, x2)\n",
    "    # plt.xlim(0.0,0.2)\n",
    "    # plt.ylim(0.0,0.2)\n",
    "    plt.scatter(\n",
    "        val_latent_variables_NN[val_labels_NN == 0, 0],\n",
    "        val_latent_variables_NN[val_labels_NN == 0, 1],\n",
    "        c=\"blue\",\n",
    "        alpha=0.1,\n",
    "        label=\"QCD\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        val_latent_variables_NN[val_labels_NN == 1, 0],\n",
    "        val_latent_variables_NN[val_labels_NN == 1, 1],\n",
    "        c=\"red\",\n",
    "        alpha=0.1,\n",
    "        label=\"Top\",\n",
    "    )\n",
    "    Z = (\n",
    "        np.asarray(\n",
    "            [\n",
    "                model.proba_from_latent(\n",
    "                    torch.tensor(\n",
    "                        np.asarray([el[0], el[1]]).reshape(1, -1), dtype=torch.float\n",
    "                    )\n",
    "                )\n",
    "                .detach()\n",
    "                .numpy()[0]\n",
    "                for el in np.c_[X1toplot.ravel(), X2toplot.ravel()]\n",
    "            ]\n",
    "        )\n",
    "    ).reshape(X1toplot.shape)\n",
    "\n",
    "    contour = plt.contourf(X1toplot, X2toplot, Z, levels=[0, 0.5, 1], alpha=0.6)\n",
    "    plt.colorbar(contour)\n",
    "\n",
    "    # plt.xlim(0,1)\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlabel(\"Latent variable 1\")\n",
    "    plt.ylabel(\"Latent variable 2\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(\"classifier_space_NN.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercises"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repeat this with different inner dimension (don't break your computer!). To explore the latent space, you can use tools like t-SNE or PCA\n",
    "* Reframe this as a regression task, learn the $C_3$ coefficient from the constituents. This could use the same architecture but a different loss function.\n",
    "* Use a similar architecture for a GAN to generate tops or QCD jets (or both! but you'll find differences)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Data_preprocessing",
    "Baseline_Classifier",
    "Exercise:",
    "A_home-made_DeepSets_classifier:_[ParticleFlow_Network](http://arxiv.org/abs/1810.05165)",
    "Exercises"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}