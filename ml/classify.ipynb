{
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple ML introducion\n",
    "\n",
    "Written by:\n",
    "- Manuel Szewc (School of Physics, University of Cincinnati)\n",
    "- Philip Ilten (School of Physics, University of Cincinnati)\n",
    "$\\renewcommand{\\gtrsim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{>}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\lessim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{<}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\as}{\\alpha_{\\mathrm{s}}}\\renewcommand{\\aem}{\\alpha_{\\mathrm{em}}}\\renewcommand{\\kT}{k_{\\perp}}\\renewcommand{\\pT}{p_{\\perp}}\\renewcommand{\\pTs}{p^2_{\\perp}}\\renewcommand{\\pTe}{\\p_{\\perp\\mrm{evol}}}\\renewcommand{\\pTse}{\\p^2_{\\perp\\mrm{evol}}}\\renewcommand{\\pTmin}{p_{\\perp\\mathrm{min}}}\\renewcommand{\\pTsmim}{p^2_{\\perp\\mathrm{min}}}\\renewcommand{\\pTmax}{p_{\\perp\\mathrm{max}}}\\renewcommand{\\pTsmax}{p^2_{\\perp\\mathrm{max}}}\\renewcommand{\\pTL}{p_{\\perp\\mathrm{L}}}\\renewcommand{\\pTD}{p_{\\perp\\mathrm{D}}}\\renewcommand{\\pTA}{p_{\\perp\\mathrm{A}}}\\renewcommand{\\pTsL}{p^2_{\\perp\\mathrm{L}}}\\renewcommand{\\pTsD}{p^2_{\\perp\\mathrm{D}}}\\renewcommand{\\pTsA}{p^2_{\\perp\\mathrm{A}}}\\renewcommand{\\pTo}{p_{\\perp 0}}\\renewcommand{\\shat}{\\hat{s}}\\renewcommand{\\a}{{\\mathrm a}}\\renewcommand{\\b}{{\\mathrm b}}\\renewcommand{\\c}{{\\mathrm c}}\\renewcommand{\\d}{{\\mathrm d}}\\renewcommand{\\e}{{\\mathrm e}}\\renewcommand{\\f}{{\\mathrm f}}\\renewcommand{\\g}{{\\mathrm g}}\\renewcommand{\\hrm}{{\\mathrm h}}\\renewcommand{\\lrm}{{\\mathrm l}}\\renewcommand{\\n}{{\\mathrm n}}\\renewcommand{\\p}{{\\mathrm p}}\\renewcommand{\\q}{{\\mathrm q}}\\renewcommand{\\s}{{\\mathrm s}}\\renewcommand{\\t}{{\\mathrm t}}\\renewcommand{\\u}{{\\mathrm u}}\\renewcommand{\\A}{{\\mathrm A}}\\renewcommand{\\B}{{\\mathrm B}}\\renewcommand{\\D}{{\\mathrm D}}\\renewcommand{\\F}{{\\mathrm F}}\\renewcommand{\\H}{{\\mathrm H}}\\renewcommand{\\J}{{\\mathrm J}}\\renewcommand{\\K}{{\\mathrm K}}\\renewcommand{\\L}{{\\mathrm L}}\\renewcommand{\\Q}{{\\mathrm Q}}\\renewcommand{\\R}{{\\mathrm R}}\\renewcommand{\\T}{{\\mathrm T}}\\renewcommand{\\W}{{\\mathrm W}}\\renewcommand{\\Z}{{\\mathrm Z}}\\renewcommand{\\bbar}{\\overline{\\mathrm b}}\\renewcommand{\\cbar}{\\overline{\\mathrm c}}\\renewcommand{\\dbar}{\\overline{\\mathrm d}}\\renewcommand{\\fbar}{\\overline{\\mathrm f}}\\renewcommand{\\pbar}{\\overline{\\mathrm p}}\\renewcommand{\\qbar}{\\overline{\\mathrm q}}\\renewcommand{\\rbar}{\\overline{\\mathrm{r}}}\\renewcommand{\\sbar}{\\overline{\\mathrm s}}\\renewcommand{\\tbar}{\\overline{\\mathrm t}}\\renewcommand{\\ubar}{\\overline{\\mathrm u}}\\renewcommand{\\Bbar}{\\overline{\\mathrm B}}\\renewcommand{\\Fbar}{\\overline{\\mathrm F}}\\renewcommand{\\Qbar}{\\overline{\\mathrm Q}}\\renewcommand{\\tms}{{t_{\\mathrm{\\tiny MS}}}}\\renewcommand{\\Oas}[1]{{\\mathcal{O}\\left(\\as^{#1}\\right)}}$"
   ],
   "metadata": {
    "id": "Simple_ML_introducion"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {
    "id": "Introduction"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook wants to implement simple Machine Learning algorithms for classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# To generate data and handle arrays\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc(\"axes\", labelsize=14)\n",
    "mpl.rc(\"xtick\", labelsize=12)\n",
    "mpl.rc(\"ytick\", labelsize=12)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification"
   ],
   "metadata": {
    "id": "Classification"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In classification problems, we are interested in predicting a class asignment $\\mathcal{C}_{k}$ to a measurement $x$. The training data consists of paired measurements and class labels $x_{\\text{train}},t_{\\text{train}}$.\n",
    "\n",
    "There are three types of classification approaches:\n",
    "\n",
    "* Discriminant functions, where we learn a function $y(x,w)$ that defines the border between classes. For the binary case, and using $t \\in \\{-1,1\\}$ , this corresponds to definen a threshold $y_{0}$ (usually zero) such that if $y(x,w)>y_{0}$, $t = 1$ and $t=-1$ otherwise. This is the case for `Perceptron` and `Support Vector Machines` among others. `DecisionTrees` can be thought of in these terms as well, although they can also estimate probabilities.\n",
    "* Discriminative models, where we learn a function $y(x,w)$ that encodes the probability of a class asignment given $x$, $p(\\mathcal{C}_{k}|x)$. A class asignment can be made by selecting the class which maximizes the probability, although other criteria can be applied as well. `LogisticRegressors` and simple `Neural Network Classifiers` are examples of this.\n",
    "* Generative models, where we learn a function $y(x,w)$ that encodes the probability of $x$ per class, $p(x|\\mathcal{C}_{k})$. A class asignment can be made by estimating the per class probabilities using Bayes' rule and selecting the class which maximizes this probability. `Naive Bayes` classifiers or conditional density estimators are examples of this.\n",
    "\n",
    "\n",
    "Let's focus more on discriminative models. Here, we are interested in the per-class probability, which is a **posterior** over class asignments.  For the binary case, we only need to specifiy $p(\\mathcal{C}_{1}|x)$ since $p(\\mathcal{C}_{2}|x)=1-p(\\mathcal{C}_{1}|x)$. We deal with the multiclass problem further down the notebook.\n",
    "\n",
    "For the binary case, we compute\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=\\frac{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})+p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}=\\frac{1}{1+\\frac{p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}}$$\n",
    "\n",
    "Defining the **log-odds ratio**\n",
    "\n",
    "$$a=\\text{Ln }\\frac{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}{p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}=\\text{Ln }\\frac{p(\\mathcal{C}_{1}|x)}{p(\\mathcal{C}_{2}|x)}$$\n",
    "\n",
    "we have that\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=\\frac{1}{1+\\text{e}^{-a}}=\\sigma(a)$$\n",
    "\n",
    "where $\\sigma(a)$ is the  **sigmoid function**. Thus, the binary problem is reduced to computing the log-odds ratio between the two classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "prob = np.linspace(-10, 10, 200)\n",
    "\n",
    "a_vals = 1 / (1 + np.exp(-prob))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(prob, a_vals, color=\"black\")\n",
    "ax.scatter(0.0, 0.5, color=\"red\")\n",
    "ax.axvline(0.0, linestyle=\"dashed\", color=\"red\")\n",
    "ax.axhline(0.5, linestyle=\"dashed\", color=\"red\")\n",
    "ax.set_ylabel(r\"$p(\\mathcal{C}_{1}|x)$\", fontsize=16)\n",
    "ax.set_xlabel(\"Log odds\", fontsize=16)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see how the sigmoid function is a function lying in $[0,1]$, even as the log odds ratio takes values in $(-\\infty,\\infty)$. This is necessary since we're interpreting it as a probability.\n",
    "\n",
    "A particularly important point is where the two classes are equally likely. There, the log odds ratio is 1 and the per class probability is 0.5, as it should. In general, this is selected as the decision boundary.\n",
    "\n",
    "Two useful properties of the sigmoid function are:\n",
    "\n",
    "$$\\sigma(-a)=1-\\sigma(a)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{d\\sigma}{da}=\\sigma(1-\\sigma)$$\n",
    "\n",
    "For the $K$ class case, we can generalize the sigmoid to the normalized exponential or **softmax** function:\n",
    "\n",
    "$$p(\\mathcal{C}_{k}|x)=\\frac{p(x|\\mathcal{C}_{k})p(\\mathcal{C}_{k})}{\\sum_{l=1}^{K}p(x|\\mathcal{C}_{l})p(\\mathcal{C}_{l})}=\\frac{e^{a_{k}}}{\\sum_{l=1}^{K}e^{a_{l}}}$$\n",
    "\n",
    "where $a_{k}=\\text{Ln }p(x|\\mathcal{C}_{k})p(\\mathcal{C}_{k})$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {
    "id": "Logistic_Regression"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The simplest discriminative classifier is the `Logistic Regressor`. For the two class case, we model the per class probability posterior as\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|\\vec{x})=y(\\vec{x},\\vec{w})=\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$$\n",
    "\n",
    "$$p(\\mathcal{C}_{0}|\\vec{x})=1-y(\\vec{x},\\vec{w})=1-\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))=\\sigma(-\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$$\n",
    "\n",
    "We already have two features of note:\n",
    "\n",
    "- $\\vec{w}^{T}\\vec{\\phi}(\\vec{x})$ acts as the decision function in a discriminant approach. In particular, $\\vec{w}^{T}\\vec{\\phi}(\\vec{x})=0$ defines the equi-probability surface.\n",
    "- The sigmoid is a **non-linear activation function**. This feature, necessary to transform a linear model into a probability estimate, will be very present in `Neural Networks` even for regression for different reasons (the universal approximation is ensured by internal non-linear activation functions between neuron layers).\n",
    "\n",
    "As in the generalized linear models for regression,  $\\vec{\\phi}$ includes in principle the bias with $\\phi_{0}(\\vec{x})=1$.\n",
    "\n",
    "As with regression, we can group all training pairs into a design matrix\n",
    "\n",
    "$$\\Phi=\\begin{pmatrix}\\vec{\\phi}^{T}(\\vec{x}_{1}) \\\\ ... \\\\ \\vec{\\phi}^{T}(\\vec{x}_{N})\\end{pmatrix}$$\n",
    "\n",
    "obtaining\n",
    "\n",
    "$$\\sigma(\\Phi\\cdot \\vec{w}) = \\begin{pmatrix}\\sigma(\\vec{\\phi}^{T}(\\vec{x}_{1}) \\cdot\\vec{w})\\\\ ... \\\\ \\sigma(\\vec{\\phi}^{T}(\\vec{x}_{N})\\cdot\\vec{w})\\end{pmatrix}=\\begin{pmatrix}\\sigma((\\vec{w}^{T} \\cdot\\vec{\\phi}(\\vec{x}_{1}))^T)\\\\ ... \\\\ \\sigma((\\vec{w}^{T} \\cdot\\vec{\\phi}(\\vec{x}_{N}))^T)\\end{pmatrix}=\\begin{pmatrix}y(\\vec{x}_{1},\\vec{w})\\\\ ... \\\\ y(\\vec{x}_{N},\\vec{w})\\end{pmatrix}$$\n",
    "\n",
    "Now we need a criteria to obtain the best coefficients $\\vec{w}$. As in regression, a natural choice is to maximize the likelhiood (or minmize the negative log-likelihood). For a binary variable, we have only two possible results which we can label as success or failure or heads and tails. Assuming the points are independent and identically distributed, each label has a Bernoulli distribution\n",
    "\n",
    "$$p(\\text{t}|\\vec{x},\\vec{w})=p(t|\\mu)=\\mu^{t}(1-\\mu)^{1-t}$$\n",
    "\n",
    "where $\\mu=p(\\mathcal{C}_{1}|\\vec{x})$ is the success probability, which is exactly what we're trying to model!\n",
    "\n",
    "Thus, for a dataset $\\vec{x}_{n}$, with $n=1,..,N$ the likelihood is\n",
    "\n",
    "$$\\mathcal{L}(\\vec{w})=p(\\text{T}|X,\\vec{w})=\\prod_{n=1}^{N}y^{t_n}_{n}(1-y_{n})^{1-t_n}$$\n",
    "\n",
    "where $y_{n}=y(x_{n},\\vec{w})$. Again, the logarithm is easier to handle\n",
    "\n",
    "$$\\ln \\mathcal{L}(\\vec{w})=\\sum_{n=1}^{N}(t_{n}\\ln y_{n}+(1-t_{n})\\ln (1-y_{n}))$$\n",
    "\n",
    "Again we can define an error by taking the negative log-likelihood. This is called the **binary cross-entropy** (BCE) between $t$ and $y$\n",
    "\n",
    "$$E(\\vec{w})=-\\sum_{n=1}^{N}(t_{n}\\ln y_{n}+(1-t_{n})\\ln (1-y_{n}))$$\n",
    "\n",
    "We can see how BCE forces the right behaviour into the model:\n",
    "\n",
    "If $t_{n}=1$, we care about $\\ln y_{n}$. Thus, $y_{n}$ needs to be close to 1 to minimize the BCE.\n",
    "\n",
    "If $t_{n}=0$, we care about $\\ln (1-y_{n})$. Thus, $y_{n}$ needs to be close to 0 to minimize the BCE.\n",
    "\n",
    "Just a reminder, $y_{n}$ is not meant to match $t_{n}$ but $p(t_{n}|\\vec{x}_{n},\\vec{w})$. Thus, we also seek a degree of confidence to be embedded into the model.\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To obtain useful estimates, we need to be able to minimize the BCE with respect to the parameters $\\vec{w}$.\n",
    "\n",
    "For logistic regression, the problem is not as easy as it was for linear regression due to the presence of the logarithms and the sigmoid function. However, we can still solve this numerically. A particularly good choice is the Iterative Reweighted Least Squares or IRLS, which considers a Newton-Ralphson update for step $i$:\n",
    "\n",
    "$$\\vec{w}^{i}=\\vec{w}^{i-1}-\\mathrm{H}^{-1}\\nabla E(\\vec{w})|_{\\vec{w}^{i-1}}$$\n",
    "\n",
    "where $\\vec{w}^{0}$ is the initialized vector of parameters, $\\nabla E(\\vec{w})$ is the BCE gradient with respect to the parameters and $\\mathrm{H}$ is the Hessian matrix. For a logistic regressor where we combine a linear model with a sigmoide function, this simplifies to a set of **iterative normal equations**:\n",
    "\n",
    "$$\\vec{w}^{i}=\\Phi^{T}\\mathrm{R}\\Phi^{-1}\\Phi^{T}\\mathrm{R}\\mathrm{z}$$\n",
    "\n",
    "where $\\Phi$ is the design matrix, $\\mathrm{R}$ a diagonal matrix whose elements are $y_{n}(1-y_{n})$ and $\\mathrm{z}$ is\n",
    "\n",
    "$\\mathrm{z}=\\Phi\\vec{w}^{i-1}-\\mathrm{R}^{-1}(\\mathrm{Y}-\\mathrm{T})$\n",
    "\n",
    "where $\\mathrm{Y}$ e $\\mathrm{T}$ are the prediction and label vectors. One should remember that $\\mathrm{R}$, $\\mathrm{z}$ and $\\mathrm{Y}$ are all functions of $\\vec{w}^{-1}$.\n",
    "\n",
    "\n",
    "We could have applied such an algorithm to linear regression, and observed how it converge to the closed solution in 1 step. Additionally, be sure to notice that is an iterative algorithm but it is not sequential since it considers the full dataset at each step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise"
   ],
   "metadata": {
    "id": "Exercise"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's consider the following dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "size1 = 250\n",
    "mu1 = [0, 0]\n",
    "cov1 = [[1, 0.95], [0.95, 1]]\n",
    "\n",
    "size2 = 200\n",
    "mu2 = [-1, 0.5]\n",
    "cov2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "np.random.seed(20200922)\n",
    "# Sample classes\n",
    "xc1 = np.random.multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "xc2 = np.random.multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "print(xc1.shape, xc2.shape)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*xc1, \"ob\", mfc=\"None\", label=\"C1\")\n",
    "ax.plot(*xc2, \"or\", mfc=\"None\", label=\"C2\")\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_aspect(\"equal\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = np.hstack([xc1, xc2]).T\n",
    "\n",
    "tc1 = np.ones(xc1.shape[1])\n",
    "tc2 = np.zeros(xc2.shape[1])\n",
    "\n",
    "t = np.concatenate([tc1, tc2]).reshape(-1, 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this sigmoid function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(logoddsvec):\n",
    "    return 1 / (1 + np.exp(-logoddsvec))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a Logistic Regressor, print the best parameters, compute the predicted probability per measurement and plot it as a function of ($x_1,x_2$)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos como evolucionan los coeficientes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "W = w[-1]\n",
    "y = sigmoid(np.dot(Phi, W))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.hist(\n",
    "    y[t[:, 0] > 0.5],\n",
    "    bins=np.linspace(0, 1, 10),\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    label=\"Predicted probabilities for $C_1$\",\n",
    ")\n",
    "plt.hist(\n",
    "    y[t[:, 0] <= 0.5],\n",
    "    bins=np.linspace(0, 1, 10),\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"red\",\n",
    "    label=\"Predicted probabilities for $C_2$\",\n",
    ")\n",
    "plt.axvline(\n",
    "    0.5, color=\"red\", linestyle=\"dashed\", linewidth=2, label=\"Decision boundary\"\n",
    ")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "###STOP_SOLUTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy_score(labels, predictions):\n",
    "    return np.mean([a == b for a, b in zip(labels, predictions)])\n",
    "\n",
    "\n",
    "def precision_score(labels, predictions):\n",
    "    return np.sum([a and b for a, b in zip(labels == 1, predictions == 1)]) / np.sum(\n",
    "        predictions == 1\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_score(labels, predictions):\n",
    "    return np.sum([a and b for a, b in zip(labels == 1, predictions == 1)]) / np.sum(\n",
    "        labels == 1\n",
    "    )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check them out"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(accuracy_score(t, y_pred))  # Accuracy\n",
    "print(precision_score(t, y_pred))  # Precision\n",
    "print(recall_score(t, y_pred))  # Recall"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "cf = confusion_matrix(t, y_pred)\n",
    "print(cf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Introduce *varying metric *The ROC curve"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multiclass (`sklearn`-based)"
   ],
   "metadata": {
    "id": "Multiclass_(`sklearn`-based)"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejemplifiquemos con un caso de juguete:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "lr = LogisticRegression()\n",
    "percep = Perceptron()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para el caso del Perceptron, tendremos una \u00fanica funci\u00f3n de decisi\u00f3n. El criterio usual es que si $y\\geq0$, asigno a la clase $C_{1}$. Esto se puede modificar tal como vimos antes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = percep.decision_function(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, levels=[-1000, 0.0, 1000])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(lr.predict_proba(X[:1]))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vayamos ahora al caso multiclase:\n",
    "- Tenemos las etiquetas, que pueden venir esencialmente en dos formas: {0,1,...,K-1},{[1,0,...,0],[0,1,0,...,0],...}. Para `sklearn`, con usar la primera estamos perfecto. Si codeamos a mano, hay que tener cuidado.\n",
    "- Tenemos las predicciones. Ac\u00e1 es donde aparece la gran diferencia. \u00bfQu\u00e9 entrenamos?\n",
    "- El primer enfoque es el de _uno contra el resto_. Para cada clase $K$, entreno un clasificador que aprenda a distinguir esa clase del resto. Tenemos entonces K clasificadores. Por ejemplo, para 4 clases: (0,123), (1,023), (2,013), (3,012). Uno asigna entonces una clase viendo como se intersectan las distintas regiones de los clasificadores. Eso introduce la posibilidad de regiones ambiguas. Adem\u00e1s, el problema se puede volver muy desbalanceado.\n",
    "- El segundo enfoque es el de _uno contra uno_. En este enfoque, entreno un clasificador para cada combinaci\u00f3n de dos clases. Tenemos entonces $\\frac{K(K-1)}{2}$ clasificadores, uno por cada problema de dos clases. Por ejemplo, para 4 clases: (0,1), (0,2), (0,3), (1,2), (1,3), (2,3). Para asignar una clase, tomamos entonces el voto mayoritario. Es decir, cual es la clase que recibe m\u00e1s votos de cada uno de estos clasificadores. Esto tambi\u00e9n introduce ambig\u00fcedades.\n",
    "- El tercer enfoque, que no asigna ambiguedades. Es aprender un \u00fanico clasificador de $K$ clases. Para el caso de funci\u00f3n discriminante, esto se logra aprendiendo K funciones $y_{k}$ al mismo tiempo, y asignando la clase como la que da el maximo $y_{k}$ entre todos. Las fronteras de decisi\u00f3n son entonces dadas por las regiones donde coinciden dos funciones. Se puede mostrar que con este criterio ya no hay ambig\u00fcedades. Para el caso de modelos discriminativos y generativos, esto es todavia m\u00e1s f\u00e1cil. \u00a1Aprendemos $p(C_{k}|x)$ tal como antes! Ahora simplemente no simplificamos la clase redundante. Tampoco hay ambig\u00fcedades.\n",
    "\n",
    "Cual de los tres enfoques usamos depende del algoritmo. En particular, hay algunos que no pueden utilizar el tercer enfoque y entonces aparece el problema. Noten que ahora escribi el criterio, mientras que antes siempre aclare que podiamos jugar con el umbral. Para el caso multiclase, el an\u00e1lisis que hicimos se vuelve m\u00e1s d\u00edficil. Para poder hacerlo, vamos a binarizar el problema."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "gt_center = np.array([[3.0, 3.0], [-3.0, -3.0], [-3.0, 3.0], [3.0, -3.0]])\n",
    "X, t = make_blobs(\n",
    "    1000,\n",
    "    n_features=2,\n",
    "    centers=gt_center,\n",
    "    cluster_std=1.0,\n",
    "    random_state=1234,\n",
    ")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenemos una funci\u00f3n discriminante y un modelo discriminativo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "lr.fit(X, t)\n",
    "lda.fit(X, t)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(lda.decision_function(X[:1]))"
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = np.argmax(lda.decision_function(Xvals), axis=1).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(lr.predict_proba(X[:1]))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = np.argmax(lr.predict_proba(Xvals), axis=1).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(t, lr.predict(X)))\n",
    "print(accuracy_score(t, lr.predict(X)))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "for k in range(4):\n",
    "    print(\"Clase \" + str(k) + \" contra todos\")\n",
    "    print(f1_score(np.where(t == k, 1.0, 0.0), np.where(lr.predict(X) == k, 1.0, 0.0)))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "lsvc_1 = LinearSVC(loss=\"hinge\", penalty=\"l2\", C=10.0, max_iter=1000)\n",
    "svc_1 = SVC(kernel=\"linear\", C=10.0, decision_function_shape=\"ovr\")\n",
    "svc_2 = SVC(kernel=\"linear\", C=10.0, decision_function_shape=\"ovo\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(lsvc_1.decision_function(X[:1]).shape)\n",
    "print(svc_1.decision_function(X[:1]).shape)\n",
    "print(svc_2.decision_function(X[:1]).shape)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(lsvc_1.coef_.shape)  # (nclasses, ncoefs)\n",
    "print(svc_1.coef_.shape)  # (nclasses*(nclasses-1)/2, ncoefs)\n",
    "print(svc_2.coef_.shape)  # (nclasses*(nclasses-1)/2, ncoefs)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(np.allclose(svc_1.coef_, svc_2.coef_))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = lsvc_1.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = svc_2.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.scatter(\n",
    "    svc_2.support_vectors_[:, 0],\n",
    "    svc_2.support_vectors_[:, 1],\n",
    "    marker=\"+\",\n",
    "    color=\"black\",\n",
    "    label=\"SV\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ],
   "metadata": {
    "scrolled": true
   }
  }
 ],
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "Simple_ML_introducion",
    "Introduction",
    "Classification",
    "Logistic_Regression",
    "Exercise",
    "Metrics",
    "Exercise",
    "Multiclass_(`sklearn`-based)"
   ]
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "mimetype": "text/x-python",
   "version": "3.10.12",
   "file_extension": ".py",
   "nbconvert_exporter": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 }
}