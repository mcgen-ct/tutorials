{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Introduction_to_Classification"
   },
   "source": [
    "# Introduction to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by:\n",
    "- Manuel Szewc (School of Physics, University of Cincinnati)\n",
    "- Philip Ilten (School of Physics, University of Cincinnati)\n",
    "$\\renewcommand{\\gtrsim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{>}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\lessim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{<}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\as}{\\alpha_{\\mathrm{s}}}\\renewcommand{\\aem}{\\alpha_{\\mathrm{em}}}\\renewcommand{\\kT}{k_{\\perp}}\\renewcommand{\\pT}{p_{\\perp}}\\renewcommand{\\pTs}{p^2_{\\perp}}\\renewcommand{\\pTe}{\\p_{\\perp\\mrm{evol}}}\\renewcommand{\\pTse}{\\p^2_{\\perp\\mrm{evol}}}\\renewcommand{\\pTmin}{p_{\\perp\\mathrm{min}}}\\renewcommand{\\pTsmim}{p^2_{\\perp\\mathrm{min}}}\\renewcommand{\\pTmax}{p_{\\perp\\mathrm{max}}}\\renewcommand{\\pTsmax}{p^2_{\\perp\\mathrm{max}}}\\renewcommand{\\pTL}{p_{\\perp\\mathrm{L}}}\\renewcommand{\\pTD}{p_{\\perp\\mathrm{D}}}\\renewcommand{\\pTA}{p_{\\perp\\mathrm{A}}}\\renewcommand{\\pTsL}{p^2_{\\perp\\mathrm{L}}}\\renewcommand{\\pTsD}{p^2_{\\perp\\mathrm{D}}}\\renewcommand{\\pTsA}{p^2_{\\perp\\mathrm{A}}}\\renewcommand{\\pTo}{p_{\\perp 0}}\\renewcommand{\\shat}{\\hat{s}}\\renewcommand{\\a}{{\\mathrm a}}\\renewcommand{\\b}{{\\mathrm b}}\\renewcommand{\\c}{{\\mathrm c}}\\renewcommand{\\d}{{\\mathrm d}}\\renewcommand{\\e}{{\\mathrm e}}\\renewcommand{\\f}{{\\mathrm f}}\\renewcommand{\\g}{{\\mathrm g}}\\renewcommand{\\hrm}{{\\mathrm h}}\\renewcommand{\\lrm}{{\\mathrm l}}\\renewcommand{\\n}{{\\mathrm n}}\\renewcommand{\\p}{{\\mathrm p}}\\renewcommand{\\q}{{\\mathrm q}}\\renewcommand{\\s}{{\\mathrm s}}\\renewcommand{\\t}{{\\mathrm t}}\\renewcommand{\\u}{{\\mathrm u}}\\renewcommand{\\A}{{\\mathrm A}}\\renewcommand{\\B}{{\\mathrm B}}\\renewcommand{\\D}{{\\mathrm D}}\\renewcommand{\\F}{{\\mathrm F}}\\renewcommand{\\H}{{\\mathrm H}}\\renewcommand{\\J}{{\\mathrm J}}\\renewcommand{\\K}{{\\mathrm K}}\\renewcommand{\\L}{{\\mathrm L}}\\renewcommand{\\Q}{{\\mathrm Q}}\\renewcommand{\\R}{{\\mathrm R}}\\renewcommand{\\T}{{\\mathrm T}}\\renewcommand{\\W}{{\\mathrm W}}\\renewcommand{\\Z}{{\\mathrm Z}}\\renewcommand{\\bbar}{\\overline{\\mathrm b}}\\renewcommand{\\cbar}{\\overline{\\mathrm c}}\\renewcommand{\\dbar}{\\overline{\\mathrm d}}\\renewcommand{\\fbar}{\\overline{\\mathrm f}}\\renewcommand{\\pbar}{\\overline{\\mathrm p}}\\renewcommand{\\qbar}{\\overline{\\mathrm q}}\\renewcommand{\\rbar}{\\overline{\\mathrm{r}}}\\renewcommand{\\sbar}{\\overline{\\mathrm s}}\\renewcommand{\\tbar}{\\overline{\\mathrm t}}\\renewcommand{\\ubar}{\\overline{\\mathrm u}}\\renewcommand{\\Bbar}{\\overline{\\mathrm B}}\\renewcommand{\\Fbar}{\\overline{\\mathrm F}}\\renewcommand{\\Qbar}{\\overline{\\mathrm Q}}\\renewcommand{\\tms}{{t_{\\mathrm{\\tiny MS}}}}\\renewcommand{\\Oas}[1]{{\\mathcal{O}\\left(\\as^{#1}\\right)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook wants to implement simple Machine Learning algorithms for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# To generate data and handle arrays\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc(\"axes\", labelsize=14)\n",
    "mpl.rc(\"xtick\", labelsize=12)\n",
    "mpl.rc(\"ytick\", labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Theory"
   },
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problems, we are interested in predicting a class asignment $\\mathcal{C}_{k}$ to a measurement $x$. The training data consists of paired measurements and class labels $x_{\\text{train}},t_{\\text{train}}$.\n",
    "\n",
    "There are three types of classification approaches:\n",
    "\n",
    "* Discriminant functions, where we learn a function $y(x,w)$ that defines the border between classes. For the binary case, and using $t \\in \\{-1,1\\}$ , this corresponds to definen a threshold $y_{0}$ (usually zero) such that if $y(x,w)>y_{0}$, $t = 1$ and $t=-1$ otherwise. This is the case for `Perceptron` and `Support Vector Machines` among others. `DecisionTrees` can be thought of in these terms as well, although they can also estimate probabilities.\n",
    "* Discriminative models, where we learn a function $y(x,w)$ that encodes the probability of a class asignment given $x$, $p(\\mathcal{C}_{k}|x)$. A class asignment can be made by selecting the class which maximizes the probability, although other criteria can be applied as well. `LogisticRegressors` and simple `Neural Network Classifiers` are examples of this.\n",
    "* Generative models, where we learn a function $y(x,w)$ that encodes the probability of $x$ per class, $p(x|\\mathcal{C}_{k})$. A class asignment can be made by estimating the per class probabilities using Bayes' rule and selecting the class which maximizes this probability. `Naive Bayes` classifiers or conditional density estimators are examples of this.\n",
    "\n",
    "\n",
    "Let's focus more on discriminative models. Here, we are interested in the per-class probability, which is a **posterior** over class asignments. For the binary case, we only need to specifiy $p(\\mathcal{C}_{1}|x)$ since $p(\\mathcal{C}_{2}|x)=1-p(\\mathcal{C}_{1}|x)$. We deal with the multiclass problem further down the notebook.\n",
    "\n",
    "For the binary case, we compute\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=\\frac{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})+p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}=\\frac{1}{1+\\frac{p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}}$$\n",
    "\n",
    "Defining the **log-odds ratio**\n",
    "\n",
    "$$a=\\text{Ln }\\frac{p(x|\\mathcal{C}_{1})p(\\mathcal{C}_{1})}{p(x|\\mathcal{C}_{2})p(\\mathcal{C}_{2})}=\\text{Ln }\\frac{p(\\mathcal{C}_{1}|x)}{p(\\mathcal{C}_{2}|x)}$$\n",
    "\n",
    "we have that\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=\\frac{1}{1+\\text{e}^{-a}}=\\sigma(a)$$\n",
    "\n",
    "where $\\sigma(a)$ is the  **sigmoid function**. Thus, the binary problem is reduced to computing the log-odds ratio between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.linspace(-10, 10, 200)\n",
    "\n",
    "a_vals = 1 / (1 + np.exp(-prob))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(prob, a_vals, color=\"black\")\n",
    "ax.scatter(0.0, 0.5, color=\"red\")\n",
    "ax.axvline(0.0, linestyle=\"dashed\", color=\"red\")\n",
    "ax.axhline(0.5, linestyle=\"dashed\", color=\"red\")\n",
    "ax.set_ylabel(r\"$p(\\mathcal{C}_{1}|x)$\", fontsize=16)\n",
    "ax.set_xlabel(\"Log odds\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how the sigmoid function is a function lying in $[0,1]$, even as the log odds ratio takes values in $(-\\infty,\\infty)$. This is necessary since we're interpreting it as a probability.\n",
    "\n",
    "A particularly important point is where the two classes are equally likely. There, the log odds ratio is 1 and the per class probability is 0.5, as it should. In general, this is selected as the decision boundary.\n",
    "\n",
    "Two useful properties of the sigmoid function are:\n",
    "\n",
    "$$\\sigma(-a)=1-\\sigma(a)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{d\\sigma}{da}=\\sigma(1-\\sigma)$$\n",
    "\n",
    "For the $K$ class case, we can generalize the sigmoid to the normalized exponential or **softmax** function:\n",
    "\n",
    "$$p(\\mathcal{C}_{k}|x)=\\frac{p(x|\\mathcal{C}_{k})p(\\mathcal{C}_{k})}{\\sum_{l=1}^{K}p(x|\\mathcal{C}_{l})p(\\mathcal{C}_{l})}=\\frac{e^{a_{k}}}{\\sum_{l=1}^{K}e^{a_{l}}}$$\n",
    "\n",
    "where $a_{k}=\\text{Ln }p(x|\\mathcal{C}_{k})p(\\mathcal{C}_{k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Logistic_Regression"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest discriminative classifier is the `Logistic Regressor`. For the two class case, we model the per class probability posterior as\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|\\vec{x})=y(\\vec{x},\\vec{w})=\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$$\n",
    "\n",
    "$$p(\\mathcal{C}_{0}|\\vec{x})=1-y(\\vec{x},\\vec{w})=1-\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))=\\sigma(-\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$$\n",
    "\n",
    "We already have two features of note:\n",
    "\n",
    "- $\\vec{w}^{T}\\vec{\\phi}(\\vec{x})$ acts as the decision function in a discriminant approach. In particular, $\\vec{w}^{T}\\vec{\\phi}(\\vec{x})=0$ defines the equi-probability surface.\n",
    "- The sigmoid is a **non-linear activation function**. This feature, necessary to transform a linear model into a probability estimate, will be very present in `Neural Networks` even for regression for different reasons (the universal approximation is ensured by internal non-linear activation functions between neuron layers).\n",
    "\n",
    "As in the generalized linear models for regression,  $\\vec{\\phi}$ includes in principle the bias with $\\phi_{0}(\\vec{x})=1$.\n",
    "\n",
    "As with regression, we can group all training pairs into a design matrix\n",
    "\n",
    "$$\\Phi=\\begin{pmatrix}\\vec{\\phi}^{T}(\\vec{x}_{1}) \\\\ ... \\\\ \\vec{\\phi}^{T}(\\vec{x}_{N})\\end{pmatrix}$$\n",
    "\n",
    "obtaining\n",
    "\n",
    "$$\\sigma(\\Phi\\cdot \\vec{w}) = \\begin{pmatrix}\\sigma(\\vec{\\phi}^{T}(\\vec{x}_{1}) \\cdot\\vec{w})\\\\ ... \\\\ \\sigma(\\vec{\\phi}^{T}(\\vec{x}_{N})\\cdot\\vec{w})\\end{pmatrix}=\\begin{pmatrix}\\sigma((\\vec{w}^{T} \\cdot\\vec{\\phi}(\\vec{x}_{1}))^T)\\\\ ... \\\\ \\sigma((\\vec{w}^{T} \\cdot\\vec{\\phi}(\\vec{x}_{N}))^T)\\end{pmatrix}=\\begin{pmatrix}y(\\vec{x}_{1},\\vec{w})\\\\ ... \\\\ y(\\vec{x}_{N},\\vec{w})\\end{pmatrix}$$\n",
    "\n",
    "Now we need a criteria to obtain the best coefficients $\\vec{w}$. As in regression, a natural choice is to maximize the likelhiood (or minmize the negative log-likelihood). For a binary variable, we have only two possible results which we can label as success or failure or heads and tails. Assuming the points are independent and identically distributed, each label has a Bernoulli distribution\n",
    "\n",
    "$$p(\\text{t}|\\vec{x},\\vec{w})=p(t|\\mu)=\\mu^{t}(1-\\mu)^{1-t}$$\n",
    "\n",
    "where $\\mu=p(\\mathcal{C}_{1}|\\vec{x})$ is the success probability, which is exactly what we're trying to model!\n",
    "\n",
    "Thus, for a dataset $\\vec{x}_{n}$, with $n=1,..,N$ the likelihood is\n",
    "\n",
    "$$\\mathcal{L}(\\vec{w})=p(\\text{T}|X,\\vec{w})=\\prod_{n=1}^{N}y^{t_n}_{n}(1-y_{n})^{1-t_n}$$\n",
    "\n",
    "where $y_{n}=y(x_{n},\\vec{w})$. Again, the logarithm is easier to handle\n",
    "\n",
    "$$\\ln \\mathcal{L}(\\vec{w})=\\sum_{n=1}^{N}(t_{n}\\ln y_{n}+(1-t_{n})\\ln (1-y_{n}))$$\n",
    "\n",
    "Again we can define an error by taking the negative log-likelihood. This is called the **binary cross-entropy** (BCE) between $t$ and $y$\n",
    "\n",
    "$$E(\\vec{w})=-\\sum_{n=1}^{N}(t_{n}\\ln y_{n}+(1-t_{n})\\ln (1-y_{n}))$$\n",
    "\n",
    "We can see how BCE forces the right behaviour into the model:\n",
    "\n",
    "If $t_{n}=1$, we care about $\\ln y_{n}$. Thus, $y_{n}$ needs to be close to 1 to minimize the BCE.\n",
    "\n",
    "If $t_{n}=0$, we care about $\\ln (1-y_{n})$. Thus, $y_{n}$ needs to be close to 0 to minimize the BCE.\n",
    "\n",
    "Just a reminder, $y_{n}$ is not meant to match $t_{n}$ but $p(t_{n}|\\vec{x}_{n},\\vec{w})$. Thus, we also seek a degree of confidence to be embedded into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain useful estimates, we need to be able to minimize the BCE with respect to the parameters $\\vec{w}$.\n",
    "\n",
    "For logistic regression, the problem is not as easy as it was for linear regression due to the presence of the logarithms and the sigmoid function. However, we can still solve this numerically. A particularly good choice is the Iterative Reweighted Least Squares or IRLS, which considers a Newton-Ralphson update for step $i$:\n",
    "\n",
    "$$\\vec{w}^{i}=\\vec{w}^{i-1}-\\mathrm{H}^{-1}\\nabla E(\\vec{w})|_{\\vec{w}^{i-1}}$$\n",
    "\n",
    "where $\\vec{w}^{0}$ is the initialized vector of parameters, $\\nabla E(\\vec{w})$ is the BCE gradient with respect to the parameters and $\\mathrm{H}$ is the Hessian matrix. For a logistic regressor where we combine a linear model with a sigmoide function, this simplifies to a set of **iterative normal equations**:\n",
    "\n",
    "$$\\vec{w}^{i}=\\Phi^{T}\\mathrm{R}\\Phi^{-1}\\Phi^{T}\\mathrm{R}\\mathrm{z}$$\n",
    "\n",
    "where $\\Phi$ is the design matrix, $\\mathrm{R}$ a diagonal matrix whose elements are $y_{n}(1-y_{n})$ and $\\mathrm{z}$ is\n",
    "\n",
    "$\\mathrm{z}=\\Phi\\vec{w}^{i-1}-\\mathrm{R}^{-1}(\\mathrm{Y}-\\mathrm{T})$\n",
    "\n",
    "where $\\mathrm{Y}$ e $\\mathrm{T}$ are the prediction and label vectors. One should remember that $\\mathrm{R}$, $\\mathrm{z}$ and $\\mathrm{Y}$ are all functions of $\\vec{w}^{-1}$.\n",
    "\n",
    "\n",
    "We could have applied such an algorithm to linear regression, and observed how it converge to the closed solution in 1 step. Additionally, be sure to notice that is an iterative algorithm but it is not sequential since it considers the full dataset at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = 250\n",
    "mu1 = [0, 0]\n",
    "cov1 = [[1, 0.95], [0.95, 1]]\n",
    "\n",
    "size2 = 200\n",
    "mu2 = [-1, 0.5]\n",
    "cov2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "np.random.seed(20200922)\n",
    "# Sample classes\n",
    "xc1 = np.random.multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "xc2 = np.random.multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "print(xc1.shape, xc2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*xc1, \"ob\", mfc=\"None\", label=\"C1\")\n",
    "ax.plot(*xc2, \"or\", mfc=\"None\", label=\"C2\")\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([xc1, xc2]).T\n",
    "\n",
    "tc1 = np.ones(xc1.shape[1])\n",
    "tc2 = np.zeros(xc2.shape[1])\n",
    "\n",
    "t = np.concatenate([tc1, tc2]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(logoddsvec):\n",
    "    return 1 / (1 + np.exp(-logoddsvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Logistic Regressor, print the best parameters, compute the predicted probability per measurement and plot it as a function of ($x_1,x_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y here. Below is the correct size, but not values.\n",
    "\n",
    "y = np.ones(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Metrics"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model, we need a criteria for class asignment. For the binary case, a natural choice is to assign $x\\in\\mathcal{C}_1$ if\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)\\geq p(\\mathcal{C}_{2}|x)$$\n",
    "\n",
    "Since the two probabilities add up to one, this criteria sets the **decision boundary** at\n",
    "\n",
    "$$p(\\mathcal{C}_{1}|x)=0.5$$\n",
    "\n",
    "This choice can be shown to maximize the **accuracy** given by\n",
    "\n",
    "$$\\mathrm{Accuracy = }\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{FP}+\\mathrm{TN}+\\mathrm{FN}}$$\n",
    "\n",
    "where TP, TN, FP and FN stand for True Positive, True Negative, False Positive and False Negative *ie* the fractions of correctly and incorrectly assigned measurements per class (positive is $C_{1}$, negative is $C_{2}$).\n",
    "\n",
    "That is, we're minimizing the **total misasignments**.\n",
    "\n",
    "Two other very useful metris are **precision** and **recall**:\n",
    "\n",
    "$$\\mathrm{Precision = }\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$$\n",
    "\n",
    "$$\\mathrm{Recall = }\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$$\n",
    "\n",
    "Precision captures how many positive samples are actually positive, and recall relates how many positive samples are we tagging as positives.\n",
    "\n",
    "All of these can be derived from the **confusion matrix**, which cointains at row, column $(i,j)$ the number of elements belonging to class $i$ that are labelled as belonging class $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(labels, predictions):\n",
    "    return np.mean([a == b for a, b in zip(labels, predictions)])\n",
    "\n",
    "\n",
    "def precision_score(labels, predictions):\n",
    "    return np.sum([a and b for a, b in zip(labels == 1, predictions == 1)]) / np.sum(\n",
    "        predictions == 1\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_score(labels, predictions):\n",
    "    return np.sum([a and b for a, b in zip(labels == 1, predictions == 1)]) / np.sum(\n",
    "        labels == 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, predictions):\n",
    "    #  pred0 pred1\n",
    "    # verdad0  VN  FP\n",
    "    # verdad1  FN  VP\n",
    "    TP = np.sum(labels[np.where(predictions[:, 0] == 1), 0])\n",
    "    FP = np.sum(1 - labels[np.where(predictions[:, 0] == 1), 0])\n",
    "    TN = np.sum(1 - labels[np.where(predictions[:, 0] == 0), 0])\n",
    "    FN = np.sum(labels[np.where(predictions[:, 0] == 0), 0])\n",
    "    return np.array([[TN, FP], [FN, TP]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify as 1 P(c1 | x) >= 0.5\n",
    "y_pred = np.where(y >= 0.5, 1, 0)\n",
    "\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(t, y_pred))  # Accuracy\n",
    "print(precision_score(t, y_pred))  # Precision\n",
    "print(recall_score(t, y_pred))  # Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum(t[np.where(y_pred[:, 0] == 1), 0])\n",
    "FP = np.sum(1 - t[np.where(y_pred[:, 0] == 1), 0])\n",
    "TN = np.sum(1 - t[np.where(y_pred[:, 0] == 0), 0])\n",
    "FN = np.sum(t[np.where(y_pred[:, 0] == 0), 0])\n",
    "\n",
    "print(TP, FP, TN, FN)\n",
    "print((TP + TN) / (TP + FP + TN + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(t, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = cf.ravel()\n",
    "print(TP, FP, TN, FN)\n",
    "print((TP + TN) / (TP + FP + TN + FN))  # Accuracy from confusion matrix\n",
    "print((TP) / (TP + FP))  # Precision from confusion matrix\n",
    "print((TP) / (TP + FN))  # Recall from confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these metrics have been computed after assigning a class label using a particular cut in the predicted class probability $p(C_{1}|\\vec{x})>0.5$.\n",
    "\n",
    "However, this choice is in some sense arbitrary (although it is the one that minimizes the number of misclassifications). One could be interested in taking different choices, which modifies the different matrixes. For example, a spam filter may prioritize not missing important emails (lowering the false negatives) at the expense of allowing more spam emails to enter (increasing the false positives).\n",
    "\n",
    "\n",
    "In all cases, there is a relevant **precision/recall trade-off** as we explore the choice of threshold $t$ for class asignment ($y=1$ if $p(C_{1}|\\vec{x})>t$).\n",
    "\n",
    "Intuitevely, low values of $t$ will include more points into the positive sample, increasing both the TP and FP classes. Thus, the recall will increase. However, the precision will decrease. If we increase the threshold, the selection is more stringent and thus the precision will increase. However, the recall will decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "precision_values = np.zeros(len(thresholds))\n",
    "recall_values = np.zeros(len(thresholds))\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    y_pred = np.where(y[:, 0] >= threshold, 1, 0)\n",
    "    precision_values[i] = precision_score(t[:, 0], y_pred)\n",
    "    recall_values[i] = recall_score(t[:, 0], y_pred)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision_values, label=\"Precision\")\n",
    "plt.plot(thresholds, recall_values, label=\"Recall\")\n",
    "plt.axvline(0.5, linestyle=\"dashed\", color=\"red\", label=\"Decision Boundary\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision and Recall vs Threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall_values, precision_values)\n",
    "\n",
    "y_pred = np.where(y[:, 0] >= 0.5, 1, 0)\n",
    "precision_standard = precision_score(t[:, 0], y_pred)\n",
    "recall_standard = recall_score(t[:, 0], y_pred)\n",
    "plt.scatter(\n",
    "    recall_standard, precision_standard, color=\"red\", label=\"Standard Decision Boundary\"\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall curve allows for a more holistic comparison of classifiers: the more the curve approaches the right-up quadrant, the better the classifier.\n",
    "\n",
    "However, the most common choice of classifier quantification through curves is not the precision-recall. The most preferred option is the **Receiver Operating Curve or ROC Curve** for short, which plots the True Positive Rate (TPR = TP / (TP + FN) = Recall) as a function of the False Positive Rate (FPR = FP / (FP + TN)) with the threshold implicitly parameterizing the curve.\n",
    "\n",
    "This choice is made because it cristallizes very clearly the trade-off between correct classification of one class at the expense of incorrect classification of the other. Intuitively, the more the curve approaches the upper-left quadrant, the better the classifier. A quantitative metric derived from the ROC curve is the **Area-Under-Curve (AUC) score**, which is the integral of the ROC Curve. The closer the AUC is to 1, the better the classifier. A random classifier will have a score of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "tpr_values = np.zeros(len(thresholds))\n",
    "fpr_values = np.zeros(len(thresholds))\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    y_pred = np.where(y[:, 0] >= threshold, 1, 0)\n",
    "    TP = np.sum(t[np.where(y_pred == 1), 0])\n",
    "    FP = np.sum(1 - t[np.where(y_pred == 1), 0])\n",
    "    TN = np.sum(1 - t[np.where(y_pred == 0), 0])\n",
    "    FN = np.sum(t[np.where(y_pred == 0), 0])\n",
    "\n",
    "    tpr_values[i] = TP / (TP + FN)  # True Positive Rate (Recall)\n",
    "    fpr_values[i] = FP / (FP + TN)  # False Positive Rate (1 - Specificity)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, tpr_values, label=\"True Positive Rate\")\n",
    "plt.plot(thresholds, fpr_values, label=\"False Positive Rate\")\n",
    "plt.axvline(0.5, linestyle=\"dashed\", color=\"red\", label=\"Decision Boundary\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"True Positive Rate and False Positive Rate vs Threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### the minus sign is because the trapezoidal rule integrates from left to right, so we need to flip the sign for the ROC curve\n",
    "AUC_score = -np.trapz(tpr_values, fpr_values)\n",
    "# print(f\"AUC Score: {AUC_score}\")\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_values, tpr_values)\n",
    "plt.plot(\n",
    "    fpr_values, fpr_values, color=\"black\", linestyle=\"dashed\", label=\"Random Classifier\"\n",
    ")\n",
    "\n",
    "y_pred = np.where(y[:, 0] >= 0.5, 1, 0)\n",
    "TP = np.sum(t[np.where(y_pred == 1), 0])\n",
    "FP = np.sum(1 - t[np.where(y_pred == 1), 0])\n",
    "TN = np.sum(1 - t[np.where(y_pred == 0), 0])\n",
    "FN = np.sum(t[np.where(y_pred == 0), 0])\n",
    "\n",
    "tpr_standard = TP / (TP + FN)  # True Positive Rate (Recall)\n",
    "fpr_standard = FP / (FP + TN)  # False Positive Rate (1 - Specificity)\n",
    "plt.scatter(fpr_standard, tpr_standard, color=\"red\", label=\"Standard Decision Boundary\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve, AUC = {:.2f}\".format(AUC_score))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `scikit-learn` for a more realistic problem, detailed below. Explore metrics and the ROC curve.\n",
    "\n",
    "To do this, make sure to cross-validate using the `cross_val_predict` and `cross_val_score` methods explained in the `regression` notebook.\n",
    "\n",
    "The new relevant classes are\n",
    "\n",
    "`sklearn.linear_model.LogisticRegression`\n",
    "\n",
    "`sklearn.metrics.accuracy_score, sklearn.metrics.precision_score,sklearn.metrics.recall_score, sklearn.metrics.confusion_matrix,sklearn.metrics.precision_recall_curve, sklearn.metrics.roc_curve, sklearn.metrics.roc_auc_score`\n",
    "\n",
    "You can also explore alternative classifiers\n",
    "\n",
    "`sklearn.linear_model.Perceptron`\n",
    "\n",
    "`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`\n",
    "\n",
    "`sklearn.naive_bayes.GaussianNB`\n",
    "\n",
    "`sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "`sklearn.ensemble.GradientBoostingClassifier`\n",
    "\n",
    "`sklearn.ensemble.RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me introduce the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"Spectral\")\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take as a simple problem a validation of NBA analytic scores. To do this, we can scrap data from BasketballReference using some nice Python utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the 2024 season as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.basketball-reference.com/leagues/NBA_{}_advanced.html\".format(\n",
    "    2024\n",
    ")  # this is the HTML from the given URL\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll(\"tr\", limit=2)  # use getText()to extract the text we need into a list\n",
    "headers = [\n",
    "    th.getText() for th in soup.findAll(\"tr\", limit=2)[0].findAll(\"th\")\n",
    "]  # exclude the first column as we will not need the ranking order from Basketball Reference for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data in a `Pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = headers[1:]\n",
    "rows = soup.findAll(\"tr\")[1:]\n",
    "player_stats = [\n",
    "    [td.getText() for td in rows[i].findAll(\"td\")] for i in range(len(rows))\n",
    "]\n",
    "stats = pd.DataFrame(player_stats, columns=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For intermediate datasets, pandas is a useful package that allow us to explore the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the playoffs\n",
    "stats = stats[: np.where(stats[\"Team\"] == \"\")[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a table with players who took part in the 2016 NBA Season.NBA.\n",
    "\n",
    "Working with real data is complex, and one usually has to do some cleaning. For example, there are players who were traded and played in multiple teams in a single season. This is evidenced by the existence of the `Tot` team assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = stats[\n",
    "    \"Team\"\n",
    "].unique()  # we can see that the team names are not consistent, so we will need to clean them up\n",
    "\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninteresting_teams = [\n",
    "    \"2TM\",\n",
    "    \"3TM\",\n",
    "]  # we will eventually remove these teams from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in uninteresting_teams:\n",
    "    print(\"Team name and number of players:\")\n",
    "    print(team, np.sum(stats[\"Team\"] == team))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything after OWS are advanced stats that talk about \"player quality\" and are derived from the previous entries (plus maybe some propietary stats.)\n",
    "\n",
    "Let me define a perhaps interesting problem:\n",
    "\n",
    "**Can I predict whether a player made the postseason based on his stats?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2016 NBA postseason teams are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_playoff_west = np.asarray([\"GSW\", \"SAC\", \"SAS\", \"UTA\", \"POR\", \"MEM\", \"HOU\"])\n",
    "print(len(no_playoff_west))\n",
    "playoff_west = np.asarray([\"DEN\", \"LAL\", \"OKC\", \"MIN\", \"PHO\", \"LAC\", \"DAL\", \"NOP\"])\n",
    "print(len(playoff_west))\n",
    "\n",
    "no_playoff_east = np.asarray([\"CHI\", \"CHO\", \"ATL\", \"TOR\", \"WAS\", \"BRK\", \"DET\"])\n",
    "print(len(no_playoff_east))\n",
    "playoff_east = np.asarray([\"CLE\", \"MIA\", \"BOS\", \"IND\", \"ORL\", \"MIL\", \"NYK\", \"PHI\"])\n",
    "print(len(playoff_east))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Preprocessing"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just split our dataframe in train / test (and cross-validate) and play with classifiers. However, we need to preprocess the data a bit before things make sense.\n",
    "\n",
    "In particular, I only care about players that played meaningful minutes. Otherwise, the data is too noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"Pos\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a copy of my dataframe and modify that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_now = stats.copy()\n",
    "stats_now = stats_now[stats_now[\"Pos\"].isin([\"SG\", \"PF\", \"PG\", \"C\", \"SF\"])]\n",
    "stats_now[\"G\"] = pd.to_numeric(stats_now[\"G\"])\n",
    "stats_now[\"MP\"] = pd.to_numeric(stats_now[\"MP\"])\n",
    "stats_now[\"USG%\"] = pd.to_numeric(stats_now[\"USG%\"])\n",
    "stats_now = stats_now[stats_now[\"USG%\"] > stats_now[\"USG%\"].mean()]\n",
    "stats_now[\"MPperG\"] = stats_now[\"MP\"] / stats_now[\"G\"]\n",
    "stats_now = stats_now[stats_now[\"G\"] >= 20]\n",
    "stats_now = stats_now[stats_now[\"MPperG\"] >= 25]\n",
    "stats_now = stats_now[~stats_now[\"Team\"].isin(uninteresting_teams)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me change the positions to a **numerical embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_now = stats_now.replace([\"PG\", \"SG\", \"SF\", \"PF\", \"C\"], [1, 2, 3, 4, 5])\n",
    "stats_now[\"Pos\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's label the data with the following code.\n",
    "\n",
    "0: no playoff west 1: playoff west 2: no playoff east 3: playoff east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_now[\"label\"] = stats_now[\"Team\"]\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(no_playoff_west, 0)\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(playoff_west, 1)\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(no_playoff_east, 2)\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(playoff_east, 3)\n",
    "print(stats_now[\"label\"].value_counts())\n",
    "plt.hist(stats_now[\"Team\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select some features.\n",
    "\n",
    "To visualize things, let's keep two for now: PER y USG%. We can store the name, the position and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_now = stats_now[[\"Player\", \"Pos\", \"PER\", \"USG%\", \"label\"]].copy()\n",
    "stats_now[\"PER\"] = pd.to_numeric(stats_now[\"PER\"])\n",
    "stats_now[\"USG%\"] = pd.to_numeric(stats_now[\"USG%\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_now.iloc[np.where(stats_now[\"Player\"] == \"LeBron James\")])\n",
    "print(stats_now.iloc[np.where(stats_now[\"Player\"] == \"Nikola Jokic\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we still have repeated players?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_now[\"Player\"].value_counts())\n",
    "plt.hist(stats_now[\"Player\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, but very few. Let's ignore that now.\n",
    "\n",
    "Now, we can divide on train and test. Since every position is different (specially back in 2016, the distinction is a bit harder now), we'll **stratify** the splitting so that position proportions are respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=445543)\n",
    "for train_index, test_index in split.split(stats_now, stats_now[\"Pos\"]):\n",
    "    strat_train_set = stats_now.iloc[train_index]\n",
    "    strat_test_set = stats_now.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_train = np.array(\n",
    "    strat_train_set.drop([\"Player\", \"Pos\", \"label\"], axis=1)\n",
    ")  # drop labels for training set\n",
    "stats_train_labels = np.array(strat_train_set[\"label\"].copy())\n",
    "stats_test = np.array(\n",
    "    strat_test_set.drop([\"Player\", \"Pos\", \"label\"], axis=1)\n",
    ")  # drop labels for training set\n",
    "stats_test_labels = np.array(strat_test_set[\"label\"].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_train.shape, stats_train_labels.shape, stats_test.shape, stats_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = dict()\n",
    "feat_dict[\"PER\"] = 0\n",
    "feat_dict[\"USG%\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stats_train[:, feat_dict[\"PER\"]], bins=10)\n",
    "plt.xlabel(r\"PER\")\n",
    "plt.show()\n",
    "plt.hist(stats_train[:, feat_dict[\"USG%\"]], bins=10)\n",
    "plt.xlabel(r\"USG%\")\n",
    "plt.show()\n",
    "plt.scatter(\n",
    "    stats_train[stats_train_labels == 0][:, feat_dict[\"PER\"]],\n",
    "    stats_train[stats_train_labels == 0][:, feat_dict[\"USG%\"]],\n",
    "    c=\"red\",\n",
    "    label=\"No Playoff West\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 0][\"Player\"]):\n",
    "    xlabel = stats_train[stats_train_labels == 0][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train[stats_train_labels == 0][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train[stats_train_labels == 1][:, feat_dict[\"PER\"]],\n",
    "    stats_train[stats_train_labels == 1][:, feat_dict[\"USG%\"]],\n",
    "    c=\"blue\",\n",
    "    label=\"Playoff West\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 1][\"Player\"]):\n",
    "    xlabel = stats_train[stats_train_labels == 1][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train[stats_train_labels == 1][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.legend(loc=\"upper left\", framealpha=0.1)\n",
    "# plt.xlim(9.0,31.0)\n",
    "# plt.ylim(15.0,35.0)\n",
    "plt.xlabel(r\"PER\")\n",
    "plt.ylabel(\"USG%\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train[stats_train_labels == 2][:, feat_dict[\"PER\"]],\n",
    "    stats_train[stats_train_labels == 2][:, feat_dict[\"USG%\"]],\n",
    "    c=\"orange\",\n",
    "    label=\"No Playoff East\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 2][\"Player\"]):\n",
    "    xlabel = stats_train[stats_train_labels == 2][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train[stats_train_labels == 2][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train[stats_train_labels == 3][:, feat_dict[\"PER\"]],\n",
    "    stats_train[stats_train_labels == 3][:, feat_dict[\"USG%\"]],\n",
    "    c=\"green\",\n",
    "    label=\"Playoff East\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 3][\"Player\"]):\n",
    "    xlabel = stats_train[stats_train_labels == 3][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train[stats_train_labels == 3][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.legend(loc=\"upper left\", framealpha=0.1)\n",
    "plt.xlabel(r\"PER\")\n",
    "plt.ylabel(\"USG%\")\n",
    "# plt.xlim(9.0,31.0)\n",
    "# plt.ylim(15.0,35.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final help, it's usually pretty useful to **scale** the data so that we do not learn differences in arbitrary units. We can do that for continous variables using the `MinMaxScaler` or the `StandardScaler`. The first one simply scales the feature so that it lies in the [0,1] interval, while the latter gaussianizes the data by substracting the mean and scaling by its standard deviation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(stats_train)\n",
    "stats_train_scaled = scaler.transform(stats_train)\n",
    "stats_test_scaled = scaler.transform(stats_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stats_train_scaled[:, feat_dict[\"PER\"]], bins=10)\n",
    "plt.xlabel(r\"PER\")\n",
    "plt.show()\n",
    "plt.hist(stats_train_scaled[:, feat_dict[\"USG%\"]], bins=10)\n",
    "plt.xlabel(r\"USG%\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train_scaled[stats_train_labels == 0][:, feat_dict[\"PER\"]],\n",
    "    stats_train_scaled[stats_train_labels == 0][:, feat_dict[\"USG%\"]],\n",
    "    c=\"red\",\n",
    "    label=\"No Playoff West\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 0][\"Player\"]):\n",
    "    xlabel = stats_train_scaled[stats_train_labels == 0][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train_scaled[stats_train_labels == 0][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train_scaled[stats_train_labels == 1][:, feat_dict[\"PER\"]],\n",
    "    stats_train_scaled[stats_train_labels == 1][:, feat_dict[\"USG%\"]],\n",
    "    c=\"blue\",\n",
    "    label=\"Playoff West\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 1][\"Player\"]):\n",
    "    xlabel = stats_train_scaled[stats_train_labels == 1][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train_scaled[stats_train_labels == 1][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.legend(loc=\"upper left\", framealpha=0.1)\n",
    "# plt.xlim(-3,3.0)\n",
    "# plt.ylim(-3.0,3.0)\n",
    "plt.xlabel(r\"PER\")\n",
    "plt.ylabel(\"USG%\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train_scaled[stats_train_labels == 2][:, feat_dict[\"PER\"]],\n",
    "    stats_train_scaled[stats_train_labels == 2][:, feat_dict[\"USG%\"]],\n",
    "    c=\"orange\",\n",
    "    label=\"No Playoff East\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 2][\"Player\"]):\n",
    "    xlabel = stats_train_scaled[stats_train_labels == 2][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train_scaled[stats_train_labels == 2][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.scatter(\n",
    "    stats_train_scaled[stats_train_labels == 3][:, feat_dict[\"PER\"]],\n",
    "    stats_train_scaled[stats_train_labels == 3][:, feat_dict[\"USG%\"]],\n",
    "    c=\"green\",\n",
    "    label=\"Playoff East\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "for i, txt in enumerate(strat_train_set[stats_train_labels == 3][\"Player\"]):\n",
    "    xlabel = stats_train_scaled[stats_train_labels == 3][:, feat_dict[\"PER\"]][i]\n",
    "    ylabel = stats_train_scaled[stats_train_labels == 3][:, feat_dict[\"USG%\"]][i]\n",
    "    plt.annotate(txt.split(\" \")[1], (xlabel, ylabel))\n",
    "\n",
    "plt.legend(loc=\"upper left\", framealpha=0.1)\n",
    "plt.xlabel(r\"PER\")\n",
    "plt.ylabel(\"USG%\")\n",
    "# plt.xlim(-3.0,3.0)\n",
    "# plt.ylim(-3.0,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally we can define the exercise:\n",
    "\n",
    "For the west and east playoffs separately, train multiple classifiers. Plot the 2d probability space, explore the metrics and decide which one is the best classifier using cross-validation. Then evaluate the final model on test and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat this using more inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_now = stats.copy()\n",
    "stats_now = stats_now[stats_now[\"Pos\"].isin([\"SG\", \"PF\", \"PG\", \"C\", \"SF\"])]\n",
    "stats_now[\"G\"] = pd.to_numeric(stats_now[\"G\"])\n",
    "stats_now[\"MP\"] = pd.to_numeric(stats_now[\"MP\"])\n",
    "stats_now[\"USG%\"] = pd.to_numeric(stats_now[\"USG%\"])\n",
    "stats_now = stats_now[stats_now[\"USG%\"] > stats_now[\"USG%\"].mean()]\n",
    "stats_now[\"MPperG\"] = stats_now[\"MP\"] / stats_now[\"G\"]\n",
    "stats_now = stats_now[stats_now[\"G\"] >= 20]\n",
    "stats_now = stats_now[stats_now[\"MPperG\"] >= 25]\n",
    "stats_now = stats_now[~stats_now[\"Team\"].isin(uninteresting_teams)]\n",
    "\n",
    "stats_now = stats_now.replace([\"PG\", \"SG\", \"SF\", \"PF\", \"C\"], [1, 2, 3, 4, 5])\n",
    "stats_now[\"Pos\"].value_counts()\n",
    "\n",
    "stats_now[\"label\"] = stats_now[\"Team\"]\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(no_playoff_west, 0)\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(playoff_west, 1)\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(no_playoff_east, 2)\n",
    "stats_now[\"label\"] = stats_now[\"label\"].replace(playoff_east, 3)\n",
    "print(stats_now[\"label\"].value_counts())\n",
    "plt.hist(stats_now[\"Team\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_now = stats_now[\n",
    "    [\n",
    "        \"Player\",\n",
    "        \"Pos\",\n",
    "        \"PER\",\n",
    "        \"USG%\",\n",
    "        \"TS%\",\n",
    "        \"3PAr\",\n",
    "        \"FTr\",\n",
    "        \"ORB%\",\n",
    "        \"DRB%\",\n",
    "        \"TRB%\",\n",
    "        \"AST%\",\n",
    "        \"STL%\",\n",
    "        \"BLK%\",\n",
    "        \"TOV%\",\n",
    "        \"label\",\n",
    "    ]\n",
    "]\n",
    "for e in [\n",
    "    \"PER\",\n",
    "    \"USG%\",\n",
    "    \"TS%\",\n",
    "    \"3PAr\",\n",
    "    \"FTr\",\n",
    "    \"ORB%\",\n",
    "    \"DRB%\",\n",
    "    \"TRB%\",\n",
    "    \"AST%\",\n",
    "    \"STL%\",\n",
    "    \"BLK%\",\n",
    "    \"TOV%\",\n",
    "]:\n",
    "    stats_now[e] = pd.to_numeric(stats_now[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Multiclass_(`sklearn`-based)"
   },
   "source": [
    "## Multiclass (`sklearn`-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already mentioned how the binary case needs to be extended to the multiclass case.\n",
    "\n",
    "However, the multiclass may be subtle due to ambiguities in the class asignment procedure. Let's see why.\n",
    "\n",
    "For the binary case\n",
    "\n",
    "- We have labels, which are usually written as {0,1},{-1,1},{[1,0],[0,1]}. As an example, `sklearn` usually takes the first convention. In any self-written implementation care should be taken when loading the labels.\n",
    "- We have predictions. For the binary case we only need to specify one number. This number could be a discriminant function $y$ (as in a Perceptron or a Support Vector Classifier) or a probability  $p(C_{1}|x)$ (Logistic Regression, Naive Bayes). We thus only need to learn a **single function which defines all decision boundaries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a toy dataset from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "gt_center = np.array([[2.0, 2.0], [-2.0, -2.0]])\n",
    "X, t = make_blobs(\n",
    "    1000,\n",
    "    n_features=2,\n",
    "    centers=gt_center,\n",
    "    cluster_std=1.5,\n",
    "    random_state=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a discriminant function and a discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "lr = LogisticRegression()\n",
    "percep = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, t)\n",
    "percep.fit(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Perceptron case, we have a single decision function, where the usual criteria is that we label a datapoint as positive if $y\\geq0$. This can be modified to explore different precision/recall trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(percep.decision_function(X[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = percep.decision_function(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, levels=[-1000, 0.0, 1000])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regressor, we have two probabilities which add to 1. We can then only store the second column outputted by `sklearn`. The default criteria is to assign as positives all datapoints that have $p(C_{1}|x)\\geq 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.predict_proba(X[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = lr.predict_proba(Xvals)[:, 1].reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, levels=[0.0, 0.5, 1.0])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go the to the multiclass case:\n",
    "\n",
    "- We have the labels, which are essentially written in two ways: {0,1,...,K-1} or {[1,0,...,0],[0,1,0,...,0],...} (which is called the one-in-K representation). `sklearn` can use the first one and adapt it internally even if the second one is preferred algorithmically. Again, hand-written implementations need to be careful.\n",
    "- We have the predictions. Here is where the big difference between binary and multiclass appears. How do we decide on assignments? It depends on the algorithm, which may choose among three possibilities:\n",
    "1. One-vs-Remainder / One-vs-All: For each class $k$ we train a classifier that distinguishes between that class and the rest. We have then $K$ classifiers which need to be combined. For example, for 4 classes we have: (0,123), (1,023), (2,013), (3,012). A class asignment is found by interesecting the different classifier regions. This introduces the possibility of ambiguous choices. Even more, usually one runs into very imbalanced classifiers.\n",
    "2. One-vs-one: Here we train a classifier for each combination of two classes, obtaining  $\\frac{K(K-1)}{2}$ classifiers in total. For 4 classes we have: (0,1), (0,2), (0,3), (1,2), (1,3), (2,3). A class assigned by majority vote, which also introduces ambiguities.\n",
    "3. The third approach, which doesn't have ambiguities, is to train a single classifier that predicts among the $K$ classes. For a discriminative function-based classifier, this is achieved by learning K functions $y_{k}$ at the same time and assigning a class by finding the maximum $y_{k}$ among all $K$ functions. The decision boundaries appear when any pair of functions coincide $y_{k}=y_{k'}$. This criteria is unambiguous. For discriminative and generative models, this is actually the easiest approach. We just need to learn  $p(C_{k}|x)$ as detailed before.\n",
    "\n",
    "Why do we need to use criteria 1 or 2? Some algorithms cannot use the third approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "gt_center = np.array([[3.0, 3.0], [-3.0, -3.0], [-3.0, 3.0], [3.0, -3.0]])\n",
    "X, t = make_blobs(\n",
    "    1000,\n",
    "    n_features=2,\n",
    "    centers=gt_center,\n",
    "    cluster_std=1.0,\n",
    "    random_state=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a discriminant function and a discriminative model look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lda = LinearDiscriminantAnalysis(solver=\"lsqr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, t)\n",
    "lda.fit(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now LDA has four decision functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.decision_function(X[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(4 * 2 * 4, 2 * 3))\n",
    "for k in range(4):\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 0, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"red\",\n",
    "        label=\"$C_{0}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 1, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"blue\",\n",
    "        label=\"$C_{1}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 2, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"limegreen\",\n",
    "        label=\"$C_{2}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lda.decision_function(X)[t == 3, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"gold\",\n",
    "        label=\"$C_{3}$\",\n",
    "    )\n",
    "    ax[k].set_xlabel(str(k) + \"-th function\")\n",
    "    ax[k].legend(loc=\"upper right\", framealpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = np.argmax(lda.decision_function(Xvals), axis=1).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for the Logistic Regressor we have four probabilities that add up to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.predict_proba(X[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(4 * 2 * 4, 2 * 3))\n",
    "for k in range(4):\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 0, k], histtype=\"step\", color=\"red\", label=\"$C_{0}$\"\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 1, k], histtype=\"step\", color=\"blue\", label=\"$C_{1}$\"\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 2, k],\n",
    "        histtype=\"step\",\n",
    "        color=\"limegreen\",\n",
    "        label=\"$C_{2}$\",\n",
    "    )\n",
    "    ax[k].hist(\n",
    "        lr.predict_proba(X)[t == 3, k], histtype=\"step\", color=\"gold\", label=\"$C_{3}$\"\n",
    "    )\n",
    "    ax[k].set_xlabel(str(k) + \"-th probability\")\n",
    "    ax[k].legend(loc=\"upper center\", framealpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = np.argmax(lr.predict_proba(Xvals), axis=1).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on algorithms that follow the third approach (this is going to be the case for Neural Networks for example). We can still compute the accuracy and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(t, lr.predict(X)))\n",
    "print(accuracy_score(t, lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other metrics have the issue of being designed for the binary case. We can always binarize the problem and train a series of binary classifiers. Another possibility is to take the classifier and binarize the labels after training. The only issue is how to re-define the threshold in this case. Because the threshold looks at relative values between per-class probabilities, it is not trivial how to explore the precision-recall trade-off with a simple parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "for k in range(4):\n",
    "    print(\"Class \" + str(k) + \" against the remainder\")\n",
    "    print(f1_score(np.where(t == k, 1.0, 0.0), np.where(lr.predict(X) == k, 1.0, 0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Perceptron` and the `SVC` classifiers are not trivial to extend to the multiclass case. The former uses the One-vs-Remainder criteria. The latter has two possibilities:\n",
    "- `LinearSVC` considers the One-vs-Remainder criteria. It can however use the `crammer_singer` criteria which approximates the third possibility detailed above.\n",
    "- `SVC` uses the One-vs-One approach. However, and due to compatibility, it outputs the decision function as if it was One-vs-Reaminder. It can be morphed into One-vs-One with a simple option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "lsvc_1 = LinearSVC(loss=\"hinge\", penalty=\"l2\", C=10.0, max_iter=1000)\n",
    "svc_1 = SVC(kernel=\"linear\", C=10.0, decision_function_shape=\"ovr\")\n",
    "svc_2 = SVC(kernel=\"linear\", C=10.0, decision_function_shape=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_1.fit(X, t)\n",
    "svc_1.fit(X, t)\n",
    "svc_2.fit(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsvc_1.decision_function(X[:1]).shape)\n",
    "print(svc_1.decision_function(X[:1]).shape)\n",
    "print(svc_2.decision_function(X[:1]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using a linear kernel (`kernel=linear`), we can see the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsvc_1.coef_.shape)  # (nclasses, ncoefs)\n",
    "print(svc_1.coef_.shape)  # (nclasses*(nclasses-1)/2, ncoefs)\n",
    "print(svc_2.coef_.shape)  # (nclasses*(nclasses-1)/2, ncoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the decision function, does not change how `SVC` learns. It is always One-vs-One.The ordering is from 0 to K-1 with \u201c0 vs 1\u201d, \u201c0 vs 2\u201d , \u2026 \u201c0 vs K-1\u201d, \u201c1 vs 2\u201d, \u201c1 vs 3\u201d, \u201c1 vs K-1\u201d, . . . \u201cK-2 vs K-1\u201d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(svc_1.coef_, svc_2.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsvc_1.decision_function(X[:1]), len(lsvc_1.decision_function(X[:1])[0]))\n",
    "print(lsvc_1.predict(X[:1]))\n",
    "print(svc_1.decision_function(X[:1]), len(svc_1.decision_function(X[:1])[0]))\n",
    "print(svc_1.predict(X[:1]))\n",
    "print(svc_2.decision_function(X[:1]), len(svc_2.decision_function(X[:1])[0]))\n",
    "print(svc_2.predict(X[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = lsvc_1.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = svc_1.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.scatter(\n",
    "    svc_1.support_vectors_[:, 0],\n",
    "    svc_1.support_vectors_[:, 1],\n",
    "    marker=\"+\",\n",
    "    color=\"black\",\n",
    "    label=\"SV\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)\n",
    "x2vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "X1, X2 = np.meshgrid(x1vals, x2vals)\n",
    "Xvals = np.array([X1.ravel(), X2.ravel()]).T\n",
    "Z = svc_2.predict(Xvals).reshape(X1.shape)\n",
    "plt.contourf(X1, X2, Z, cmap=\"plasma\", levels=[-0.5, 0.5, 1.5, 2.5, 3.5])\n",
    "plt.colorbar()\n",
    "plt.scatter(X[t == 0, 0], X[t == 0, 1], c=\"red\", label=\"$C_{0}$\")\n",
    "plt.scatter(X[t == 1, 0], X[t == 1, 1], c=\"blue\", label=\"$C_{1}$\")\n",
    "plt.scatter(X[t == 2, 0], X[t == 2, 1], c=\"limegreen\", label=\"$C_{2}$\")\n",
    "plt.scatter(X[t == 3, 0], X[t == 3, 1], c=\"salmon\", label=\"$C_{3}$\")\n",
    "plt.scatter(\n",
    "    svc_2.support_vectors_[:, 0],\n",
    "    svc_2.support_vectors_[:, 1],\n",
    "    marker=\"+\",\n",
    "    color=\"black\",\n",
    "    label=\"SV\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the famous MNIST dataset (we could also do the IRIS).\n",
    "\n",
    "Train a classifier on the full set of 10 labels, and compute all possible metrics. Decide which numbers are easy to distinguish and which are hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is a classic ML benchmark. It consists of 70 000 small images containing hand-written digits. The \"target\" is the number they're supposed to be.\n",
    "\n",
    "This dataset is so common that `sklearn` has a function to download them. Check the specific function for the `sklearn` version you have. We can borrow some code to order the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from Geron\n",
    "def sort_by_target(mnist):\n",
    "    reorder_train = np.array(\n",
    "        sorted([(target, i) for i, target in enumerate(mnist.target.loc[: 60000 - 1])])\n",
    "    )[:, 1]\n",
    "    reorder_test = np.array(\n",
    "        sorted([(target, i) for i, target in enumerate(mnist.target.loc[60000:])])\n",
    "    )[:, 1]\n",
    "    mnist.data[:60000] = mnist.data.loc[reorder_train]\n",
    "    mnist.target[:60000] = mnist.target.loc[reorder_train]\n",
    "    mnist.data[60000:] = mnist.data.loc[reorder_test + 60000]\n",
    "    mnist.target[60000:] = mnist.target.loc[reorder_test + 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "\n",
    "    mnist = fetch_openml(\"mnist_784\", version=1, cache=True)\n",
    "    mnist.target = mnist.target.astype(\n",
    "        np.int8\n",
    "    )  # fetch_openml() returns targets as strings\n",
    "    sort_by_target(mnist)  # fetch_openml() returns an unsorted dataset\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "\n",
    "    mnist = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a `data` and `target` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images have 784 features, each corresponding to a pixel of the 28 x 28 image. The feature values are in the [0,255] range where 0 is white and 255 is black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = np.array(mnist[\"data\"]), np.array(mnist[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select a random digit to visualize.\n",
    "a_digit = X[35735]\n",
    "\n",
    "# we can reshape the digit to 28x28 pixels, and plot it\n",
    "a_digit_image = a_digit.reshape(28, 28)\n",
    "plt.imshow(a_digit_image, cmap=mpl.cm.binary, interpolation=\"None\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a classifier, it is useful to scale the features. Remember that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Theory",
    "Logistic_Regression",
    "Exercise",
    "Metrics",
    "Exercise",
    "Preprocessing",
    "Multiclass_(`sklearn`-based)",
    "Exercise"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}