{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Computer Modern Roman\",\n",
    "    }\n",
    ")\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "import numpy as np\n",
    "\n",
    "# if Graphviz is uninstalled\n",
    "# !pip install graphviz\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Autodifferentiation"
   },
   "source": [
    "# Autodifferentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary: In this tutorial we will build our own simplified autodifferentiation engine.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Analytic_versus_numerical_differentiation"
   },
   "source": [
    "## Analytic versus numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiation should be a familiar concept - let's start by considering the following one-dimensional function:\n",
    "$$\n",
    "f(x) = x^3 - 10x^2 + 5x - 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "f = lambda x: x**3 - 10 * x**2 + 5 * x - 2\n",
    "\n",
    "# Define a range of x values\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = f(x)\n",
    "\n",
    "# Plot the function\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "ax.plot(x, y, label=r\"$f(x) = x^3 - 10x^2 + 5x - 2$\")\n",
    "ax.set_xlabel(r\"$x$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$f(x)$\", fontsize=14)\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the derivative of this function with respect to $x$? This amounts to understanding the \"slope\" at a given point. The first way we learn to do this is via finite difference\n",
    "$$\n",
    "\\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "which can be implemented practically as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the finite difference derivative\n",
    "finite_diff_derivative = lambda f, x, h: (f(x + h) - f(x)) / h\n",
    "\n",
    "# Set the step size\n",
    "h = 1e-5\n",
    "# Compute the derivative at a specific point\n",
    "x0 = 6\n",
    "df = finite_diff_derivative(f, x0, h)\n",
    "\n",
    "# Initialize figure and axis for plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "# Plot the original function\n",
    "ax.plot(x, y, label=r\"$f(x) = x^3 - 10x^2 + 5x - 2$\")\n",
    "\n",
    "# Plot the tangent line\n",
    "x_tangent_finite_diff = np.linspace(x0 - 1, x0 + 1, 100)\n",
    "y_tangent_finite_diff = f(x0) + df * (x_tangent_finite_diff - x0)\n",
    "ax.plot(\n",
    "    x_tangent_finite_diff,\n",
    "    y_tangent_finite_diff,\n",
    "    color=\"orange\",\n",
    "    label=r\"$f'(x)$ $(\\mathrm{finite \\, difference})$\",\n",
    ")\n",
    "ax.plot(x0, f(x0), \"o\", ms=4, color=\"orange\")\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel(r\"$x$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$f(x)$\", fontsize=14)\n",
    "ax.legend(frameon=False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we also know that the derivative of this function can be computed completely analytically\n",
    "$$\n",
    "\\frac{df}{dx} = 3x^2 - 20x + 5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analytic derivative\n",
    "analytic_derivative = lambda x: 3 * x**2 - 20 * x + 5\n",
    "\n",
    "# Compute the derivative at a specific point\n",
    "x0 = 6\n",
    "df_analytic = analytic_derivative(x0)\n",
    "\n",
    "# Initialize figure and axis for plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "# Plot the original function\n",
    "ax.plot(x, y, label=r\"$f(x) = x^3 - 10x^2 + 5x - 2$\")\n",
    "\n",
    "# Plot the tangent line\n",
    "x_tangent_analytic = np.linspace(x0 - 1, x0 + 1, 100)\n",
    "y_tangent_analytic = f(x0) + df_analytic * (x_tangent_analytic - x0)\n",
    "\n",
    "# Finite difference\n",
    "ax.plot(\n",
    "    x_tangent_finite_diff,\n",
    "    y_tangent_finite_diff,\n",
    "    color=\"orange\",\n",
    "    label=r\"$f'(x)$ (finite difference)\",\n",
    ")\n",
    "ax.plot(x0, f(x0), \"o\", ms=4, color=\"orange\")\n",
    "# Analytic\n",
    "ax.plot(\n",
    "    x_tangent_analytic,\n",
    "    y_tangent_analytic,\n",
    "    \"--\",\n",
    "    color=\"red\",\n",
    "    label=r\"$f'(x)$ (analytic)\",\n",
    ")\n",
    "ax.plot(x0, f(x0), \"o\", ms=4, color=\"red\")\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel(r\"$x$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$f(x)$\", fontsize=14)\n",
    "ax.legend(frameon=False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Autodifferentiation"
   },
   "source": [
    "## Autodifferentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All modern machine learning paradigms rely on constructing complex expressions from a sequence of differentiable operations. These expressions often involve anywhere from $\\mathcal{O}(\\text{tens})$ to $\\mathcal{O}(\\text{billions})$ of operations and parameters. To efficiently compute gradients, the entire computation must be organized to enable low-overhead access to derivative information from the outset. This process (tracking derivatives throughout a computation) is known as **automatic differentiation**, or autodifferentiation (*autodiff* for short). The prefix auto- reflects the fact that the derivatives of each operation are made available automatically during execution.\n",
    "\n",
    "Software systems that implement this functionality are called **autodifferentiation engines**, and all modern differentiable programming libraries are built around them. As we will see, the design choices behind these engines significantly shape how differential libraries are used in practice.\n",
    "\n",
    "In the remainder of this tutorial, we will slowly build up a simplified autodiff engine from scratch. We will restrict ourselves to scalar functions (functions which map arbitrary dimensional input to a single scalar value $f(\\vec{x}) \\to \\mathbb{R}$). To start let's build up a new class called `Scalar` that will handle and define the core organization of our autodiff engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Scalar(1.0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, our class only has the ability to define new data types - if we also want to perform operations on these class instances, we can define them using the dunder methods (https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types). For example, we can define the `Scalar` \"addition\" operation using the `__add__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            return Scalar(self.data + other.data)\n",
    "        else:\n",
    "            return Scalar(self.data + other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add two Scalar instances\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some more common utility operations that we may want to use (`__sub__`, `__mul__`, for subtraction and multiplication, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            return Scalar(self.data + other.data)\n",
    "        else:\n",
    "            return Scalar(self.data + other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            return Scalar(self.data - other.data)\n",
    "        else:\n",
    "            return Scalar(self.data - other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            return Scalar(self.data * other.data)\n",
    "        else:\n",
    "            return Scalar(self.data * other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the dunder operations work as expected\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e * b\n",
    "print(\"c = a + b =>\", a.data, \"+\", b.data, \"=\", c.data)\n",
    "print(\"d = a - b =>\", a.data, \"-\", b.data, \"=\", d.data)\n",
    "print(\"e = c * d =>\", c.data, \"*\", d.data, \"=\", e.data)\n",
    "print(\"f = e * b =>\", e.data, \"*\", b.data, \"=\", f.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tracing_and_computation_graphs"
   },
   "source": [
    "## Tracing and computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we iteratively constructed the quantity `f`:\n",
    "```\n",
    "f = b * ((a + b) * (a - b))\n",
    "```\n",
    "through a set of successive operations. In the end, we want to be able to understand how a change in any of the values in each operation may effect the final output value. In other words, in order to understand how the variables (`a` and `b`) influence the final *expression* (`f`) we need to track which operations are used in constructing the expression (also known as constructing a ***computational graph***). In differential programming, the process of recording operations and generating a computational graph is known as ***tracing***.\n",
    "\n",
    "In our implementation, we'll trace the preceding operations via the iterable `set()` function. In Python the `set()` function stores an unordered collection of unique and immutable (cannot be changed in-place) objects. In the context of tracing, `set()` is nice because it ensures each node contains a unique set of operations, however, this is a convention - Python lists or tuples could also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data, _children=(), _op=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Scalar(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Scalar(self.data - other.data, _children=(self, other), _op=\"-\")\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Scalar(self.data * other.data, _children=(self, other), _op=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = b * ((a + b) * (a - b))\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e * b\n",
    "\n",
    "# Now we have access to our computation graph\n",
    "# Print in the syntax ({input_1, input_2}, operation) = value\n",
    "print(\"(\", f._prev, \",\", f._op, \") =\", f)\n",
    "print(\"(\", e._prev, \",\", e._op, \") =\", e)\n",
    "print(\"(\", d._prev, \",\", d._op, \") =\", d)\n",
    "print(\"(\", c._prev, \",\", c._op, \") =\", c)\n",
    "print()\n",
    "\n",
    "# Note the a and b do not have children, values without children\n",
    "# are commonly referred to as \"leaf nodes\" in the computation graph\n",
    "print(\"(\", a._prev, \",\", a._op, \") =\", a)\n",
    "print(\"(\", b._prev, \",\", b._op, \") =\", b)\n",
    "print()\n",
    "\n",
    "# Note that we should be cautious to only use binary operations when set() is used,\n",
    "# information about the computation graph can easily be lost -- for example:\n",
    "g = a + b + a\n",
    "print(\"(\", g._prev, \",\", g._op, \") =\", g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be useful to output the computation graph\n",
    "def graph(scalar):\n",
    "    \"\"\"\n",
    "    Recursively prints the computation graph of a Scalar object.\n",
    "    \"\"\"\n",
    "    if scalar._prev:\n",
    "        print(f\"({scalar._prev}, '{scalar._op}') = {scalar.data}\")\n",
    "        for child in scalar._prev:\n",
    "            graph(child)\n",
    "    else:\n",
    "        print(f\"Leaf node, {scalar.data}\")\n",
    "\n",
    "\n",
    "# f = b * ((a + b) * (a - b))\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e * b\n",
    "\n",
    "# Now we can \"see\" the graph\n",
    "graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more powerful and informative computation graph output, we can add optional labels to each `Scalar()` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=None):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        if label is not (None):\n",
    "            self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Scalar(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Scalar(self.data - other.data, _children=(self, other), _op=\"-\")\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Scalar(self.data * other.data, _children=(self, other), _op=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(scalar):\n",
    "    \"\"\"\n",
    "    Recursively prints the computation graph of a Scalar object.\n",
    "    \"\"\"\n",
    "    if scalar._prev:\n",
    "        print(f\"{scalar.label} = ({scalar._prev}, '{scalar._op}') = {scalar.data}\")\n",
    "        for child in scalar._prev:\n",
    "            graph(child)\n",
    "    else:\n",
    "        print(f\"Leaf node, {scalar.label} = {scalar.data}\")\n",
    "\n",
    "\n",
    "# f = b * ((a + b) * (a - b))\n",
    "a = Scalar(1.0)\n",
    "a.label = \"a\"\n",
    "b = Scalar(2.0)\n",
    "b.label = \"b\"\n",
    "c = a + b\n",
    "c.label = \"c\"\n",
    "d = a - b\n",
    "d.label = \"d\"\n",
    "e = c * d\n",
    "e.label = \"e\"\n",
    "f = e * b\n",
    "f.label = \"f\"\n",
    "\n",
    "# Now we can see the graph\n",
    "graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above \"graph\" is rudimentary at best, it would be much nicer to have a full visualization. Without going into details, we can define the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(scalar):\n",
    "    \"\"\"\n",
    "    Trace the computation graph of the Scalar object and return nodes and edges.\n",
    "    \"\"\"\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(scalar)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def visualize_graph(scalar):\n",
    "    \"\"\"\n",
    "    Visualize the computation graph of the Scalar object using Graphviz.\n",
    "    \"\"\"\n",
    "    nodes, edges = trace(scalar)\n",
    "    graph = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "    for n in nodes:\n",
    "        graph.node(\n",
    "            name=str(id(n)),\n",
    "            label=f\"{n.label} | data: {n.data:.4f}\",\n",
    "            shape=\"record\",\n",
    "            style=\"filled\",\n",
    "            fillcolor=\"white\",\n",
    "        )\n",
    "        if n._op:\n",
    "            graph.node(\n",
    "                name=str(id(n)) + n._op,\n",
    "                label=n._op,\n",
    "                shape=\"circle\",\n",
    "                facecolor=\"lightgray\",\n",
    "                style=\"filled\",\n",
    "            )\n",
    "            graph.edge(str(id(n)) + n._op, str(id(n)))\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        graph.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now output a nice visualization of the computation graph\n",
    "visualize_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "The_goal_of_autodiff"
   },
   "source": [
    "## The goal of autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a nice visualization of the computation graph, we can start to answer the question we've been aiming for since the first construction of the `Scalar()` class:\n",
    "\n",
    "**\"How does a change in the values `a`, `b`, `c`, `d`, or `e` influence the value of `f`?\"**\n",
    "\n",
    "There is a very crude way to understand this question: increase the value of interest by a small amount and see how the value of `f` changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define step size\n",
    "h = 1e-5\n",
    "\n",
    "# Default value of f\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f1 = e * b\n",
    "\n",
    "# Change in f with respect to a\n",
    "a = Scalar(1.0 + h)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f2 = e * b\n",
    "print(\"Change in f with respect to a:\", (f2.data - f1.data) / h)\n",
    "\n",
    "# Change in f with respect to b\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0 + h)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f2 = e * b\n",
    "print(\"Change in f with respect to b:\", (f2.data - f1.data) / h)\n",
    "\n",
    "# Change in f with respect to c\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "c += Scalar(h)\n",
    "d = a - b\n",
    "e = c * d\n",
    "f2 = e * b\n",
    "print(\"Change in f with respect to c:\", (f2.data - f1.data) / h)\n",
    "\n",
    "# Change in f with respect to d\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "d += Scalar(h)\n",
    "e = c * d\n",
    "f2 = e * b\n",
    "print(\"Change in f with respect to d:\", (f2.data - f1.data) / h)\n",
    "\n",
    "# Change in f with respect to e\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "e += Scalar(h)\n",
    "f2 = e * b\n",
    "print(\"Change in f with respect to e:\", (f2.data - f1.data) / h)\n",
    "\n",
    "# Change in f with respect to f\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f2 = e * b\n",
    "f2 += Scalar(h)\n",
    "print(\"Change in f with respect to f:\", (f2.data - f1.data) / h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly what we have done above is just taking the derivative of `f` with respect to each of it's dependent parameters. The above example would may make it seem like this would be a perfectly good way to do things, however, I hope it's clear that this \"manual\" or finite-difference prescription above is not scaleable. Take, for instance, an expression with 1,000 leaf nodes (independent variables)!\n",
    "\n",
    "Ideally we'd like to keep the exact gradient value for each parameter either as the computation graph is built (forward-mode autodifferentiation) or by traversing backwards after the graph is constructed (reverse-mode autodifferentiation). Because many machine learning tasks and neural networks rely heavily on the latter, we will mainly focus on reverse-mode autodiff and comment briefly on forward mode at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Reverse-mode_autodiff"
   },
   "source": [
    "## Reverse-mode autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse-mode autodiff works by first storing the dependencies of the expression tree in memory (tracing), typically referred to as the ***forward-pass***, followed by a reverse or ***backward-pass*** through the computation graph in which partial derivatives of the final output with respect to each intermediate variables (adjoints) are computed.\n",
    "\n",
    "Up to now we have fully implemented the forward-pass, specifically tracing the operations constructing the computational graph. For the backward-pass, the main goal is to compute gradients for each `Scalar` instance in the computation graph. This requires each instance to have a gradient (`self.grad`) attribute which we will conventionally initialize to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=None):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        if label is not (None):\n",
    "            self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Scalar(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Scalar(self.data * other.data, _children=(self, other), _op=\"*\")\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Scalar(self.data - other.data, _children=(self, other), _op=\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(scalar):\n",
    "    \"\"\"\n",
    "    Visualize the computation graph of the Scalar object using Graphviz.\n",
    "    \"\"\"\n",
    "    nodes, edges = trace(scalar)\n",
    "    graph = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "    for n in nodes:\n",
    "        graph.node(\n",
    "            name=str(id(n)),\n",
    "            label=f\"{n.label} | data: {n.data:.4f} | grad: {n.grad:.4f}\",\n",
    "            shape=\"record\",\n",
    "            style=\"filled\",\n",
    "            fillcolor=\"white\",\n",
    "        )\n",
    "        if n._op:\n",
    "            graph.node(\n",
    "                name=str(id(n)) + n._op,\n",
    "                label=n._op,\n",
    "                shape=\"circle\",\n",
    "                facecolor=\"lightgray\",\n",
    "                style=\"filled\",\n",
    "            )\n",
    "            graph.edge(str(id(n)) + n._op, str(id(n)))\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        graph.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have access to our computation graph\n",
    "a = Scalar(1.0, label=\"a\")\n",
    "b = Scalar(2.0, label=\"b\")\n",
    "c = a + b\n",
    "c.label = \"c\"\n",
    "d = a - b\n",
    "d.label = \"d\"\n",
    "e = c * d\n",
    "e.label = \"e\"\n",
    "f = e * b\n",
    "f.label = \"L\"\n",
    "\n",
    "visualize_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to go through each node manually first, and then think about ways to automate the process. The simplest node is the output node (and the one, at the end of the day, that we will always set manually as a seed for the rest of the gradients to grow off of)\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{ \\partial \\mathcal{L}} = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.grad = 1.0\n",
    "visualize_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform what would typically be called the first `backward` call (typically referred to as calling \"backward\" on `L` and what will be called `L._backward` later), specifically, we'd like to set the gradients of each of the nodes that flow into the output node, i.e. `e` and `b`. The gradient with respect to `e` is trivial if we remember our rules for differentiation from kindergarten\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial e} = b,\n",
    "$$\n",
    "the derivative with respect to `b`, on the other hand, is slightly less trivial -- we will return to this once we have made our way through the whole graph.\n",
    "\n",
    "For now, let's set the gradient for `e`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.grad = b.data\n",
    "visualize_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have reached a key crossroad in autodifferentiation. If you understand how to compute the gradient at these nodes, you more or less understand how all neural networks work. We want to compute the gradient of $\\mathcal{L}$ with respect to `c` and `d`. The key to the whole castle rests in the hands of the chain rule\n",
    "$$\n",
    "\\texttt{c.grad} = \\frac{\\partial \\mathcal{L}}{\\partial c} = \\frac{\\partial \\mathcal{L}}{\\partial e}\\frac{\\partial e}{\\partial c},\n",
    "$$\n",
    "remember that we have already traversed the \"`e`\" node when we performed the backward call on `L`, we already have access to $\\partial \\mathcal{L} / \\partial e$ through `e`'s `self.grad`! All we need to compute is the \"local\" derivative of `e` with respect to `c`\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial c} = d\n",
    "$$\n",
    "which, when combined with `e.grad` gives\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial c} = b \\times d.\n",
    "$$\n",
    "Likewise, for `d` we have\n",
    "$$\n",
    "\\texttt{d.grad} = \\frac{\\partial \\mathcal{L}}{\\partial d} = \\frac{\\partial \\mathcal{L}}{\\partial e}\\frac{\\partial e}{\\partial d} = b \\times c.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.grad = d.data * b.data\n",
    "d.grad = c.data * b.data\n",
    "visualize_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the final two gradients, the gradients `L` with respect to `a` and `b` can be found following the same prescription as above but with a small twist. We continue with the chain rule\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial \\mathcal{L}}{\\partial e} \\frac{\\partial e}{\\partial a}\n",
    "$$\n",
    "however, in this case, `e` is a function of two parameters, `e = c(a) * d(a)`, each with a dependence on `a`. Luckily, we also know how differentiation behaves with respect to composite functions\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial \\mathcal{L}}{\\partial e} \\left(\\frac{\\partial e}{\\partial c} \\frac{\\partial c}{\\partial a} + \\frac{\\partial e}{\\partial d} \\frac{\\partial d}{\\partial a}\\right) = \\frac{\\partial \\mathcal{L}}{\\partial c}\\frac{\\partial c}{\\partial a} + \\frac{\\partial \\mathcal{L}}{\\partial d}\\frac{\\partial d}{\\partial a} = b (c + d) = 2ab\n",
    "$$\n",
    "and we again see that we only need to compute the \"local\" derivatives $\\partial c/\\partial a$ and $\\partial d/\\partial a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad = 2 * a.data * b.data\n",
    "b.grad = -3 * b.data**2 + a.data**2\n",
    "visualize_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:"
   },
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -3b^2 + a^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:"
   },
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Why don't we want to implement `__sub__` as it's own method with it's own `_backward` function?\n",
    "\n",
    "Hint: A similar reason would arise when implementing, for example, division.\n",
    "\n",
    "b. What item in the `Scalar` constructor would we need to modify if we wanted to fix the problem presented above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Automated_backward-pass"
   },
   "source": [
    "## Automated backward-pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to embed the backwards pass into our `Scalar` class. From our setup, each `Scalar` instance has it's own \"local\" operation through `_op` as well as the inputs through `_children` (`_prev`). During the backward-pass, we need to assign each instance a \"derivative-function\" that will compute the gradient of the inputs with respect to the outputs and assign these gradients to the inputs (`_children`) when the backward function is called on the output. These \"backward\" functions are operation dependent and therefore should be implemented in the dunder methods.\n",
    "\n",
    "The main things to remember are that:\n",
    "\n",
    "1. At any given position in the computation graph during the backward pass we always have access to the gradient of the output with respect to the forward-adjacent node.\n",
    "2. the chain rule tells us that if we have access to the gradient of the forward-adjacent node, all we need is the derivative of the local operation (the derivative of the current output with respect to its inputs).\n",
    "\n",
    "Let's work this out explicitly for `Scalar` with a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "`__add__`"
   },
   "source": [
    "#### `__add__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two cases to consider, the first case is if we are distributing gradients from the final output (remember that the backward-pass starts by assigning the gradient of the final output to 1), `output = a + b`, in which case the gradients of the inputs are trivial\n",
    "$$\n",
    "\\frac{\\partial \\texttt{output}}{\\partial \\texttt{a}} = \\frac{\\partial (\\texttt{a + b})}{\\partial \\texttt{a}} = 1.\n",
    "$$\n",
    "The second relevant situation is anywhere inside the computation graph (say, for example, one beyond adjacent to the output). Take, for example,\n",
    "```\n",
    "a = b + c\n",
    "output = a * d.\n",
    "```\n",
    "We're interested in the derivative of the final `output` with respect to `b` (which we will compute later in `Scalar` by calling `a._backward()`)  - in this case, we would have already computed the derivative of the `output` with respect to `a`, and thus have access to $\\partial$`output`$/\\partial$`a`\n",
    "$$\n",
    "\\frac{\\partial \\texttt{output}}{\\partial \\texttt{b}} = \\frac{\\partial \\texttt{output}}{\\partial \\texttt{a}} \\frac{\\partial \\texttt{a}}{\\partial \\texttt{b}} = \\frac{\\partial \\texttt{output}}{\\partial \\texttt{a}} \\frac{\\partial (\\texttt{b + c})}{\\partial \\texttt{b}} = \\frac{\\partial \\texttt{output}}{\\partial \\texttt{a}}.\n",
    "$$\n",
    "In both cases, the backwards pass needs to assign the same gradient.\n",
    "\n",
    "Take a moment and convince yourself of two things:\n",
    "\n",
    "1. The argument above applies no matter the position in the computation graph.\n",
    "2. The same result also applies for the gradients of the output with respect to the `other` output (`c` in the second example). Both cases can be accommodated by including a `_backward` function to the mother (output in the forward pass) which assigns gradients to the daughters (inputs during the forward pass) as:\n",
    "```\n",
    "def __add__(self, other):\n",
    "    output = Scalar(self.data + other.data, _children = (self, other), _op = '+')\n",
    "    def _backward():\n",
    "        self.grad += output.grad\n",
    "        other.grad += output.grad\n",
    "    output._backward = _backward\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An_aside_on_Python_closures"
   },
   "source": [
    "### An aside on Python closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem weird that during the backward pass we can call `_backward` on an output node and have this assign gradient values to its children by calling `self.grad` and `other.grad`. Why does the output, which is its own `Scalar` instance with its own `self` variables, remember its inputs without referring to `_children`? This is due to Python's ***closure*** abilities.\n",
    "\n",
    "A **closure** in Python is a function that \"remembers\" the variables from the scope in which it was created, even after that scope is done executing. When we define the `_backward` function inside a method like `__add__` or `__mul__`, the function captures the variables from its enclosing scope—specifically, the `self`, `other`, and `output` objects. This means that even after the `__add__` or `__mul__` method has returned and the output node is being used elsewhere, the `_backward` function still has access to the original input nodes (`self` and `other`). This is why, when we call `output._backward()`, the function can update the gradients of the input nodes directly (e.g., `self.grad += output.grad`), even though we're calling it from the context of the output node. The closure \"remembers\" the references to the input nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the arguments above, what should the `_backward` function be for the `__mul__` operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=None):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        if label is not (None):\n",
    "            self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data}, grad = {self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        output = Scalar(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output.grad\n",
    "            other.grad += output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        output = Scalar(self.data * other.data, _children=(self, other), _op=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            \"\"\"\n",
    "            self.grad += ...\n",
    "            other.grad += ...\n",
    "            \"\"\"\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __neg__(self):\n",
    "        output = Scalar(-self.data, _children=(self,), _op=\"neg\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad -= output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Scalar(1.0, label=\"a\")\n",
    "b = Scalar(2.0, label=\"b\")\n",
    "c = -b\n",
    "c.label = \"c\"\n",
    "d = a + b\n",
    "d.label = \"d\"\n",
    "e = a + c\n",
    "e.label = \"e\"\n",
    "f = d * e\n",
    "f.label = \"f\"\n",
    "L = f * b\n",
    "L.label = \"L\"\n",
    "\n",
    "# Manually set the gradient of the final output\n",
    "L.grad = 1.0\n",
    "# Look at the first backward iteration\n",
    "L._backward()\n",
    "visualize_graph(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Scalar(1.0, label=\"a\")\n",
    "b = Scalar(2.0, label=\"b\")\n",
    "c = -b\n",
    "c.label = \"c\"\n",
    "d = a + b\n",
    "d.label = \"d\"\n",
    "e = a + c\n",
    "e.label = \"e\"\n",
    "f = d * e\n",
    "f.label = \"f\"\n",
    "L = f * b\n",
    "L.label = \"L\"\n",
    "\n",
    "# String together the full backward pass\n",
    "L.grad = 1.0\n",
    "L._backward()\n",
    "f._backward()\n",
    "e._backward()\n",
    "d._backward()\n",
    "c._backward()\n",
    "\n",
    "# Visualize the final graph\n",
    "visualize_graph(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Topological_ordering_and_the_fully_automating_the_backward_pass"
   },
   "source": [
    "## Topological ordering and the fully automating the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the order in which we perform the `_backward` call for each node is key to obtaining the correct gradients during the backward pass. The final task is to automate the calls to each `_backward` function in the correct order. The single output scalar function graphs we are interested have been classified and studied in graph theory - they are called directed acyclic graphs (DAGs) and many algorithms exist for ordering nodes such that they don't become \"tangled\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    \"\"\"\n",
    "    Recursively build the topological order of the computation graph and perform the backward pass.\n",
    "    \"\"\"\n",
    "    # Topological order all of the children in the graph\n",
    "    topology = []\n",
    "    visited = set()\n",
    "\n",
    "    def build_topology(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "                build_topology(child)\n",
    "            topology.append(v)\n",
    "\n",
    "    build_topology(self)\n",
    "\n",
    "    # Manually set the gradient of the final output\n",
    "    self.grad = 1\n",
    "    # Iterate one variable at a time\n",
    "    for v in reversed(topology):\n",
    "        v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass\n",
    "a = Scalar(1.0, label=\"a\")\n",
    "b = Scalar(2.0, label=\"b\")\n",
    "c = -b\n",
    "c.label = \"c\"\n",
    "d = a + b\n",
    "d.label = \"d\"\n",
    "e = a + c\n",
    "e.label = \"e\"\n",
    "f = d * e\n",
    "f.label = \"f\"\n",
    "L = f * b\n",
    "L.label = \"L\"\n",
    "\n",
    "# Perform backward pass\n",
    "backward(L)\n",
    "visualize_graph(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=None):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        if label is not (None):\n",
    "            self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data}, grad = {self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "        output = Scalar(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output.grad\n",
    "            other.grad += output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "        output = Scalar(self.data * other.data, _children=(self, other), _op=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * output.grad\n",
    "            other.grad += self.data * output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __neg__(self):\n",
    "        if not isinstance(self, Scalar):\n",
    "            self = Scalar(self)\n",
    "        output = Scalar(-self.data, _children=(self,), _op=\"neg\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad -= output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "        output = self + (-other)\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Recursively build the topological order of the computation graph and perform the backward pass.\n",
    "        \"\"\"\n",
    "        # Topological order all of the children in the graph\n",
    "        topology = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "\n",
    "        build_topology(self)\n",
    "\n",
    "        # Manually set the gradient of the final output\n",
    "        self.grad = 1\n",
    "        # Iterate one variable at a time\n",
    "        for v in reversed(topology):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass\n",
    "a = Scalar(1.0, label=\"a\")\n",
    "b = Scalar(2.0, label=\"b\")\n",
    "c = -b\n",
    "c.label = \"c\"\n",
    "d = a + b\n",
    "d.label = \"d\"\n",
    "e = a + c\n",
    "e.label = \"e\"\n",
    "f = d * e\n",
    "f.label = \"f\"\n",
    "L = f * b\n",
    "L.label = \"L\"\n",
    "\n",
    "# Perform backward pass\n",
    "L.backward()\n",
    "visualize_graph(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the following functions and check that the `Scalar` class is performing as expected by\n",
    "\n",
    "1. Writing out the explicit computation graph,\n",
    "2. Computing the gradients manually, and\n",
    "3. Using the tools developed above to check your work (visualize the computation graph and double check the backward pass).\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = (a^3 - b^2) a - ab\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L} = (a + b + c) (-a + b + c)(a - b + c)(a + b - c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Custom_operations"
   },
   "source": [
    "## Custom operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we may desire to have custom operations as a node within our network that are not available via the traditional dunder methods. For example, in the next tutorial where we build out a neural network using the `Scalar` class, in some examples we will want to terminate our network with something called an **activation function**. For our purposes, this function will just be a simple $\\tanh$ but if we want it to jive with the backward-pass, we will need to implement the function itself in the `Scalar` class with instructions on how to pass derivatives to inputs (via the `_backward` method).\n",
    "\n",
    "For now, we'll implement $\\tanh$ using `NumPy`, which is already loaded. The only other thing we'll need is the derivative of $\\tanh$\n",
    "$$\n",
    "\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)\n",
    "$$\n",
    "so we can implement its `_backward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=None):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        if label is not (None):\n",
    "            self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar({self.data}, grad = {self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "        output = Scalar(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output.grad\n",
    "            other.grad += output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "        output = Scalar(self.data * other.data, _children=(self, other), _op=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * output.grad\n",
    "            other.grad += self.data * output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __neg__(self):\n",
    "        if not isinstance(self, Scalar):\n",
    "            self = Scalar(self)\n",
    "        output = Scalar(-self.data, _children=(self,), _op=\"neg\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad -= output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "        output = self + (-other)\n",
    "        return output\n",
    "\n",
    "    def tanh(self):\n",
    "        output = Scalar(np.tanh(self.data), _children=(self,), _op=\"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - output.data * output.data) * output.grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Recursively build the topological order of the computation graph and perform the backward pass.\n",
    "        \"\"\"\n",
    "        # Topological order all of the children in the graph\n",
    "        topology = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "\n",
    "        build_topology(self)\n",
    "\n",
    "        # Manually set the gradient of the final output\n",
    "        self.grad = 1\n",
    "        # Iterate one variable at a time\n",
    "        for v in reversed(topology):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Scalar(1.0, label=\"t\")\n",
    "y = t.tanh()\n",
    "y.label = \"y\"\n",
    "print(\"y =\", y)\n",
    "y.backward()\n",
    "visualize_graph(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `tanh` using it's exponential definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# Hint: As it stands so far, the current `Scalar` class does not have all of the required Python dunder methods implemented for this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Minimization"
   },
   "source": [
    "## Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fully completed one forward-pass followed by a backward-pass. After the backward pass, we have the gradients for each of the nodes in the computation graph. As we discussed at the beginning of this tutorial, one of the main goals of autodiff is to unlock the ability to optimize the final output of an arbitrarily complicated functions. So, how does access to gradients help us do this? Say I wanted to ***decrease*** the value of $\\mathcal{L}$. Clearly, I would want to move each input (leaf node) in a direction which would decrease the final output value. This can be achieved straightforwardly using the gradient information of the leaf nodes:\n",
    "$$\n",
    "a \\to a - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "$$\n",
    "where $\\alpha$ is an arbitrary parameter determining how large of a step to take when varying $\\mathcal{L}$, typically referred to in machine learning parlance as the ***learning rate***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass\n",
    "a = Scalar(1.0)\n",
    "b = Scalar(2.0)\n",
    "c = -b\n",
    "d = a + b\n",
    "e = a + c\n",
    "f = d * e\n",
    "L = f * b\n",
    "# Perform backward pass\n",
    "L.backward()\n",
    "\n",
    "print(\"Base L =\", L)\n",
    "\n",
    "agrad = a.grad\n",
    "bgrad = b.grad\n",
    "print(\"a.grad =\", a.grad)\n",
    "print(\"b.grad =\", b.grad)\n",
    "\n",
    "# Define learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# We want to minimize L\n",
    "a = a - alpha * a.grad\n",
    "b = Scalar(2.0)\n",
    "c = -b\n",
    "d = a + b\n",
    "e = a + c\n",
    "f = d * e\n",
    "La = f * b\n",
    "# La.backward()\n",
    "\n",
    "print(\"a updated L =\", La)\n",
    "\n",
    "# We want to minimize L\n",
    "a = Scalar(1.0)\n",
    "b = b - alpha * bgrad\n",
    "c = -b\n",
    "d = a + b\n",
    "e = a + c\n",
    "f = d * e\n",
    "Lb = f * b\n",
    "# Lb.backward()\n",
    "\n",
    "print(\"b updated L =\", Lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to find the global minimum through an iterated optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our function\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "# Create a 2d contour of f(a,b) = b * ((a + b) * (a - b))\n",
    "a_vals = np.linspace(-10, 10, 100)\n",
    "b_vals = np.linspace(-10, 10, 100)\n",
    "A, B = np.meshgrid(a_vals, b_vals)\n",
    "Z = B * ((A + B) * (A - B))\n",
    "# Plot a pcolomesh\n",
    "pcolormesh = ax.pcolormesh(A, B, Z, shading=\"auto\", cmap=\"viridis\")\n",
    "# Plot the contour\n",
    "contour = ax.contour(A, B, Z, levels=50, colors=\"black\", linewidths=0.5)\n",
    "# Plot the initial point\n",
    "ax.plot(a.data, b.data, \"ro\", label=\"Initial point (a, b)\")\n",
    "# Add a colorbar\n",
    "# cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar = fig.colorbar(pcolormesh, ax=ax)\n",
    "cbar.set_label(r\"$f(a, b)$\")\n",
    "ax.set_xlabel(r\"$a$\")\n",
    "ax.set_ylabel(r\"$b$\")\n",
    "ax.set_title(r\"$f(a, b) = b ((a + b)(a - b))$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now, very easily create a gradient vector field over any domain of interest\n",
    "def gradient_vector_field(x_range, y_range, step=0.5):\n",
    "    x_vals = np.arange(x_range[0], x_range[1], step)\n",
    "    y_vals = np.arange(y_range[0], y_range[1], step)\n",
    "    X, Y = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "    # Compute the gradient\n",
    "    dX = np.zeros_like(X)\n",
    "    dY = np.zeros_like(Y)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            a = Scalar(X[i, j])\n",
    "            b = Scalar(Y[i, j])\n",
    "            c = -b\n",
    "            d = a + b\n",
    "            e = a + c\n",
    "            f = d * e\n",
    "            L_val = f * b\n",
    "            L_val.backward()\n",
    "            dX[i, j] = a.grad\n",
    "            dY[i, j] = b.grad\n",
    "\n",
    "    return X, Y, dX, dY\n",
    "\n",
    "\n",
    "# Define the range for the gradient vector field\n",
    "x_range = (-10, 10)\n",
    "y_range = (-10, 10)\n",
    "\n",
    "# Compute the gradient vector field\n",
    "X, Y, dX, dY = gradient_vector_field(x_range, y_range)\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "# Plot the function as a pcolormesh\n",
    "pcolormesh = ax.pcolormesh(A, B, Z, shading=\"auto\", cmap=\"viridis\")\n",
    "# Plot the gradient vector field\n",
    "ax.quiver(\n",
    "    X, Y, dX, dY, color=\"black\", alpha=1.0, scale=3000\n",
    ")  # , headlength = 3, width = 0.01)\n",
    "# Plot the contour\n",
    "contour = ax.contour(A, B, Z, levels=50, colors=\"black\", linewidths=0.5)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(pcolormesh, ax=ax)\n",
    "cbar.set_label(r\"$f(a, b)$\")\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel(r\"$a$\")\n",
    "ax.set_ylabel(r\"$b$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define an automated optimization that looks to maximize or minimize the function of interest\n",
    "def minimize(a_init, b_init, alpha=0.01, steps=10):\n",
    "    a = Scalar(a_init, label=\"a\")\n",
    "    b = Scalar(b_init, label=\"b\")\n",
    "\n",
    "    a_b_trajectory = []\n",
    "    a_b_trajectory.append((a.data, b.data))\n",
    "\n",
    "    for _ in range(steps):\n",
    "        c = -b\n",
    "        d = a + b\n",
    "        e = a + c\n",
    "        f = d * e\n",
    "        L = f * b\n",
    "\n",
    "        # Perform backward pass\n",
    "        L.backward()\n",
    "\n",
    "        # Update parameters in direction that minimizes L\n",
    "        a.data -= alpha * a.grad\n",
    "        b.data -= alpha * b.grad\n",
    "\n",
    "        a_b_trajectory.append((a.data, b.data))\n",
    "\n",
    "        # Reset gradients for the next iteration\n",
    "        a.grad = 0.0\n",
    "        b.grad = 0.0\n",
    "\n",
    "    return a_b_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through n_trajectories trajectories with different initial conditions assigned randomly\n",
    "n_trajectories = 50\n",
    "trajectories = []\n",
    "for i in range(n_trajectories):\n",
    "    a_init = np.random.uniform(-10, 10)\n",
    "    b_init = np.random.uniform(-10, 10)\n",
    "    trajectory = minimize(a_init, b_init, alpha=1e-3, steps=25)\n",
    "    trajectories.append(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the minimum and maximum values for a and b for plotting\n",
    "a_min = -20\n",
    "a_max = 20\n",
    "b_min = -20\n",
    "b_max = 20\n",
    "\n",
    "# Create a grid for the pcolormesh\n",
    "a_vals = np.linspace(a_min, a_max, 100)\n",
    "b_vals = np.linspace(b_min, b_max, 100)\n",
    "A, B = np.meshgrid(a_vals, b_vals)\n",
    "# Compute the function values for the pcolormesh\n",
    "Z = B * ((A + B) * (A - B))\n",
    "\n",
    "# Create a figure for the trajectories\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "# Plot the function as a pcolormesh\n",
    "pcolormesh = ax.pcolormesh(A, B, Z, shading=\"auto\", cmap=\"viridis\")\n",
    "\n",
    "# Produce a unique color for each trajectory\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, len(trajectories)))\n",
    "\n",
    "# For each trajectory, plot connected points\n",
    "for trajectory in trajectories:\n",
    "    a_vals, b_vals = zip(*trajectory)\n",
    "    # Make the initial point black and the rest of the trajectory colored\n",
    "    if len(trajectory) > 0:\n",
    "        # Plot the first point in black\n",
    "        ax.plot(a_vals[0], b_vals[0], \"ko\", markersize=5, label=\"Initial point\")\n",
    "        # Plot the rest of the trajectory with the assigned color\n",
    "        ax.plot(\n",
    "            a_vals,\n",
    "            b_vals,\n",
    "            marker=\"o\",\n",
    "            markersize=3,\n",
    "            alpha=0.5,\n",
    "            color=colors[trajectories.index(trajectory)],\n",
    "        )\n",
    "\n",
    "# Set the limits\n",
    "ax.set_xlim(a_min, a_max)\n",
    "ax.set_ylim(b_min, b_max)\n",
    "\n",
    "# Add the gradient vector field\n",
    "X, Y, dX, dY = gradient_vector_field((a_min, a_max), (b_min, b_max), step=1.0)\n",
    "# Plot the gradient vector field\n",
    "ax.quiver(X, Y, dX, dY, color=\"black\", alpha=0.75, scale=5000)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(pcolormesh, ax=ax)\n",
    "cbar.set_label(r\"$f(a, b)$\")\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel(r\"$a$\")\n",
    "ax.set_ylabel(r\"$b$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tools"
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For real-world research and workflows, there are a number of existing differential programming libraries that offer sophisticated autodiff capabilities. Each library differs slightly in underlying design decisions which may or may not impact usefulness for your application. All options will offer well-documented, highly optimized algorithms that can leverage optional GPU acceleration within high-level abstractions. This lowers the barrier to entry and allows new users to write powerful programs very quickly. There is a dedicated notebook but I differentiate the same function we worked so hard to get right above using `Scalar` in two popular differential programming libraries, PyTorch and JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**PyTorch**"
   },
   "source": [
    "### **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "L = b * ((a + b) * (a - b))\n",
    "\n",
    "# Note that PyTorch will parse L into its own internal computation\n",
    "# graph representation, so we do not need to manually construct it\n",
    "# as we've been doing. In PyTorch, only the leaf nodes need to be\n",
    "# defined with requires_grad=True.\n",
    "\n",
    "# Perform backward pass\n",
    "L.backward()\n",
    "\n",
    "# Print out gradients of leaf nodes\n",
    "print(\"a.grad =\", a.grad)\n",
    "print(\"b.grad =\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**JAX**"
   },
   "source": [
    "### **JAX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda a, b: b * ((a + b) * (a - b))\n",
    "\n",
    "A = 1.0\n",
    "B = 2.0\n",
    "\n",
    "df_da = jax.grad(f, argnums=0)\n",
    "print(\"a.grad =\", df_da(A, B))\n",
    "\n",
    "df_db = jax.grad(f, argnums=1)\n",
    "print(\"b.grad =\", df_db(A, B))\n",
    "\n",
    "# Because JAX is a functional programming library, second derivatives are automatically supported\n",
    "d2f_da2 = jax.jacobian(df_da, argnums=0)\n",
    "d2f_db2 = jax.jacobian(df_db, argnums=1)\n",
    "print(\"a.second_grad =\", d2f_da2(A, B))\n",
    "print(\"b.second_grad =\", d2f_db2(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**TBD:**"
   },
   "source": [
    "# **TBD:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**Forward-mode**"
   },
   "source": [
    "## **Forward-mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**Reverse-on-forward**"
   },
   "source": [
    "## **Reverse-on-forward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**Higher_order_derivatives/Hessians**"
   },
   "source": [
    "## **Higher order derivatives/Hessians**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "**References**"
   },
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blogs**:\n",
    "\n",
    "- https://jingnanshi.com/blog/autodiff.html\n",
    "- https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation\n",
    "- https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/\n",
    "\n",
    "**Repos**:\n",
    "- https://github.com/karpathy/micrograd\n",
    "- https://github.com/rasmusbergpalm/nanograd\n",
    "- https://github.com/breandan/picograd\n",
    "- https://github.com/HIPS/autograd/tree/master\n",
    "\n",
    "**Textbook**:\n",
    "\n",
    "Griewank, Andreas, and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2nd ed. Philadelphia, PA: Society for Industrial and Applied Mathematics, 2008.\n",
    "\n",
    "**Papers**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Analytic_versus_numerical_differentiation",
    "Autodifferentiation",
    "Tracing_and_computation_graphs",
    "The_goal_of_autodiff",
    "Reverse-mode_autodiff",
    "Exercise:",
    "Exercise:",
    "Automated_backward-pass",
    "`__add__`",
    "An_aside_on_Python_closures",
    "Exercise",
    "Topological_ordering_and_the_fully_automating_the_backward_pass",
    "Exercise",
    "Custom_operations",
    "Exercise",
    "Minimization",
    "Tools",
    "**PyTorch**",
    "**JAX**",
    "**TBD:**",
    "**Forward-mode**",
    "**Reverse-on-forward**",
    "**Higher_order_derivatives/Hessians**",
    "**References**"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}