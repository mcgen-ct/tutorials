{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329e2fda",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we'll see what are Normalizing Flows exactly and play a bit with a standard implementation. Let's import what we need. We need to have [`pytorch`](https://pytorch.org/get-started/locally/) and [`nflows`](https://github.com/bayesiains/nflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python stuff\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stuff for torch+nflows\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "# optimizer from torch\n",
    "from torch import optim\n",
    "\n",
    "# base Flow to construct model\n",
    "from nflows.flows.base import Flow\n",
    "#base distribution to use\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "# the MADE coupling layer\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "# this adds a RandomPermutation to add variance between layers\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "# this will combine modules\n",
    "from nflows.transforms.base import CompositeTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81e96c",
   "metadata": {},
   "source": [
    "# Transformation rules of probability density functions\n",
    "\n",
    "The transformation rule of pdfs for a change of variable $x=f(z)$ is\n",
    "\n",
    "$p_{X}(x)=p_{Z}\\left(f^{-1}(x)\\right)|\\text{det} \\nabla_{x}f^{-1}\\left(x\\right)|$\n",
    "\n",
    "The determinant of the Jacobian can be rewritten in terms of $f$ for ease of computation as\n",
    "\n",
    "$p_{X}(x)=p_{Z}\\left(f^{-1}(x)\\right)|\\text{det} \\nabla_{x}f^{-1}\\left(x\\right)|=p_{Z}\\left(f^{-1}(x)\\right)|\\text{det} \\nabla_{z}f\\left(f^{-1}(x)\\right)|^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108224e",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "Show how you can transform a normally distributed $x\\sim \\mathcal{N}(0,1)$ to a normally distributed variable $x\\sim \\mathcal{N}(\\mu,\\sigma)$ through the change of variables $x = \\mu + \\sigma z$ by completing this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "### an example of this\n",
    "N = 10000\n",
    "pdf_z = st.norm(loc=0,scale=1)\n",
    "Z = pdf_z.rvs(N)\n",
    "plt.hist(Z,histtype='step',density=True)\n",
    "ztoplot = np.linspace(-5,5,100) # dummy variables for plotting the pdf\n",
    "pdf_vals_z = pdf_z.pdf(ztoplot) # evalute pdf for plotting\n",
    "plt.plot(ztoplot,pdf_vals_z)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7871051",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= 1.0 # arbitrary values\n",
    "sigma= 0.5 # arbitrary values\n",
    "X = mu+sigma*Z\n",
    "plt.hist(X,histtype='step',density=True)\n",
    "xtoplot = mu+sigma*ztoplot # dummy variables for plotting the pdf\n",
    "gradient_xtoplot_over_z = ... # compute the gradient\n",
    "pdf_vals_x = pdf_z.pdf(ztoplot) *  ... # evaluate the new pdf using the old pdf + jacobian\n",
    "plt.plot(xtoplot,pdf_vals_x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bf9d6",
   "metadata": {},
   "source": [
    "\n",
    "# Normalizing Flows\n",
    "\n",
    "At its essence, Normalizing Flows are bijective functions that map a sample space to a new space where data is distributed however we chose it to. That is, if we have data $x\\sim p_{X}$, we want to learn an invertible function $x = f(z,\\theta)$ such that $z$ follows an base distribution easy to sample from and to evaluate. The most common choice is a normal distribution $z \\sim p_{Z}\\equiv \\mathcal{N}(0,1)$. \n",
    "\n",
    "$f$ will be a learnable neural network with parameters $\\theta$ and an easy to compute gradient. The loss function which $\\theta$ needs to minimize is nothing more than the negative Log Likelihood obtained using the transformation rule of pdfs\n",
    "\n",
    "$\\mathcal{L}=- \\sum_{x\\in \\mathcal{D}}\\text{Ln }p_{X}(x) = - \\sum_{x\\in \\mathcal{D}}\\text{Ln }[p_{Z}(f^{-1}(x,\\theta))|\\text{det }\\nabla_{z}f|^{-1}]$\n",
    "\n",
    "$\\mathcal{L}= \\sum_{x\\in \\mathcal{D}}\\left(-\\text{Ln }[p_{Z}(f^{-1}(x,\\theta))]+\\text{Ln }[|\\text{det }\\nabla_{z}f|]\\right)$\n",
    "\n",
    "And assuming a standard normal distribution \n",
    "\n",
    "$\\mathcal{L}= \\sum_{x\\in \\mathcal{D}}\\left(-\\text{Ln }\\mathcal{N}\\left(f^{-1}(x,\\theta);0,1\\right)+\\text{Ln }[|\\text{det }\\nabla_{z}f|]\\right)$\n",
    "\n",
    "The trick is how to chose a learnable $f$ with easy gradient (which is not a problem using the gradient chain rule with standard NNs + backpropagation) but also easily invertable to go back and forth from $x$ to $z$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17473c08",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0235ff",
   "metadata": {},
   "source": [
    "In the previous, very simplified example, we know that a good choice of $f(z,\\theta)$ is simply $f(z,\\theta)=\\theta_{0}+\\theta_{1}z$ with inverse $f^{-1}(x,\\theta)=(x-\\theta_{0})/\\theta_{1}$ and jacobian $|\\text{det }\\nabla_{z}f|=|\\theta_{1}|$ (which does not depend on the evaluation on $z = (x-\\theta_{0})/\\theta_{1}$. We can thus simply write the loss function and do a very naive grid minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34563f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(theta0,theta1):\n",
    "    return np.sum(-st.norm(loc=0,scale=1).logpdf((X-theta0)/theta1)+np.log(theta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ba30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0vals = np.linspace(0.5,1.5,100) # substitute adequate range if you changed mu, sigma before\n",
    "theta1vals = np.linspace(0.3,0.7,100) # substitute adequate range if you changed mu, sigma before\n",
    "theta0vals_plot, theta1vals_plot = np.meshgrid(theta0vals,theta1vals)\n",
    "# print(theta0vals_plot.shape,theta1vals_plot.shape)\n",
    "loss_function_vals = np.zeros(theta0vals_plot.shape)\n",
    "for ntheta1val, theta1val in enumerate(theta1vals):\n",
    "    for ntheta0val, theta0val in enumerate(theta0vals):\n",
    "        loss_function_vals[ntheta1val,ntheta0val]=loss_function(theta0val,theta1val)\n",
    "plt.contourf(theta0vals,theta1vals,loss_function_vals,cmap='gist_heat_r')\n",
    "plt.axhline(sigma)\n",
    "plt.axvline(mu)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0min, theta1min = theta0vals_plot.flatten()[np.argmin(loss_function_vals)],theta1vals_plot.flatten()[np.argmin(loss_function_vals)]\n",
    "print(theta0min,theta1min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(mu,sigma),loss_function(theta0min,theta1min) # why? likely overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X,histtype='step',density=True)\n",
    "# plt.plot(xtoplot,pdf_vals_x)\n",
    "# now we use the min parameters explicitly with xtoplot\n",
    "pdf_vals_x_bis = st.norm(loc=0,scale=1).pdf((xtoplot-theta0min)/theta1min)/theta1min\n",
    "plt.plot(xtoplot,pdf_vals_x_bis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17bcb8",
   "metadata": {},
   "source": [
    "Note that we **haven't seen the true Z during training**. The technique is aimed at learning $p_{X}(x)$. We did cheat by knowing that the simple parameterization was good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a012e7",
   "metadata": {},
   "source": [
    "# Choice of f\n",
    "\n",
    "There are [many](https://arxiv.org/pdf/1908.09257.pdf) ways to do this, but the usual trick consists of concatenating several individual, simpler modules. That is\n",
    "\n",
    "$z_{1} = f_{1}(z)$\n",
    "\n",
    "$z_{i} = f_{i}(z_{i-1})$ with $i=2,...,n-1$\n",
    "\n",
    "$x=f_{n}(z_{n-1})$\n",
    "\n",
    "and having each individual $f_{i}$ module as a simple, invertible function whose parameters are Neural Networks. The choice of module aims to be easily invertible while allowing for as much expressivity as possible. That is, we want to be able to train while also capturing Jacobians as general as possible.\n",
    "\n",
    "Let's talk about one possible and very popular choice, **Autoregressive flows**. Other strategies can be found in the referred paper. As always, each choice has advantages and disadvantages.\n",
    "\n",
    "## [Masked Autoregressive flows](https://arxiv.org/pdf/1705.07057.pdf)\n",
    "\n",
    "For inputs with dimension $K$, we parameterise $z_{i,k}$ as a function of the previous dimensions in $z_{i-1}$:\n",
    "\n",
    "$z_{i,k}=f_{i}(z_{i-1,k},g_{i,k}(z_{i-1,1:k-1}))$\n",
    "\n",
    "That is, a function of the same input feature at the previous step $z_{i-1,k}$ and a **conditioner** g that takes the previous input and combines all the previous features $z_{i-1,1:k-1}$. When choosing a parametric, invertible form for $f$, $g_{k}$ is a map from $z_{i-1,1:k-1}$. In the previous example, because there are no previous feature dimensions as the problem was 1D, $g_{1}$ was simply $g_{1}=[\\theta_{0},\\theta_{1}]$.\n",
    "\n",
    "What is the advantage of this parameterization? That the Jacobian is triangular! One can show that\n",
    "\n",
    "$\\text{det }\\nabla f_{i} = \\prod_{k=1}^{K}\\frac{\\partial z_{i,k}}{\\partial z_{i-1,k}}$\n",
    "\n",
    "\n",
    "The forward flow ($z_{0}=z\\rightarrow z_{n}=x$) itself can be obtained in one sweep by \"masking\" the features appropriately (that's why it's called Masked). \"Masking\" for a Neural Network means setting some features to zero before feeding the input to said Neural Network. In this case, masking allows for the conditioner to be a single set of Neural Networks, one for each parameter of $f$, each of them a function $\\mathbb{R}^{K}\\to \\mathbb{R}$ that is evaluated for $x_{i,1:k-1}$ simply by masking or setting to zero the $k:K$ remaining entries.\n",
    "\n",
    "The inverse function and jacobian computation ($z_{0}=z\\leftarrow z_{n}=x$) is more challenging computationally speaking because we need to use recursion and compute each entry one step at a time with $z_{i-1,1}=f^{-1}_{i}(z_{i,1})$ and $z_{i-1,k}=f^{-1}_{i}(z_{i,1},g_{i,k}(z_{i-1,1:k-1}))$.\n",
    "\n",
    "If we are more interested in the inverse (for problems such as a Variational Inference), we can use **Inverse Autoregressive Flows** (IAF) where the transformation is instead:\n",
    "\n",
    "$z_{i,k}=f_{i}(z_{i-1,k},g_{i,k}(z_{i,1:k-1}))$\n",
    "\n",
    "(note the different index in the conditioner) and thus the recursion needs to be applied in the forward direction $z_{0}=z\\rightarrow z_{n}=x$.\n",
    "\n",
    "The rule of thumb is: **MAF** for fast density estimation and **IAF** for fast sampling. The difference in speed is not always an issue, and as usual with ML rules of thumb can be disregarded for simple enough problems...\n",
    "\n",
    "One can also use **Coupling flows** instead of **Autoregressive** ones. Coupling flows are equally fast in the forward and backward directions, so there is no difference between density estimation and sampling. However, they may be less flexible although very good performances can be obtained with Neural Spline Flows or Non Volume Preserving transformations. In the end, one should know what to play with and decide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b23578",
   "metadata": {},
   "source": [
    "## Coupling functions\n",
    "\n",
    "In any case, once we have defined we are using MAFs or IAFs, we need to define the coupling function $f_{i}$. This defines the parameters obtained using the conditioners $g_{i,k}$ which are Neural Networks.\n",
    "\n",
    "A very common couplign function is [MADE](https://arxiv.org/pdf/1502.03509.pdf) where the update is using a Gaussian kernel and the conditioner models the mean and variance of the Gaussian: \n",
    "\n",
    "$z_{i,k} = z_{i-1,k}\\text{exp }\\alpha_{i}(z_{i-1,1:k-1}) + \\mu_{i}(z_{i-1,1:k-1})$\n",
    "\n",
    "$\\alpha_{i}$ and $\\mu_{i}$ are Neural Networks, which are evaluated on the first $k-1$ features by masking the remaining $K-(k-1)$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f437a",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We'll use `nflows` to implement a MAF with MADE. There are many packages, with different implementations of different flows, so I recommend you always chose based on your problem. `nflows` is not perfect but it will suffice for the examples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape things for flows\n",
    "X=X.reshape(-1,1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base distribution.\n",
    "base_distribution = StandardNormal(shape=[X.shape[1]])\n",
    "\n",
    "# Define an invertible transformation.\n",
    "num_layers = 5\n",
    "\n",
    "transforms = []\n",
    "for _ in range(num_layers):\n",
    "    transforms.append(MaskedAffineAutoregressiveTransform(features=X.shape[1], \n",
    "                                                          hidden_features=4,num_blocks=2))\n",
    "    \n",
    "    transforms.append(RandomPermutation(features=1)) # useless for 1 feature\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "# Combine into a flow.\n",
    "\n",
    "flow = Flow(transform, base_distribution)\n",
    "optimizer = optim.Adam(flow.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382118b9",
   "metadata": {},
   "source": [
    "Before continuing, take some time to think about the relevant hyperparameters of the model. That is, what choices have I made. An \"obvious\" one is that for each initialized `MaskedAffineAutoregressiveTransform` there are two Neural Networks with `hidden_features` units per layer and `num_blocks` layers. You can look for more hyperparameters by going over the source code of the `nflows` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7385f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform inputs to torch tensors\n",
    "X_torch = torch.tensor(X, dtype=torch.float32)\n",
    "xtoplot_torch = torch.tensor(xtoplot.reshape(-1,1),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4e377",
   "metadata": {},
   "source": [
    "The `nflows` package allows for straightforward evaluation of the likelihood with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.log_prob(inputs=X_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba23bed",
   "metadata": {},
   "source": [
    "Let's see how things look before training the Flow. We can evaluate the likelihood using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f686b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_vals_x = np.exp(flow.log_prob(xtoplot_torch).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac84a0",
   "metadata": {},
   "source": [
    "And we can sample using the learned likelihood using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa17237",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = flow.sample(len(X)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13936a",
   "metadata": {},
   "source": [
    "We can see how initially everything is random and thus a poor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268102ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = plt.hist(X,histtype='step',density=True,label='Truth',color='black')\n",
    "plt.hist(x_sample,histtype='step',density=True,label='Flow',color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.plot(xtoplot,pdf_vals_x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d26be",
   "metadata": {},
   "source": [
    "Now, let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of epochs\n",
    "num_iter = 1000\n",
    "\n",
    "for i in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    # the loss is simply - E[Log Prob] !\n",
    "    loss = -flow.log_prob(inputs=X_torch).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6cc09",
   "metadata": {},
   "source": [
    "And now we can re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_vals_x = np.exp(flow.log_prob(xtoplot_torch).detach().numpy())\n",
    "x_sample = flow.sample(len(X)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c90886",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = plt.hist(X,histtype='step',density=True,label='Truth',color='black')\n",
    "plt.hist(x_sample,histtype='step',bins=b,density=True,label='Flow',color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.plot(xtoplot,pdf_vals_x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e3dec",
   "metadata": {},
   "source": [
    "Much better! \n",
    "\n",
    "You can now play with dimensions or devise some tests to quantify the agreement (like two sample tests, because this simple example is one-dimensional) and define interesting problems. As for us, we'll go to one possible application, Anomaly detection, in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca991943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
