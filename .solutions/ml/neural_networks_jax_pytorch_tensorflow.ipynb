{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Introduction_to_Neural_Networks"
   },
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Rikab Gambhir (Center for Theoretical Physics, MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore Neural Networks, the fundamental building block of deep learning. We will go into the very basics of the theory of Neural Networks and Universal Function Approximation. Then, we will explore practical immplementations of Neural Networks and deep learning that are widely used both in physics applications and also are widespread in industry.\n",
    "\n",
    "This tutorial is divided into 4 parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Neural Network Basics**: Constructing multi-layer perceptrons and studying universal function approximation.\n",
    "2. **JAX**: An increasingly popular library used for machine learning. This library is extremeley similar to basic numpy, but has extra features like autodifferentiation and compilation that make it useful for machine learning.\n",
    "3. **PyTorch**: A commonly used ML library. Developed by Meta. Especially nice for implementing fancy modern ML models, since they're mostly developed in PyTorch anyways!\n",
    "4. **Tensorflow**: Less common in 2025, but many ML tools still use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**\n",
    "\n",
    "I will assume knowledge of the following:\n",
    "1. Basic python and numpy. You should be comfortable with matrix operations within numpy, dealing with lists and loops, defining functions, and classes.\n",
    "2. You are familiar with the previous tutorials on regression, classification, normalizing flows, and unsupervised learning. In particular you should appreciate the idea of finding parameters that minimize the log-likelhood (or other metrics) for function fitting, and the general importance of finding/optimizing for functions for statistical tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_1:_Neural_Network_Basics"
   },
   "source": [
    "# Chapter 1: Neural Network Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials, e.g. [regression](https://colab.research.google.com/github/mcgen-ct/tutorials/blob/main/ml/regression.ipynb), the goal was to model a fixed functional form $f(x)$ where $f$ depended on some parameters $\\theta$. For example, a linear fit of the form $f(x) = \\theta_0 + \\theta_1 x$.\n",
    "\n",
    "In Deep Learning, we want to be more ambitious. We do not want to assume a specific functional form: rather than only ``searching'' over a fixed set of basis functions, we want to search over *all* functions, or at least a very large class of functions. Our strategy for doing this is to take a functional form with an extremeley large set of parameters, such that in the infinite parameter limit all functions of a particular class fit within the parameterization. For example. the set of functions:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{i=0}^N \\theta_i x^i\n",
    "\\end{align}\n",
    "\n",
    "models all one-dimension analytic functions as $N \\to \\infty$. However, we would like a more general parameterization that can work for many dimensions and even model non-smooth (or even non-continuous) functions arbtirarily well.\n",
    "\n",
    "A **Neural Network (NN)**  (also known as a **Multilayer Perceptron (MLP)** a **feedforward network**, or a **Dense Neural Network (DNN)** depending on the context) parameterizes *all* peicewise-continuous functions from $\\mathbb{R}^{n} \\to \\mathbb{R}^m$ arbitrarily well with a very simple parameterization.\n",
    "\n",
    "\n",
    "To define a neural network, we first specify $L-2$ integers $N_1, ..., N_{L-1}$. Just for notation, choose $N_0 = n$ as the input dimension, and $N_L = m$ as the output dimension. $L$ is referred to as the *depth* of the network (or number of layers), and the $N$'s are the *width* of each layer. Unless you are doing something fancy (e.g. autoencoders), it is typical to choose $N$ to all be the same.\n",
    "\n",
    "Then, we define a set of *layer functions*, which are maps $f^{\\ell}:\\mathbb{R}^{N_{\\ell-1}}\\to\\mathbb{R}^{N_{\\ell}}$, as:\n",
    "\n",
    "$$ f^{\\ell}(x) = \\sigma(W^{(\\ell)}x + b^{(\\ell)})$$\n",
    "\n",
    "where $W^{(\\ell)} \\in \\mathbb{R}^{N_{\\ell} \\times N_{\\ell -1}}$ and $b^{(\\ell)} \\in \\mathbb{R}^{N_{\\ell}}$ are the parameters that define the layer, and $\\sigma$ is some pre-determined nonlinear transformation. This can differ between layers, but it is common to chose $\\sigma$ to be the same for every layer except the last, where $\\sigma$ is often instead chosen such that its image matches the desired output space. An extremeley common and simple chose for $\\sigma$ is the ReLU (Rectified Linear Unit) function, which we will use throuhout the rest of this tutorial:\n",
    "\n",
    "$$ \\sigma(x) = \\max(0, x)$$\n",
    "\n",
    "Then, the full neural network is defined by:\n",
    "\n",
    "$$ f = f^{L} \\cdot f^{L-1} \\cdot ... \\cdot f^{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an MLP from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Neural Network from Scratch #\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "L = 3\n",
    "N = 16  # We will use the same N throughout for simplicity\n",
    "\n",
    "\n",
    "# Function to initialize the W's and b's\n",
    "# For now, lets just pick random numbers!\n",
    "def init_params(input_dim, output_dim, L, N):\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            W = np.random.randn(N, input_dim) / np.sqrt(input_dim)\n",
    "            b = np.random.randn(N) / np.sqrt(input_dim)\n",
    "            # The sqrt(input_dim) normalization is not important for our toy examples, but it is common to do for stability reasons\n",
    "\n",
    "        elif l == L - 1:\n",
    "            W = np.random.randn(output_dim, N) / np.sqrt(N)\n",
    "            b = np.random.randn(output_dim) / np.sqrt(N)\n",
    "\n",
    "        else:\n",
    "            W = np.random.randn(N, N) / np.sqrt(N)\n",
    "            b = np.random.randn(N) / np.sqrt(N)\n",
    "\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "\n",
    "    return Ws, bs\n",
    "\n",
    "\n",
    "# Implement the ReLU function\n",
    "def sigma(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a neural network given x, the weights W, and the biases b\n",
    "\n",
    "\n",
    "def MLP(x, Ws, bs):\n",
    "    y = x.copy()\n",
    "\n",
    "    for l in range(L):\n",
    "        # Fun python fact: \"@\" implements matrix multiplication!\n",
    "        y = Ws[l] @ y + bs[l]\n",
    "\n",
    "        # Don't apply sigma to the final output so that our answer isn't forced positive\n",
    "        if l != L - 1:\n",
    "            y = sigma(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our MLP function by graphing the function f:R2 -> R1\n",
    "\n",
    "# Define some test points in R2\n",
    "xs1 = np.linspace(-1, 1, 100)\n",
    "xs2 = np.linspace(-1, 1, 100)\n",
    "\n",
    "xs1, xs2 = np.meshgrid(xs1, xs2)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "Ws, bs = init_params(input_dim, output_dim, L, N)\n",
    "\n",
    "ys = []\n",
    "for x in zip(xs1.flatten(), xs2.flatten()):\n",
    "    x = np.array(x)\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "\n",
    "ys = np.array(ys)\n",
    "ys = ys.reshape(xs1.shape)\n",
    "\n",
    "# 3d plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(xs1, xs2, ys)\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"y = MLP(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on functional vs. object-oriented programming**\n",
    "\n",
    "In the above code, we defined our MLP purely using python functions. There is no neural network \"object\" with an internal state keeping track of the parameters. Instead, the parameters $W$ and $b$ are also treated as inputs to functions. This is *functional programming*, in which there are no objects with internal states that get modified. This is the approach to ML used by JAX.\n",
    "\n",
    "It is also possible to define an MLP *class*, which is an object that contains the parameters as internal states that can potentially be modified, and methods that implement the model and evaluate $f(x)$. This is the approach to ML used by PyTorch and Tensorflow.\n",
    "\n",
    "It is largely a matter of programming taste which you prefer. Below, we will see a brief example of the above code, but written in an object-oriented style rather than functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MLP_Class:\n",
    "    def __init__(self, input_dim, output_dim, L, N):\n",
    "        # Initialize the network arguments\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.L = L\n",
    "        self.N = N\n",
    "\n",
    "        # Initialize the network internal state using the same initi function\n",
    "        self.Ws, self.bs = init_params(input_dim, output_dim, L, N)\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        # Just use the same exact function as above\n",
    "        return MLP(x, self.Ws, self.bs)\n",
    "\n",
    "    # \"Magic Method\" that lets us call the class as if it were a function (just syntatic magic)\n",
    "    def __call__(self, x):\n",
    "        return self.evaluate(x)\n",
    "\n",
    "\n",
    "my_MLP = My_MLP_Class(input_dim, output_dim, L, N)\n",
    "\n",
    "# Access the weights\n",
    "my_weights = my_MLP.Ws\n",
    "print(\"The number of layers is \", len(my_weights), \",Expected 3\")\n",
    "\n",
    "# Evaluate the function\n",
    "print(\"f(1,1) = \", my_MLP(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historical Notes and Semantics**\n",
    "\n",
    "The case where $L = 2$ (no ``hidden layers'' between the input and output) with the output dimensionality is $1$ is called a perceptron historically. These were introduced with $\\sigma$ not as ReLU, but rather:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "(the sigmoid function, hence the notation), and were used back in the day as a model of a biological neuron. The neuron \"activates\" (produces $1$) when $x$ is large, and \"deactivates\" (produces $0$ when $x$ is small, where $b$ is then a bias. For this reason, $\\sigma$ is called an activation function. This is also why our models are called \"Neural Networks\". The \"network\" is because the parameters of the weight matrix $w_{ij}$ are drawn as lines connecting a node $i$ in the previous layer to a node $j$ in the next layer. It's important to remember though, that these are just affine transformations interleaved by some simple nonlinear functions, and there isn't really anything magic here, just slightly-nonlinear algebra.\n",
    "\n",
    "The name \"feedforward\" network just refers to the function-compositional aspect of the model. It is to be contrasted with a \"backwards pass\", where derivatives with respect to the network are actually computed in reverse-order due to chain-rule simplifications. The name \"dense\" neural network is to emphasize that this is the simplest possible network one can build. There are many modern models with additional properties (such as gauranteeing symmetries, or working in spaces other than simple vector spaces, or deliberately constraining the function space), but many of these can be reduced to very large MLPs with constrained weights. When we say \"dense\" or \"fully-conencted\" MLPs, we typically mean there are no constraints on the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_1.1:_Universal_Function_Approximation"
   },
   "source": [
    "## Chapter 1.1: Universal Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of MLPs is that they are an efficient way to parameterize a large class of functions. This is captured by the **Universal Function Approximation Theorem(s) (UFAT)** (there are lots of variants, but at the level of rigor we are working at, we won't worry about this).\n",
    "\n",
    "**Emotionally**, the UFAT tells us that for sufficiently large $N$ and $L$, an MLP can approximate any (reasonable) $n$-to-$m$ dimensional function arbitrarily well.\n",
    "\n",
    "**Slightly more precisely**, a version of UFAT says: For any piecewise-continuous function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ defined on a compact domain $D \\subset \\mathbb{R}^n$, and for any \"error tolerance\" $\\epsilon  > 0$, there exists a large enough $N$ and $L$ such that one can define an MLP with specially-chosen parameters $W$ and $b$ such that:\n",
    "$$ \\int_D dx |f(x) - MLP(x)| < \\epsilon $$\n",
    "i.e. that we have approximated the function to within the specified error.\n",
    "\n",
    "[Side note: It is actually always possible to do this with just $L$ = 3 (meaning just one hidden layer with chosen $N$ in our defined $L$ counting), but typically this requires an exponentially large $N$ and isn't of practical use for what we will be doing].\n",
    "\n",
    "\n",
    "We will not prove the UFAT. However, we will explore a weaker-version of it that is easier to understand: If instead we explore continuous-and-piecewise-once-differentiable functions rather than just piecewise-continuous, then there is an easy construction using ReLU networks. If a function is piecewise-once-differentiable, then it can be well-approximated by a piecewise-linear function. We will see below (as exercises) how ReLU networks can exactly reproduce piecewise linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Modeling_|x|"
   },
   "source": [
    "#### Exercise: Modeling |x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x) = |x|$ in 1 dimension, design an MLP with a choice of N, L, weights W, and biases b that *exactly* match $f(x)$.\n",
    "\n",
    "HINT: It is possible to do this with $L = 2$ (one hidden layer) and $N = 2$.\n",
    "\n",
    "HINT 2: It is possible to do this with $b = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.abs(x)\n",
    "\n",
    "\n",
    "L = 2\n",
    "N = 2\n",
    "\n",
    "W0 = np.array([[1.0], [-1.0]])  # hidden unit 1:  +x  # hidden unit 2:  \u2013x\n",
    "b0 = np.array([0.0, 0.0])  # no shift\n",
    "\n",
    "W1 = np.array([[1.0, 1.0]])  # add the two ReLU outputs\n",
    "b1 = np.array(\n",
    "    [\n",
    "        0.0,\n",
    "    ]\n",
    ")  # no shift\n",
    "\n",
    "Ws = [W0, W1]\n",
    "bs = [b0, b1]\n",
    "\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "\n",
    "# Evaluate the solution\n",
    "ys = []\n",
    "for x in xs:\n",
    "    x = np.array([x])\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "\n",
    "ys = np.array(ys)\n",
    "\n",
    "# Plot\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, ys, label=\"MLP(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Approximating_a_smooth_1D_function."
   },
   "source": [
    "### Exercise: Approximating a smooth 1D function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x) = \\sin(10x) \\exp(-2x^2)$ on the interval $[-1,1]$, design an MLP with ReLU-activations that approximates the function to within an error of $\\epsilon < 0.01$ (where error is the mean-absolute error, as defined above). As a bonus, your implementation should be systematically improvable, e.g. it should be straightforward to make the MLP bigger to reduce the error further. Don't cheat and use minimization to get the parameters, explicitly construct them!\n",
    "\n",
    "\n",
    "HINT: First construct a continuous piecewise linear appoximation to the function, then implement this piecewise linear function as an MLP. It is possible to do this without knowledge of the actual form of $f$.\n",
    "\n",
    "HINT 2: This is possible to do systematically with $L = 2$ as before, but with a very large $N$. My personal solution requires $N$ between 100 and 150.\n",
    "\n",
    "HINT 3: A piecewise-linear continuous function can be written as $f(x) = c_0 + m_0x + + \\sum_{j = 1}^{n-1}(m_j - m_{j-1})\\sigma(x - x_j)$, where $\\sigma$ is ReLU, $x_{1}...x_{n-1}$ are the internal breakpoints, $m_j$ are the slopes to the right of each breakpoint, and $c_0$ is the $y$-coordinate at the leftmost point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any function f, define a piecewise-linear approximation with n breakpoints.\n",
    "def piecewise_linear_params(n, f):\n",
    "    # Breakpoints of the piecewise linear approximation\n",
    "    xk = np.linspace(-1.0, 1.0, n + 1)  # x0, \u2026, xn\n",
    "    yk = f(xk)\n",
    "\n",
    "    # Approximate the slopes numerically (technically, possible exactly)\n",
    "    mk = np.diff(yk) / np.diff(xk)  # m0 \u2026 m_{n-1}\n",
    "\n",
    "    # slope jumps delta_m_j at each breakpoint\n",
    "    dm = mk[1:] - mk[:-1]\n",
    "\n",
    "    # constant term that glues the first segment to y(-1)\n",
    "    c0 = yk[0] - mk[0] * xk[0]\n",
    "\n",
    "    # Initialize weights\n",
    "    N = n + 1  # annoying indexing\n",
    "    W0 = np.ones((N, 1))\n",
    "    b0 = np.zeros(N)\n",
    "\n",
    "    # Add and subtract a ReLU, like the |x| example\n",
    "    W0[0, 0] = 1.0\n",
    "    b0[0] = 0.0\n",
    "    W0[1, 0] = -1.0\n",
    "    b0[1] = 0.0\n",
    "\n",
    "    # interior break-points\n",
    "    for j, t_j in enumerate(xk[1:-1], start=2):  # j = 2 \u2026 n\n",
    "        W0[j, 0] = 1.0\n",
    "        b0[j] = -t_j  # shift to next break point\n",
    "\n",
    "    # Second layer where everything is just 1 and -1 to add and subtract\n",
    "    W1 = np.zeros((1, N))\n",
    "    W1[0, 0] = mk[0]  #  +m0\u00b7ReLU(x)\n",
    "    W1[0, 1] = -mk[0]  #  \u2212m0\u00b7ReLU(\u2212x)\n",
    "\n",
    "    for j, d_m in enumerate(dm, start=2):\n",
    "        W1[0, j] = d_m\n",
    "\n",
    "    b1 = np.array([c0])  # constant offset for leftmost point\n",
    "\n",
    "    Ws = [W0, W1]\n",
    "    bs = [b0, b1]\n",
    "    return Ws, bs\n",
    "\n",
    "\n",
    "num_breakpoints = 100  # Increase for more accuracy!\n",
    "Ws, bs = piecewise_linear_params(num_breakpoints, f)\n",
    "xs = np.linspace(-1, 1, 1000)\n",
    "\n",
    "ys = []\n",
    "for x in xs:\n",
    "    x = np.array([x])\n",
    "    ys.append(MLP(x, Ws, bs))\n",
    "ys = np.array(ys)\n",
    "\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, ys, label=\"MLP(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Mean absolute error: \", np.mean(np.abs(ys[:, 0] - f(xs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "END_SOLUTION"
   },
   "source": [
    "###END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_1.2:_Functional_Optimization"
   },
   "source": [
    "## Chapter 1.2: Functional Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the ability to approximate function spaces with MLP's! The fun part of Machine Learning (the Learning) comes in when we can phrase problems as *functional optimization* problems:\n",
    "\n",
    "\"Out of all the (reasonably nice) functions from $\\mathbb{R}^n \\to \\mathbb{R}^m$ , which function $f$ minimizes the loss functional $L[f]$?\"\n",
    "\n",
    "Almost every interesting problem in life, statistics, and physics can be phrased this way. In fact, this is completely identical to Lagrangian mechanics, in the case that $L[f]$ can be written as the integral of a local Lagrangian. In simple cases (ordinary classical mechanics) this functional optimization can be performed using the Euler-Lagrange equations. But in many cases (e.g. where $L[f]$ is written as a sum rather than an integral so EL does not apply, or we can't solve the EL equations, etc), we must settle for numerics.\n",
    "\n",
    "You have seen in previous tutorials how many statistics problems (e.g. regression, classification, and density estimation) can be seen as functional optimization. In those examples, there were only a few parameters defining the function space: now there are *many* parameters and our function space is as close to the space of all possible functions as possible. We can no longer just use a simple parameter minimizer in this case.\n",
    "\n",
    "The strategy will be **gradient descent**. If we have an estimate of:\n",
    "$$\\nabla_{\\theta}L[f]$$\n",
    "then by simply moving $\\theta$ in the opposite direction of the gradient, we will move towards a local minimum. The process of iterating this is called **training**, and each iteration is called an **epoch**. In statistical settings, where $L[f]$ is some statistical measure (like in the regression examples), this training requires data to obtain statistical estimates of $\\nabla_{\\theta}L[f]$, hence the need for **training data**. There are many variants of gradient descent that work on the same principle but have varying numerical properties, like stochastic gradient descent and ADAM, but we will not dive deeper into these here.\n",
    "\n",
    "In principle, if we know $L$ (which we usually do, because it is typically part of the problem specification), we can explictly construct $\\nabla_{\\theta}L[f]$ exactly, since we know how $f$ depends on our parameters $\\theta = (W, b)$. However, it is still painful to manually construct. This is where libraries like **JAX**, **PyTorch**, and **Tensorflow** come in. These libraries are capable of **autodifferentiation**: computations are kept track of in a graph structure, so that gradients can be easily and exactly computed alongside the execution of the function. Exploring this further will be the subject of another tutorial, for now we wil take it for granted that these libraries can perform autodifferentiation.\n",
    "\n",
    "We have reached the limit of what we can practically do without the use of libraries in a reasonable amount of time. Now, we will explore how to use these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Interlude:_The_problems_we_will_solve:"
   },
   "source": [
    "# Interlude: The problems we will solve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be interested in using Neural Networks to solve classification problems. We have previously seen how to do this with logistic regression and cross-entropies in the [classification tutorial](https://colab.research.google.com/github/mcgen-ct/tutorials/blob/main/ml/classification.ipynb). We will now see how to do this with Neural Networks.\n",
    "\n",
    "We will have two problems: an easy problem, and a hard problem. The easy problem is \"Two Moons\", a classic ML test case in 2 dimensions. The hard problem is the \"MNIST\" dataset, a dataset of handwritten digits that is commonly used to test classification algorithms. This is a 28x28 pixel image dataset, so the input dimension is 784, so MLPs can really shine compared to classical fixed-form regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Moons Dataset\n",
    "\n",
    "X_moons, y_moons = make_moons(1024, noise=0.15, random_state=0)\n",
    "X_moons = StandardScaler().fit_transform(X_moons)  # Just normalizing the data\n",
    "\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], \"ro\", label=\"Class 0\")\n",
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], \"bo\", label=\"Class 1\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X_mnist = mnist.data / 255.0  # Normalize pixel values to [0, 1]\n",
    "y_mnist = mnist.target.astype(int)  # Convert target to integers\n",
    "\n",
    "# The MNIST input dimension is 784, but we can visualize it as 28x28 images\n",
    "plt.imshow(X_mnist[y_mnist == 7][0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Example MNIST Digit (7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2:_JAX"
   },
   "source": [
    "# Chapter 2: JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is a library developed by Google that is very similar to numpy, but with some extra features that make it useful for machine learning. It is based on the idea of functional programming, where functions are first-class citizens and can be passed around like any other object. JAX is particularly useful for machine learning because it has built-in support for autodifferentiation, which allows us to compute gradients of functions with respect to their inputs.\n",
    "\n",
    "Compared to PyTorch and Tensorflow, JAX is more low-level and requires more manual work to set up. However, it is also more flexible and allows for more control over computations. JAX is particularly useful for research and experimentation, where you want to try out new ideas quickly without having to worry about the details of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.1:_JAX_basics;_vmapping,_autodifferentiation,_and_compilation."
   },
   "source": [
    "## Chapter 2.1: JAX basics; vmapping, autodifferentiation, and compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has three useful features that we should be aquainted with:\n",
    "\n",
    "\n",
    "1. Vmapping: We can write a function acting on a single variable, and then execute that function on an entire list at once without using loops. In fact, this is much faster than looping (in Python), since Python loops must wait for the previous iteration to finish. Note that numpy can technically do this too, but it becomes especially important in JAX\n",
    "2. Just-In-Time compilation (JIT): Python is a scripted language, meaning lines of code are carried out as your computer sees them. In compiled languages, the computer looks at the entire program, translates to machine code (compilation), then executes. You pay an up-front time cost for the initial compilation, but every subsequent execution is much faster since the machine code is typically highly optimized. JIT allows us to pre-compile functions in Python. The cost is that we have to be a little be conscious of things like memory, and we cannot use things like ordinary if-statements or for-loops.\n",
    "3. Autodifferentiation: If we write a function in JAX, we can automatically compute its exact derivative. We don't have to manually compute it ourselves! This even works with multi-variate functions, functions that are highly-composed and require lots of chain-ruling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "def theta(x):\n",
    "    return x > 0\n",
    "\n",
    "\n",
    "def f(x, a):\n",
    "    return a * (1 - jnp.cos(x)) * theta(x)\n",
    "\n",
    "\n",
    "a = 3.0\n",
    "xs = jnp.linspace(-2 * jnp.pi, 2 * jnp.pi, 10000)\n",
    "\n",
    "# Vmapping time save test:\n",
    "start_time = time.time()\n",
    "ys = []\n",
    "for x in xs:\n",
    "    ys.append(f(x, a))\n",
    "ys = jnp.array(ys)\n",
    "end_time = time.time()\n",
    "print(\"Loop time: \", end_time - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# vmap(f) is a new function with the same signature as f.\n",
    "# vmap(f, in_axes = (0, None)) means we only want to vectorize over the first argument (x), not the second.\n",
    "\n",
    "ys = vmap(f, in_axes=(0, None))(xs, a)\n",
    "end_time = time.time()\n",
    "print(\"Vmap time: \", end_time - start_time)\n",
    "\n",
    "\n",
    "# Compile the function\n",
    "start_time = time.time()\n",
    "f_jit = jit(f)\n",
    "f_jit(\n",
    "    0, a\n",
    ").block_until_ready()  # Need to run the compiled function once to compile \"just in time\"\n",
    "end_time = time.time()\n",
    "print(\"Compilation time: \", end_time - start_time)\n",
    "# Note that compilation is NOT always faster, especially for only simple functions.\n",
    "# Also machine-dependent!\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "ys = []\n",
    "for x in xs:\n",
    "    ys.append(f_jit(x, a))\n",
    "ys = jnp.array(ys)\n",
    "end_time = time.time()\n",
    "print(\"JIT loop time: \", end_time - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "ys = vmap(f_jit, in_axes=(0, None))(xs, a)\n",
    "end_time = time.time()\n",
    "print(\"JIT vmap time: \", end_time - start_time)\n",
    "\n",
    "\n",
    "# Get the exact gradient with respect to x\n",
    "f_prime = jax.grad(f, argnums=0)  # Argnums is the argument we want the gradient of.\n",
    "f_prime_jit = jit(f_prime)\n",
    "f_prime_jit(\n",
    "    0.0, a\n",
    ").block_until_ready()  # Need to run the compiled function once to compile \"just in time\"\n",
    "\n",
    "# Get the exact gradient with respect to a\n",
    "f_prime_a = jax.grad(f, argnums=1)  # Argnums is the argument we want the gradient of.\n",
    "f_prime_a_jit = jit(f_prime_a)\n",
    "\n",
    "\n",
    "plt.plot(xs, vmap(f_jit, in_axes=(0, None))(xs, a), label=\"f(x)\")\n",
    "plt.plot(xs, vmap(f_prime_jit, in_axes=(0, None))(xs, a), label=\"d_x f(x)\")\n",
    "plt.plot(xs, vmap(f_prime_a_jit, in_axes=(0, None))(xs, a), label=\"d_a f(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:_Autodifferentiation_Practice"
   },
   "source": [
    "### Exercise: Autodifferentiation Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an *arbitrary* scalar-valued function $f(x)$, which you know is smooth, write a function that computes the Taylor expansion of $f$ around a point $x_0$ to order $n$.\n",
    "\n",
    "Specifically, your function should take as input $f$, $x_0$, and $n$, and return a new function, $f_n$, which is the Taylor expansion of $f$ around $x_0$ to order $n$. The function $f_n$ should take as input a single variable $x$, and return the value of the Taylor expansion at that point.\n",
    "\n",
    "HINT: Construct a list of functions $f_0, f_1, ..., f_n$ where $f_i$ is the $i$-th derivative of $f$, or equivalently, $f_i$ is the single derivative of $f_{i-1}$. Then evaluate this list of functions at $x_0$ to obtain the coefficients of the Taylor expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # EXAMPLE FUNCTION\n",
    "    return jnp.sin(x)\n",
    "\n",
    "\n",
    "def build_taylor_series(f, x0, n):\n",
    "    # Compute the derivatives at x0\n",
    "    derivative_functions = [f]\n",
    "    derivatives_at_x0 = [f(x0)]\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        derivative_functions.append(grad(derivative_functions[-1], argnums=0))\n",
    "        derivatives_at_x0.append(derivative_functions[-1](x0))\n",
    "\n",
    "    # Define the Taylor series expansion\n",
    "    def taylor_series(x):\n",
    "        series = 0.0\n",
    "        for i in range(n + 1):\n",
    "            series += derivatives_at_x0[i] * (x - x0) ** i / np.math.factorial(i)\n",
    "        return series\n",
    "\n",
    "    return taylor_series\n",
    "\n",
    "\n",
    "# Test the solution\n",
    "x0 = 0.0\n",
    "n = 5\n",
    "xs = jnp.linspace(-2 * jnp.pi, 2 * jnp.pi, 10000)\n",
    "plt.plot(xs, f(xs), label=r\"f(x)\", color=\"black\", lw=2)\n",
    "\n",
    "for i in range(n + 1):\n",
    "    taylor_series = build_taylor_series(f, x0, i)\n",
    "    plt.plot(xs, vmap(taylor_series, in_axes=0)(xs), label=\"f_{}(x)\".format(i))\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.2:_End-to-End_MLP_and_Training_from_Scratch"
   },
   "source": [
    "## Chapter 2.2: End-to-End MLP and Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reproduce the MLP we defined above, but now using JAX. We will use the functional programming style, so we will define our MLP as a function that takes in the parameters $W$ and $b$ as inputs.\n",
    "\n",
    "A small difference is that rather than having W and b as separate inputs, we will combine them into a single input called `params`. This is just for convenience to make taking gradients cleaner. Also, since we are interested in classification, we will use a sigmoid activation for the last layer in the binary classification case to ensure the output is between 0 and 1, which is useful for classification tasks, and use a softmax for the multiclass case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX MLP. Basically identical to the above numpy code.\n",
    "def MLP_jax(x, params):\n",
    "    y = x\n",
    "    Ws, bs = params\n",
    "\n",
    "    for l in range(len(Ws) - 1):\n",
    "        y = jnp.dot(Ws[l], y) + bs[l]\n",
    "        y = relu(y)\n",
    "\n",
    "    y = jnp.dot(Ws[-1], y) + bs[-1]  # No activation on the last layer\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# Our classifier will be an MLP with a sigmoid at the end. Change to softmax for multi-class classification.\n",
    "def classifier(x, params):\n",
    "    # Its common to call the input to the sigmoid the \"logits\". But really its the log-likelihood ratio of the classes.\n",
    "    logits = MLP_jax(x, params)\n",
    "    return 1 / (1 + jnp.exp(-logits))\n",
    "\n",
    "\n",
    "# Gradient with respect to the parameters\n",
    "classifier_grad = grad(classifier, argnums=1)\n",
    "\n",
    "\n",
    "# Initialize the parameters for the MLP\n",
    "def init_params_jax(input_dim, output_dim, L, N):\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            # Basically the same as the numpy version, but using JAX's random module.\n",
    "            # Unlike numpy, JAX's random module is functional and requires a PRNG key.\n",
    "            W = random.normal(random.PRNGKey(l), (N, input_dim)) / jnp.sqrt(input_dim)\n",
    "            b = random.normal(random.PRNGKey(l + 1), (N,)) / jnp.sqrt(input_dim)\n",
    "        elif l == L - 1:\n",
    "            W = random.normal(random.PRNGKey(l), (output_dim, N)) / jnp.sqrt(N)\n",
    "            b = random.normal(random.PRNGKey(l + 1), (output_dim,))\n",
    "        else:\n",
    "            W = random.normal(random.PRNGKey(l), (N, N)) / jnp.sqrt(N)\n",
    "            b = random.normal(random.PRNGKey(l + 1), (N,))\n",
    "\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "\n",
    "    return Ws, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test just to make sure it works\n",
    "\n",
    "N = 16\n",
    "L = 3\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "xs1 = jnp.linspace(-1, 1, 100)\n",
    "xs2 = jnp.linspace(-1, 1, 100)\n",
    "\n",
    "xs1, xs2 = jnp.meshgrid(xs1, xs2)\n",
    "xs = jnp.array(list(zip(xs1.flatten(), xs2.flatten())))\n",
    "print(\"Shape of xs: \", xs.shape)  # Should be (10000, 2)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "Ws, bs = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "# VMAP our model. We can also JIT it, but its actually better to JIT the entire training step function later.\n",
    "vmapped_classifier = vmap(classifier, in_axes=(0, None))\n",
    "vmapped_classifier_grads = vmap(classifier_grad, in_axes=(0, None))\n",
    "\n",
    "ys = vmapped_classifier(xs, (Ws, bs))\n",
    "ys = ys.reshape(xs1.shape)\n",
    "\n",
    "# 3d plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(xs1, xs2, ys)\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"y = MLP(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the training. We want to minimize the cross-entropy loss function, which is defined as:\n",
    "$$ L[f] = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(f(x_i)) + (1 - y_i) \\log(1 - f(x_i)) $$\n",
    "\n",
    "where $y_i$ is the true label for the $i$-th data point, and $f(x_i)$ is the model score for the $i$-th data point.\n",
    "\n",
    "We will use JAX's autodifferentiation to compute the gradient of the loss function with respect to the parameters, and then use gradient descent to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BCE loss\n",
    "def bce_loss(x, y_true, params):\n",
    "    y_pred = vmapped_classifier(x, params).squeeze()\n",
    "    epsilon = 1e-10  # Small value to avoid log(0), just in case\n",
    "    return -jnp.mean(\n",
    "        y_true * jnp.log(y_pred + epsilon)\n",
    "        + (1 - y_true) * jnp.log(1 - y_pred + epsilon)\n",
    "    )\n",
    "\n",
    "\n",
    "grad_bce_loss = grad(bce_loss, argnums=2)  # Gradient with respect to the parameters\n",
    "\n",
    "\n",
    "# Gradient step function to update the parameters\n",
    "@jit  # <- Inline way to JIT the function\n",
    "def gradient_step(x, y_true, params, learning_rate=0.01):\n",
    "    # Compute the loss gradients (the loss itself is not needed, just the gradients)\n",
    "    loss = bce_loss(x, y_true, params)  # But we will compute it anyways for logging\n",
    "    grads = grad_bce_loss(x, y_true, params)\n",
    "\n",
    "    # Update the parameters using gradient descent\n",
    "    Ws, bs = params\n",
    "    dWs, dbs = grads\n",
    "    new_Ws = [W - learning_rate * dW for W, dW in zip(Ws, dWs)]\n",
    "    new_bs = [b - learning_rate * db for b, db in zip(bs, dbs)]\n",
    "    new_params = (new_Ws, new_bs)\n",
    "\n",
    "    return new_params, loss\n",
    "\n",
    "\n",
    "# ##### Train the model on the Two Moons dataset #####\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "train_fraction = 0.8\n",
    "test_fraction = 1 - train_fraction\n",
    "epochs = 2000\n",
    "learning_rate = 0.1\n",
    "N = 8  # Its nice to choose N ~ 2^n, since differences in width are only really important on log scale\n",
    "L = 3\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(train_fraction * len(X_moons))\n",
    "\n",
    "# Ensure the dataset is shuffled before splitting\n",
    "# [Just for fun, try removing this part and see how badly things get ruined]\n",
    "indices = np.random.permutation(len(X_moons))\n",
    "X_moons = X_moons[indices]\n",
    "y_moons = y_moons[indices]\n",
    "X_train, y_train = X_moons[:train_size], y_moons[:train_size]\n",
    "X_test, y_test = X_moons[train_size:], y_moons[train_size:]\n",
    "\n",
    "# Initialize the parameters\n",
    "params = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "# Arrays for saving training info\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    # Perform a gradient step\n",
    "    params, loss = gradient_step(X_train, y_train, params, learning_rate)\n",
    "    train_losses.append(loss)\n",
    "    # Evaluate on the test set\n",
    "    test_loss = bce_loss(X_test, y_test, params)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our answers!\n",
    "\n",
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot the decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = X_test[:, 0].min() - 0.1, X_test[:, 0].max() + 0.1\n",
    "y_min, y_max = X_test[:, 1].min() - 0.1, X_test[:, 1].max() + 0.1\n",
    "xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 100), jnp.linspace(y_min, y_max, 100))\n",
    "xs = jnp.array(list(zip(xx.ravel(), yy.ravel())))\n",
    "print(\"Shape of xs for contour plot: \", xs.shape)  # Should be (10000, 2)\n",
    "Z = vmap(classifier, in_axes=(0, None))(xs, params)\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, alpha=0.8, cmap=\"coolwarm\")\n",
    "ax.scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    c=y_test,\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have coded an MLP from scratch and trained it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise_2.1:_Ruining_the_model"
   },
   "source": [
    "### Exercise 2.1: Ruining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will deliberate try to break the model. The goal here is to get a little experience with failure modes.\n",
    "\n",
    "Try to deliberately overfit the model. You have a few knobs to play with: The depth $L$, the width $N$, the learning rate, the number of epochs, and the amount of training data. Try to find a combination of these that overfits the training data, i.e. the training loss is very low but the validation loss is high.\n",
    "\n",
    "Try to see what happens if you forget to shuffle the data! This is a very common mistake. I made this mistake when writing this tutorial, and it took me about 30 minutes to realize why the model was not learning anything.\n",
    "\n",
    "Try extreme learning rates, like $10^{-6}$ or $10^{2}$.\n",
    "\n",
    "When we loaded in the data a few cells ago, we normalized the data. What happens if the data is not O(1)? Try arranging the data so that the inputs are all $O(10^6)$ or $O(10^{-6})$. Technically, the optimal classifier should be coordinate-invariant, but in practice there can be issues! Try some other coordinate transforms of the data too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else unchanged, choosing $N \\gtrapprox 32$ and $L \\gtrapprox 8$ will overfit the training data. There are many other options though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise_2.2:_MNIST_with_our_JAX_MLP"
   },
   "source": [
    "### Exercise 2.2: MNIST with our JAX MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the above code to classify MNIST. You will need to change the loss function and the output activation to account for the fact that MNIST is a multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the MNIST Dataset\n",
    "train_fraction = 0.8\n",
    "test_fraction = 1 - train_fraction\n",
    "\n",
    "# Shuffle the dataset before splitting\n",
    "indices = np.random.permutation(len(X_mnist))\n",
    "X_mnist = X_mnist[indices]\n",
    "y_mnist = y_mnist[indices]\n",
    "train_size = int(train_fraction * len(X_mnist))\n",
    "X_train_mnist, y_train_mnist = X_mnist[:train_size], y_mnist[:train_size]\n",
    "X_test_mnist, y_test_mnist = X_mnist[train_size:], y_mnist[train_size:]\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return jax.nn.one_hot(y, num_classes)\n",
    "\n",
    "\n",
    "input_dim = 784  # MNIST images are 28x28 pixels, flattened to 784\n",
    "output_dim = 10  # 10 classes for digits 0-9\n",
    "\n",
    "\n",
    "# YOUR PARAMETERS HERE\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "N = 16\n",
    "L = 3\n",
    "\n",
    "# YOUR MODIFIED CLASSIFIER HERE\n",
    "classifier_mnist = lambda x, params: jax.nn.softmax(MLP_jax(x, params))\n",
    "vmapped_classifier = vmap(classifier_mnist, in_axes=(0, None))\n",
    "\n",
    "\n",
    "# YOUR MULTICLASS LOSS FUNCTION HERE\n",
    "def multiclass_cross_entropy_loss(x, y_true, params):\n",
    "    y_pred = vmapped_classifier(x, params)\n",
    "    epsilon = 1e-10  # Small value to avoid log(0)\n",
    "    return -jnp.mean(jnp.sum(y_true * jnp.log(y_pred + epsilon), axis=1))\n",
    "\n",
    "\n",
    "# YOUR TRAINING LOOP HERE\n",
    "params_mnist = init_params_jax(input_dim, output_dim, L, N)\n",
    "train_losses_mnist = []\n",
    "test_losses_mnist = []\n",
    "\n",
    "\n",
    "@jit\n",
    "def gradient_step_mnist(x, y_true, params, learning_rate=0.01):\n",
    "    loss = multiclass_cross_entropy_loss(x, y_true, params)\n",
    "    grads = grad(multiclass_cross_entropy_loss, argnums=2)(x, y_true, params)\n",
    "\n",
    "    Ws, bs = params\n",
    "    dWs, dbs = grads\n",
    "    new_Ws = [W - learning_rate * dW for W, dW in zip(Ws, dWs)]\n",
    "    new_bs = [b - learning_rate * db for b, db in zip(bs, dbs)]\n",
    "    new_params = (new_Ws, new_bs)\n",
    "\n",
    "    return new_params, loss\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    params_mnist, loss = gradient_step_mnist(\n",
    "        X_train_mnist,\n",
    "        one_hot_encode(y_train_mnist, output_dim),\n",
    "        params_mnist,\n",
    "        learning_rate,\n",
    "    )\n",
    "    train_losses_mnist.append(loss)\n",
    "    test_loss = multiclass_cross_entropy_loss(\n",
    "        X_test_mnist, one_hot_encode(y_test_mnist, output_dim), params_mnist\n",
    "    )\n",
    "    test_losses_mnist.append(test_loss)\n",
    "\n",
    "    # compute the accuracy\n",
    "    y_pred = jnp.argmax(\n",
    "        vmap(classifier_mnist, in_axes=(0, None))(X_test_mnist, params_mnist), axis=1\n",
    "    )\n",
    "    accuracy = jnp.mean(y_pred == y_test_mnist)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses_mnist, label=\"Train Loss\")\n",
    "plt.plot(test_losses_mnist, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot some predictions\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax[i // 5, i % 5].imshow(X_test_mnist[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax[i // 5, i % 5].set_title(\n",
    "        f\"Predicted: {jnp.argmax(classifier_mnist(X_test_mnist[i], params_mnist))}, True: {y_test_mnist[i]}\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "    ax[i // 5, i % 5].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.3_(BONUS):_Solving_ODE's_with_MLPs_and_Autodiff"
   },
   "source": [
    "## Chapter 2.3 (BONUS): Solving ODE's with MLPs and Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore how we can use MLPs to solve ODEs. The idea is to represent our solution $f(x)$ as an MLP, and write the ODE as the minimum of a loss function. We can use autodiff to compute the derivatives of $f(x)$, and then use gradient descent to minimize the loss function. Unlike what we did above, there is no training (this is not a statistical problem). Also, we are now dealing with gradients of the function within respect to the input AND with respect to the model parameters.\n",
    "\n",
    "Suppose we have an ODE of the form $F(f, f', x) = 0$, where $f$ is the function we want to solve for, $f'$ is the derivative of $f$ with respect to $x$, and $F$ is some function that defines the ODE. We can define a loss function as:\n",
    "$$ L[f] = \\int dx |F(f, f', x)|^2 $$\n",
    "\n",
    "where the integral is over the domain of $x$ we are interested in. The goal is to minimize this loss function with respect to the parameters of the MLP that defines $f(x)$.\n",
    "\n",
    "\n",
    "HINT: If $f(x)$ is just an MLP, then your solution will likely just collapse to just $f(x) = 0$. Try to find a way to write $f(x) = $ something involving an MLP but also manifestly satisfies the initial condition.\n",
    "\n",
    "BONUS: If you try to do a second-order ODE using our MLP, the solution will fail miserably. Why? Hint: this relates to piecewise-linearity. Can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the rest of this code to build our approximate ODE solver! This requires only minor modifications to the code we have already written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only solve the ODE on a compact domain\n",
    "x_domain = jnp.linspace(-4, 4, 1000)\n",
    "x_domain = x_domain.reshape(-1, 1)  # Reshape to (1000, 1) for compatibility with MLP\n",
    "\n",
    "x_0 = 0\n",
    "f_0 = 1  # Initial condition: f(0) = 1\n",
    "\n",
    "\n",
    "def my_solution(x, params):\n",
    "    g_x = MLP_jax(x, params)  # Get the output of the MLP\n",
    "\n",
    "    # This solution will automatically satisfy the initial condition\n",
    "    f_x = f_0 + (x - x_0) * g_x\n",
    "\n",
    "    return f_x\n",
    "\n",
    "\n",
    "vmapped_ODE_solution = vmap(\n",
    "    my_solution, in_axes=(0, None)\n",
    ")  # Vectorized solution function\n",
    "\n",
    "# lambda function to make the output of my_solution a scalar so that we can compute the gradient\n",
    "vmapped_grad_ODE = vmap(\n",
    "    grad(lambda x, params: my_solution(x, params).squeeze(), argnums=0),\n",
    "    in_axes=(0, None),\n",
    ")  # Vectorized gradient of MLP\n",
    "\n",
    "\n",
    "def F(f_x, grad_f_x, x):\n",
    "    # Example ODE: f = exp(-0.25 * x)\n",
    "    ODE_term = 0.25 * f_x + grad_f_x\n",
    "\n",
    "    return ODE_term\n",
    "\n",
    "\n",
    "vmapped_grad_MLP = vmap(\n",
    "    grad(lambda x, params: MLP_jax(x, params).squeeze(), argnums=0), in_axes=(0, None)\n",
    ")  # Vectorized gradient of MLP\n",
    "\n",
    "\n",
    "def ODE_loss(params):\n",
    "    # Compute the loss as the mean squared error between the network output and the ODE solution\n",
    "    f_x = vmapped_ODE_solution(x_domain, params)  # params is a tuple of (Ws, bs)\n",
    "    grad_f_x = vmapped_grad_ODE(x_domain, params)\n",
    "\n",
    "    # Compute the ODE residual\n",
    "    residual = F(f_x, grad_f_x, x_domain)\n",
    "\n",
    "    # Mean squared error loss\n",
    "    return jnp.mean(residual**2)\n",
    "\n",
    "\n",
    "def gradient_step_ODE(params, learning_rate=0.01):\n",
    "    # Compute the loss and its gradient\n",
    "    loss = ODE_loss(params)\n",
    "    grads = grad(ODE_loss)(params)\n",
    "\n",
    "    # Update the network parameters using gradient descent\n",
    "    Ws, bs = params\n",
    "    dWs, dbs = grads\n",
    "    new_Ws = [W - learning_rate * dW for W, dW in zip(Ws, dWs)]\n",
    "    new_bs = [b - learning_rate * db for b, db in zip(bs, dbs)]\n",
    "    new_params = (new_Ws, new_bs)\n",
    "\n",
    "    return new_params, loss\n",
    "\n",
    "\n",
    "# Initialize the parameters for the ODE solver\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "L = 3\n",
    "N = 17\n",
    "params = init_params_jax(input_dim, output_dim, L, N)\n",
    "\n",
    "\n",
    "# Train the ODE solver\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    params, loss = gradient_step_ODE(params, learning_rate=0.01)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot the solution\n",
    "y_vals = vmapped_ODE_solution(x_domain, params)\n",
    "plt.plot(\n",
    "    x_domain, jnp.exp(-0.25 * x_domain), label=\"True Solution: exp(-x)\", color=\"blue\"\n",
    ")\n",
    "plt.plot(x_domain, y_vals, label=\"MLP Solution\", color=\"orange\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_2.4:_Some_notes_on_JAX_Prebuilt_Libraries"
   },
   "source": [
    "## Chapter 2.4: Some notes on JAX Prebuilt Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has a number of prebuilt libraries that can be useful for machine learning. Some of the most popular ones are:\n",
    "1. **Flax**: A neural network library for JAX that provides a high-level interface for building and training neural networks. It is similar to PyTorch in terms of functionality, but uses JAX's functional programming style.\n",
    "2. **stax**: A library for probabilistic programming in JAX. It provides a high-level interface for building and training probabilistic models, and is similar to PyMC3 or TensorFlow Probability.\n",
    "\n",
    "However, we will not cover these in any great depth This is because if you are going to use prebuilt libraries, you are probably better off using PyTorch or TensorFlow, which have more mature libraries and a larger community. JAX is more useful for experimentation with explicitly defined models and getting into the guts of it all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for completeness, we will show how to use stax to build a simple MLP. This is not meant to be a comprehensive tutorial on these libraries, but rather a quick introduction to their usage so you can see the syntax. The syntax between stax and flax is virtually identical. Both are also directly meant to mimic PyTorch anyways, so after this example, we will go into an in-depth PyTorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import stax\n",
    "\n",
    "# prepare data\n",
    "indices = np.random.permutation(len(X_moons))\n",
    "X_moons = X_moons[indices]\n",
    "y_moons = y_moons[indices]\n",
    "X_moons = X_moons.astype(np.float32)  # Convert to float32 for JAX compatibility\n",
    "y_moons = y_moons.astype(np.float32)  # Convert to float32 for JAX compatibility\n",
    "\n",
    "\n",
    "# A model is a sequential list of layers\n",
    "init, apply = stax.serial(\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,  # Dense(N) is a fully connected layer with output dimension N. The input dimension is inferred from the input data.\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1),\n",
    ")\n",
    "\n",
    "\n",
    "# init is the function to initialize the parameters of the model.\n",
    "# apply is the function to apply the model to the input data.\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key1, key2 = random.split(key)\n",
    "_, params = init(key2, (-1, 2))\n",
    "\n",
    "\n",
    "# Define a loss function for binary classification\n",
    "def loss(params, x, y):\n",
    "    logits = jnp.squeeze(apply(params, x))\n",
    "    return -jnp.mean(\n",
    "        y * jax.nn.log_sigmoid(logits) + (1 - y) * jax.nn.log_sigmoid(-logits)\n",
    "    )\n",
    "\n",
    "\n",
    "@jit  # Same type of gradient step as before, but now using stax\n",
    "def step(params, x, y, lr=0.01):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "\n",
    "    # tree_map is a JAX function that applies a function to each leaf of a pytree (like a list of arrays)\n",
    "    # Makes it easy to update the parameters all at once\n",
    "    return jax.tree_util.tree_map(lambda a, b: a - lr * b, params, grads)\n",
    "\n",
    "\n",
    "for _ in range(10000):\n",
    "    params = step(params, X_moons, y_moons)\n",
    "\n",
    "print(\"stax loss:\", loss(params, X_moons, y_moons))\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_moons[:, 0].min() - 0.1, X_moons[:, 0].max() + 0.1\n",
    "y_min, y_max = X_moons[:, 1].min() - 0.1, X_moons[:, 1].max() + 0.1\n",
    "xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 100), jnp.linspace(y_min, y_max, 100))\n",
    "xs = jnp.array(list(zip(xx.ravel(), yy.ravel())))\n",
    "Z = jnp.squeeze(apply(params, xs))\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(xx, yy, jax.nn.sigmoid(Z), alpha=0.8, cmap=\"coolwarm\")\n",
    "ax.scatter(\n",
    "    X_moons[:, 0],\n",
    "    X_moons[:, 1],\n",
    "    c=y_moons,\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3:_PyTorch"
   },
   "source": [
    "# Chapter 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a popular ML library developed by Meta (formerly Facebook). It is widely used in industry and research. It is typically more high-level than JAX, and has a wider range of prebuilt modules and utilities for common ML tasks. It is also older and more widely supported with documentation and tutorials online.\n",
    "\n",
    "Unlike JAX, PyTorch is object-oriented, meaning that we will define an MLP as a class with an internal state that keeps track of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3.1:_Primer_on_PyTorch_tensors_and_autodiff"
   },
   "source": [
    "## Chapter 3.1: Primer on PyTorch tensors and autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has its own tensor class, which are similar to (but not the same as) numpy arrays. They can live on a \"device\" (e.g. `cuda` or `cpu`), and can be used to perform computations on that device. PyTorch tensors have many of the same methods as numpy arrays, but also have some additional methods for ML tasks, such as `backward()` for computing gradients.\n",
    "\n",
    "Autodiff in PyTorch is different than in JAX. In PyTorch, we define a computation graph by performing operations on tensors, and then call `backward()` on the output tensor to compute the gradients. This is different from JAX, where we define a function and then call `grad()` on that function to compute the gradients. Once a tensor is in the graph, it cannot be converted back to numpy without first `detach`ing it.\n",
    "\n",
    "To emphasize, in JAX, we take derivatives of functions, and the derivative of a function is another function. In PyTorch, we tell the graph to keep track of a tensor, we pass the tensor through a function, and then we ``backpropagate'' the resultant tensor. Derivatives are computed on the output tensor and the result is a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pytorch tensor defined from our numpy moons dataset\n",
    "x_pytorch = torch.tensor(X_moons, dtype=torch.float32, device=device)\n",
    "\n",
    "# Converting the pytorch tensor back to a numpy tensor\n",
    "x_numpy = x_pytorch.numpy()\n",
    "\n",
    "\n",
    "##### DEFINING GRADIENTS AND COMPUTATION GRAPHS ######\n",
    "\n",
    "\n",
    "# Define a function and compute gradients\n",
    "def f_torch(x):\n",
    "    return torch.sin(10 * x) * torch.exp(-2 * x**2)\n",
    "\n",
    "\n",
    "# requires_grad=True to compute gradients later! Try setting to False and see what happens!\n",
    "\n",
    "x_single = torch.tensor([0.0], device=device, requires_grad=True)  # Single value tensor\n",
    "y_single = f_torch(x_single)  # Compute the function value\n",
    "\n",
    "\n",
    "# We want to compute the gradient of the function and plot it.\n",
    "# First, we call .backward on the output of the function (not the function itself!).\n",
    "# This tells Pytorch its time to compute the gradients in the computational graph.\n",
    "\n",
    "y_single.backward()  # Compute the gradient for the single value\n",
    "\n",
    "# Now the gradients are computed. To access it, we use .grad() on the input tensor.\n",
    "print(\"Gradient at x=0.0: \", x_single.grad.item())  # Should print the gradient at x=0.0\n",
    "\n",
    "\n",
    "##### MULTIPLE INPUTS ######\n",
    "\n",
    "# The same as above, but now on a vectorized input\n",
    "xs = torch.linspace(-1, 1, 1000, device=device, requires_grad=True)\n",
    "ys = f_torch(xs)\n",
    "\n",
    "# We cannot just use ys.backward() because ys is a vector, not a scalar.\n",
    "# ys has 1000 elements, and xs has 1000 elements, so PyTorch thinks there is a 1000 x 1000 jacobian!\n",
    "# We need to specify a gradient direction to sum the gradients over the output dimension. Since y_j is indepndent of x_i for i!=j, we can just use a vector of ones as the gradient direction.\n",
    "gradient_direction = torch.ones_like(ys, device=device)  #\n",
    "_ = ys.backward(gradient=gradient_direction)  # Compute the gradients\n",
    "# Now we can access the gradients\n",
    "xs_grad = (\n",
    "    xs.grad\n",
    ")  # This will be a tensor of the same shape as xs in the specified direction.\n",
    "\n",
    "\n",
    "# Plot the function and its gradient\n",
    "plt.plot(xs.detach().numpy(), ys.detach().numpy(), label=\"f(x)\")\n",
    "plt.plot(xs.detach().numpy(), xs_grad, label=\"f'(x)\")\n",
    "\n",
    "\n",
    "# Zero-ing out gradients\n",
    "# In PyTorch, gradients accumulate by default, so we need to zero them out before the next backward pass.\n",
    "x_single.grad.zero_()  # Zero out the gradients\n",
    "xs.grad.zero_()  # Zero out the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chapter_3.2:_Pre-built_PyTorch_Modules"
   },
   "source": [
    "## Chapter 3.2: Pre-built PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sequential` class\n",
    "\n",
    "There are many possible layers of abstraction in PyTorch. The highest-level, \"I don't care, I just want a neural network, don't bother me with details\" way to define an MLP is to use the `Sequential` class.\n",
    "\n",
    "Most models are simply a sequence of layers, where each layer is a function that takes in the output of the previous layer and produces an output. The `Sequential` class allows us to define a model as a sequence of layers, where each layer is applied in order. The ``nn`` module provides many prebuilt layers, such as `Linear`, `ReLU`, and `Softmax`, that can be used to define a model. Note that `Linear` actually means \"Affine\", i.e. it implements the affine transformation $W x + b$.\n",
    "\n",
    "Let's see how to implement an MLP. Compared to the above implementations, this will be much shorter and cleaner, since most of this is already implemented for us in PyTorch (but the cost is that we have less control over the details of the implementation, like if we wanted to mess with layer weights). Note that we don't even have to worry about the initialization of the weights, since PyTorch does this for us automatically.\n",
    "\n",
    "We also don't have to bother defining a loss function or gradient descent, sice this also already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),  # We just stack a bunch of layers\n",
    "    nn.Linear(\n",
    "        32, output_dim\n",
    "    ),  # No activation on the last layer. Instead, we'll put the sigmoid or softmax in the loss function..\n",
    ").to(\n",
    "    device\n",
    ")  # .to(device) moves the model to the GPU if available\n",
    "\n",
    "\n",
    "# Lots of optimizers to choose from! SGD is ordinary (stochastic) gradient descent, Adam is a more advanced optimizer.\n",
    "opt_sgd = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "opt_adam = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "# Prebuilt BCE. Already includes the sigmoid, so we don't need to apply it in the model.\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 300\n",
    "train_fraction = 0.8\n",
    "opt = opt_adam  # Choose the optimizer\n",
    "\n",
    "X_torch = torch.tensor(X_moons, dtype=torch.float32, device=device)\n",
    "y_torch = torch.tensor(y_moons, dtype=torch.float32, device=device).unsqueeze(\n",
    "    1\n",
    ")  # Unsqueeze to make it a column vector\n",
    "\n",
    "# Training/test split, shuffle the dataset\n",
    "train_size = int(train_fraction * len(X_torch))\n",
    "indices = torch.randperm(len(X_torch))  # Shuffle the dataset\n",
    "X_torch = X_torch[indices]\n",
    "y_torch = y_torch[indices]\n",
    "X_train, y_train = X_torch[:train_size], y_torch[:train_size]\n",
    "X_test, y_test = X_torch[train_size:], y_torch[train_size:]\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for _ in range(300):\n",
    "    opt.zero_grad()  # Zero out the gradients before the backward pass. VITAL!\n",
    "    loss = loss_fn(model(X_train), y_train)\n",
    "    loss.backward()  # Tell the graph its time to compute the gradients\n",
    "    opt.step()  # \"step\" automatically updates the parameters using the gradients computed in the backward pass\n",
    "    train_losses.append(loss.item())  # Save the training loss\n",
    "    test_loss = loss_fn(model(X_test), y_test)\n",
    "    test_losses.append(test_loss.item())  # Save the test loss\n",
    "    if _ % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {_}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"PyTorch loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test losses\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot the decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = X_test[:, 0].min() - 0.1, X_test[:, 0].max() + 0.1\n",
    "y_min, y_max = X_test[:, 1].min() - 0.1, X_test[:, 1].max() + 0.1\n",
    "xx, yy = torch.meshgrid(\n",
    "    torch.linspace(x_min, x_max, 100), torch.linspace(y_min, y_max, 100)\n",
    ")\n",
    "xs = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(\n",
    "    device\n",
    ")  # Stack to create a grid of points\n",
    "Z = model(xs)  # Get the model predictions\n",
    "\n",
    "\n",
    "# Dont forget to apply the sigmoid to the logits!\n",
    "Z = torch.sigmoid(Z)  # Apply sigmoid to the logits\n",
    "\n",
    "Z = Z.detach().cpu().numpy()\n",
    "Z = Z.reshape(xx.shape)  # Reshape to match the grid shape\n",
    "ax.contourf(xx.cpu().numpy(), yy.cpu().numpy(), Z, alpha=0.8, cmap=\"coolwarm\")\n",
    "ax.scatter(\n",
    "    X_test[:, 0].cpu().numpy(),\n",
    "    X_test[:, 1].cpu().numpy(),\n",
    "    c=y_test.cpu().numpy(),\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise_3.1:"
   },
   "source": [
    "### Exercise 3.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we don't have to code anything, lets try some more advanced things.\n",
    "1) Compare the ADAM optimizer to the SGD optimizer.\n",
    "2) Trade out the ReLU activation for sigmoid (used in the original MLP's back in the day), selu (a variant of ReLU that doesn't go to 0), and others. How do they compare? The modern lore in 2025 is to use \"Swish\" (aka \"SiLU\") activations, which are supposedly better than ReLU. There are many more at https://docs.pytorch.org/docs/main/nn.functional.html#non-linear-activation-functions. Try them out!\n",
    "3) PyTorch MLP layers are called \"Linear\" despite being affine transformations. Let's see what happens if they are literally linear: the bias term can be removed with `bias=False`, e.g. `nn.Linear(32,32, bias = False)`. What happens to the decision boundary? As you do this, reflect on the exercise we did earlier where we constructed MLPs to exactly match piecewise linear functions, and the role the bias played there.\n",
    "4) It should be straightforward to change the model to work on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Chapter_1:_Neural_Network_Basics",
    "Chapter_1.1:_Universal_Function_Approximation",
    "Exercise:_Modeling_|x|",
    "Exercise:_Approximating_a_smooth_1D_function.",
    "END_SOLUTION",
    "Chapter_1.2:_Functional_Optimization",
    "Interlude:_The_problems_we_will_solve:",
    "Chapter_2:_JAX",
    "Chapter_2.1:_JAX_basics;_vmapping,_autodifferentiation,_and_compilation.",
    "Exercise:_Autodifferentiation_Practice",
    "Chapter_2.2:_End-to-End_MLP_and_Training_from_Scratch",
    "Exercise_2.1:_Ruining_the_model",
    "Exercise_2.2:_MNIST_with_our_JAX_MLP",
    "Chapter_2.3_(BONUS):_Solving_ODE's_with_MLPs_and_Autodiff",
    "Chapter_2.4:_Some_notes_on_JAX_Prebuilt_Libraries",
    "Chapter_3:_PyTorch",
    "Chapter_3.1:_Primer_on_PyTorch_tensors_and_autodiff",
    "Chapter_3.2:_Pre-built_PyTorch_Modules",
    "Exercise_3.1:"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}