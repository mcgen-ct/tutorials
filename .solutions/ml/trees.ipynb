{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Introduction_to_Tree-based_methods_(with_external_dependencies)"
   },
   "source": [
    "# Introduction to Tree-based methods (with external dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by:\n",
    "- Manuel Szewc (School of Physics, University of Cincinnati)\n",
    "- Philip Ilten (School of Physics, University of Cincinnati)\n",
    "$\\renewcommand{\\gtrsim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{>}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\lessim}{\\raisebox{-2mm}{\\hspace{1mm}$\\stackrel{<}{\\sim}$\\hspace{1mm}}}\\renewcommand{\\as}{\\alpha_{\\mathrm{s}}}\\renewcommand{\\aem}{\\alpha_{\\mathrm{em}}}\\renewcommand{\\kT}{k_{\\perp}}\\renewcommand{\\pT}{p_{\\perp}}\\renewcommand{\\pTs}{p^2_{\\perp}}\\renewcommand{\\pTe}{\\p_{\\perp\\mrm{evol}}}\\renewcommand{\\pTse}{\\p^2_{\\perp\\mrm{evol}}}\\renewcommand{\\pTmin}{p_{\\perp\\mathrm{min}}}\\renewcommand{\\pTsmim}{p^2_{\\perp\\mathrm{min}}}\\renewcommand{\\pTmax}{p_{\\perp\\mathrm{max}}}\\renewcommand{\\pTsmax}{p^2_{\\perp\\mathrm{max}}}\\renewcommand{\\pTL}{p_{\\perp\\mathrm{L}}}\\renewcommand{\\pTD}{p_{\\perp\\mathrm{D}}}\\renewcommand{\\pTA}{p_{\\perp\\mathrm{A}}}\\renewcommand{\\pTsL}{p^2_{\\perp\\mathrm{L}}}\\renewcommand{\\pTsD}{p^2_{\\perp\\mathrm{D}}}\\renewcommand{\\pTsA}{p^2_{\\perp\\mathrm{A}}}\\renewcommand{\\pTo}{p_{\\perp 0}}\\renewcommand{\\shat}{\\hat{s}}\\renewcommand{\\a}{{\\mathrm a}}\\renewcommand{\\b}{{\\mathrm b}}\\renewcommand{\\c}{{\\mathrm c}}\\renewcommand{\\d}{{\\mathrm d}}\\renewcommand{\\e}{{\\mathrm e}}\\renewcommand{\\f}{{\\mathrm f}}\\renewcommand{\\g}{{\\mathrm g}}\\renewcommand{\\hrm}{{\\mathrm h}}\\renewcommand{\\lrm}{{\\mathrm l}}\\renewcommand{\\n}{{\\mathrm n}}\\renewcommand{\\p}{{\\mathrm p}}\\renewcommand{\\q}{{\\mathrm q}}\\renewcommand{\\s}{{\\mathrm s}}\\renewcommand{\\t}{{\\mathrm t}}\\renewcommand{\\u}{{\\mathrm u}}\\renewcommand{\\A}{{\\mathrm A}}\\renewcommand{\\B}{{\\mathrm B}}\\renewcommand{\\D}{{\\mathrm D}}\\renewcommand{\\F}{{\\mathrm F}}\\renewcommand{\\H}{{\\mathrm H}}\\renewcommand{\\J}{{\\mathrm J}}\\renewcommand{\\K}{{\\mathrm K}}\\renewcommand{\\L}{{\\mathrm L}}\\renewcommand{\\Q}{{\\mathrm Q}}\\renewcommand{\\R}{{\\mathrm R}}\\renewcommand{\\T}{{\\mathrm T}}\\renewcommand{\\W}{{\\mathrm W}}\\renewcommand{\\Z}{{\\mathrm Z}}\\renewcommand{\\bbar}{\\overline{\\mathrm b}}\\renewcommand{\\cbar}{\\overline{\\mathrm c}}\\renewcommand{\\dbar}{\\overline{\\mathrm d}}\\renewcommand{\\fbar}{\\overline{\\mathrm f}}\\renewcommand{\\pbar}{\\overline{\\mathrm p}}\\renewcommand{\\qbar}{\\overline{\\mathrm q}}\\renewcommand{\\rbar}{\\overline{\\mathrm{r}}}\\renewcommand{\\sbar}{\\overline{\\mathrm s}}\\renewcommand{\\tbar}{\\overline{\\mathrm t}}\\renewcommand{\\ubar}{\\overline{\\mathrm u}}\\renewcommand{\\Bbar}{\\overline{\\mathrm B}}\\renewcommand{\\Fbar}{\\overline{\\mathrm F}}\\renewcommand{\\Qbar}{\\overline{\\mathrm Q}}\\renewcommand{\\tms}{{t_{\\mathrm{\\tiny MS}}}}\\renewcommand{\\Oas}[1]{{\\mathcal{O}\\left(\\as^{#1}\\right)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook wants to implement decision trees, random forests and gradient boosting. A lot of it is based on [Aurelien Geron's lectures](https://github.com/ageron/handson-ml3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# To generate data and handle arrays\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc(\"axes\", labelsize=14)\n",
    "mpl.rc(\"xtick\", labelsize=12)\n",
    "mpl.rc(\"ytick\", labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2  # pip install opencv-python\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "# Useful classes for data manipulation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Useful classes for model evaluation and selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "# a baseline classifier\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "\n",
    "# The necessary models\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Theory"
   },
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree, most usually based on the **Classification and Regression Tree** (CART) framework, work by **recursively partitioning** the input space through a series of **binary decisions** in order to predict a target (either for classification or regression). Once an appropriate partitioning of the feature space is achieved, a new prediction is computed by following the set of binary splittings.\n",
    "\n",
    "Under the CART framework, at a given decision step $m$, we split a **node** $m$ containing $N_{m}$ instances into two by looking at all available features $\\vec{x}\\in\\mathbb{R}^{D}$. A decision tree finds a feature $i$ and the best cut $\\theta_{m}$ in **one** of the features such that when the data is divided in two new nodes according to $x_{i}\\leq \\theta$ the weighted sum of a given metric $H$ evaluated over the two candidate nodes is optimized:\n",
    "\n",
    "$$G(\\theta_{m},i)=\\frac{N_{m,x_{i}\\leq \\theta_{m}}}{N_{m}}H(\\text{instances with }x_{i}\\leq \\theta)+\\frac{N_{m,x_{i}> \\theta_{m}}}{N_{m}}H(\\text{instances with }x_{i}> \\theta_{m})$$\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\\theta^{*}_{m},i^{*} = \\arg \\min_{\\theta_{m},i}\\sum_{n=1}^{N_{m}}G(\\theta_{m},i)$$\n",
    "\n",
    "The resulting two nodes are called **children**. The initial node is called a **root** and the nodes which have no children, and are thus final, are called **leaves**.\n",
    "\n",
    "For $K$ class classification problems, the usual metric $H$ is either the `Gini` or the `entropy` defined as\n",
    "\n",
    "$$\\mathrm{Gini} = \\sum\\sum_{k=1}^{K}p_{m,k}(1-p_{m,k})$$\n",
    "\n",
    "$$\\mathrm{Entropy} = -\\sum_{k=1}^{K}p_{m,k}\\ln p_{m,k}$$\n",
    "\n",
    "where $p_{k}$ are the fraction of instances belonging to class $k$ in the node.\n",
    "\n",
    "For regression problems, it's usually the mean squared error defined as\n",
    "\n",
    "$$\\mathrm{MSE} = \\frac{1}{N_{m}}\\sum_{n=1}^{N_{m}}(y_{m}-\\bar y_{m})^{2}$$\n",
    "\n",
    "where $\\bar y_{m}$ is the average target value in the node $\\bar y_{m} = \\frac{1}{N_{m}}\\sum_{n=1}^{N_{m}}y_{m}$.\n",
    "\n",
    "\n",
    "Decision Trees are **greedy** algorithms, in that all binary partitions are decided based on how well they perform, without regard to **global strategies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `sklearn`:\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "\n",
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "\n",
    "* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support missing values.\n",
    "\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "\n",
    "* Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "\n",
    "* Able to handle multi-output problems.\n",
    "\n",
    "* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "\n",
    "* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "\n",
    "* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "* Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n",
    "\n",
    "* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Classification_with_Decision_Trees"
   },
   "source": [
    "## Classification with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `sklearn` implementation of decision trees.\n",
    "\n",
    "Let's use a dataset as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -N https://gitlab.com/mcgen-ct/tutorials/-/raw/2025-cteq/.full/ml/datasets/season-1112.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datahub.io/sports-data/english-premier-league and https://www.football-data.co.uk/notes.txt\n",
    "df = pd.read_csv(\"season-1112.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has all matches of the 2011-2012 English Premier League season.\n",
    "For each match, we have local and away goals both at half-time and at the end of the match. We also have the number of shots, shots on goal, fouls, yellow cards, red cards and betting odds from some known sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play with this.\n",
    "\n",
    "One possibility is trying to predict a winner based on all other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a copy before further processingHagamos una copia antes de empezar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = (\n",
    "    df.copy()\n",
    ")  # [['HomeTeam','AwayTeam', 'FTHG','FTAG','FTR','HTHG', 'HTAG', 'HTR','HS','AS','HST', 'AST','HF','AF', 'HY', 'AY', 'HR', 'AR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_train, df_copy_test = train_test_split(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data to try and understand how a Decision Tree works.\n",
    "\n",
    "To plot, let's look at only two features for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = np.zeros(len(df_copy_train))\n",
    "target_train[df_copy_train[\"FTR\"] == \"H\"] = 1.0\n",
    "target_train[df_copy_train[\"FTR\"] == \"D\"] = 0.0\n",
    "target_train[df_copy_train[\"FTR\"] == \"A\"] = -1.0\n",
    "features_train = np.asarray(df_copy_train[[\"FTHG\", \"FTAG\"]])\n",
    "\n",
    "target_test = np.zeros(len(df_copy_test))\n",
    "target_test[df_copy_test[\"FTR\"] == \"H\"] = 1.0\n",
    "target_test[df_copy_test[\"FTR\"] == \"D\"] = 0.0\n",
    "target_test[df_copy_test[\"FTR\"] == \"A\"] = -1.0\n",
    "features_test = np.asarray(df_copy_test[[\"FTHG\", \"FTAG\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scatter = features_train + 0.1 * np.random.randn(len(features_train), 2)\n",
    "plt.scatter(features_scatter[:, 0], features_scatter[:, 1], c=target_train, alpha=0.2)\n",
    "plt.colorbar()\n",
    "xvals = np.linspace(0.0, 8.0, 10)\n",
    "plt.plot(xvals, xvals, linestyle=\"dotted\", color=\"black\", label=\"Draw\")\n",
    "plt.xlabel(\"Home team goals\")\n",
    "plt.ylabel(\"Away team goals\")\n",
    "plt.xlim(-0.2, 8.2)\n",
    "plt.ylim(-0.2, 6.2)\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forget about all the DT hyperparameters for now and just train a naive classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict(np.asarray([1.0, 2.0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(0.0, 8.0, 100)\n",
    "yvals = np.linspace(0.0, 6.0, 100)\n",
    "X, Y = np.meshgrid(xvals, yvals)\n",
    "Z = dt.predict(np.c_[X.ravel(), Y.ravel()]).reshape(X.shape)\n",
    "plt.contourf(xvals, yvals, Z, levels=[-1.5, -0.5, 0.5, 1.5], label=\"DT\")\n",
    "plt.colorbar()\n",
    "plt.scatter(features_train[:, 0], features_train[:, 1], c=target_train)\n",
    "plt.plot(xvals, xvals, linestyle=\"dotted\", color=\"black\", label=\"Draw\")\n",
    "plt.xlabel(\"Home team goals\")\n",
    "plt.ylabel(\"Away team goals\")\n",
    "plt.xlim(-0.2, 8.2)\n",
    "plt.ylim(-0.2, 6.2)\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's really overfitting! Weird looking curves.\n",
    "\n",
    "We wouldn't be able to tell from the confusion matrix though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(target_train, dt.predict(features_train)))\n",
    "print(confusion_matrix(target_test, dt.predict(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But an inspection of the defined tree would show it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    dt,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    feature_names=[\"Home team goals\", \"Away team goals\"],\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot can also be exported as a `.dot` file and saved as `.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=\"futbol.dot\",\n",
    "    feature_names=[\"Home team goals\", \"Away team goals\"],\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# dot to png\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !apt-get install graphviz\n",
    "\n",
    "! dot -Tpng futbol.dot -o futbol.png\n",
    "\n",
    "# Plot the image\n",
    "img = cv2.imread(\"futbol.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DT can only do cuts in the individual features. Thus, it looks at home and away goals separately. But we know that in futbol the only important thing is to score more than the other team. The fact that it can only perform cuts on individual features can be a problem for DTs (but also the reason why we do not need to preprocess the features to remove units).\n",
    "\n",
    "We can do some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = df_copy_train[[\"FTHG\", \"FTAG\"]]\n",
    "features_train[\"Local - Visitante\"] = features_train[\"FTHG\"] - features_train[\"FTAG\"]\n",
    "features_train = np.asarray(features_train)\n",
    "\n",
    "features_test = df_copy_test[[\"FTHG\", \"FTAG\"]]\n",
    "features_test[\"Local - Visitante\"] = features_test[\"FTHG\"] - features_test[\"FTAG\"]\n",
    "features_test = np.asarray(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=\"futbol.dot\",\n",
    "    feature_names=[\"Home\", \"Away\", \"Goal Difference\"],\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng futbol.dot -o futbol.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"futbol.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such an easy example, DTs are not particularly useful. But now let's look at all the features that are less obvious in relation to wins. Let's remove the betting scores also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_train = df_copy_train[[\"HomeTeam\", \"AwayTeam\"]]\n",
    "features_train = df_copy_train.drop(\n",
    "    [\n",
    "        \"Div\",\n",
    "        \"Date\",\n",
    "        \"Referee\",\n",
    "        \"HomeTeam\",\n",
    "        \"AwayTeam\",\n",
    "        \"FTHG\",\n",
    "        \"FTAG\",\n",
    "        \"FTR\",\n",
    "        \"HTHG\",\n",
    "        \"HTAG\",\n",
    "        \"HTR\",\n",
    "        \"B365H\",\n",
    "        \"B365D\",\n",
    "        \"B365A\",\n",
    "        \"BWH\",\n",
    "        \"BWD\",\n",
    "        \"BWA\",\n",
    "        \"GBH\",\n",
    "        \"GBD\",\n",
    "        \"GBA\",\n",
    "        \"IWH\",\n",
    "        \"IWD\",\n",
    "        \"IWA\",\n",
    "        \"LBH\",\n",
    "        \"LBD\",\n",
    "        \"LBA\",\n",
    "        \"SBH\",\n",
    "        \"SBD\",\n",
    "        \"SBA\",\n",
    "        \"WHH\",\n",
    "        \"WHD\",\n",
    "        \"WHA\",\n",
    "        \"SJH\",\n",
    "        \"SJD\",\n",
    "        \"SJA\",\n",
    "        \"VCH\",\n",
    "        \"VCD\",\n",
    "        \"VCA\",\n",
    "        \"BSH\",\n",
    "        \"BSD\",\n",
    "        \"BSA\",\n",
    "        \"Bb1X2\",\n",
    "        \"BbMxH\",\n",
    "        \"BbAvH\",\n",
    "        \"BbMxD\",\n",
    "        \"BbAvD\",\n",
    "        \"BbMxA\",\n",
    "        \"BbAvA\",\n",
    "        \"BbOU\",\n",
    "        \"BbMx>2.5\",\n",
    "        \"BbAv>2.5\",\n",
    "        \"BbMx<2.5\",\n",
    "        \"BbAv<2.5\",\n",
    "        \"BbAH\",\n",
    "        \"BbAHh\",\n",
    "        \"BbMxAHH\",\n",
    "        \"BbAvAHH\",\n",
    "        \"BbMxAHA\",\n",
    "        \"BbAvAHA\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "names_test = df_copy_test[[\"HomeTeam\", \"AwayTeam\"]]\n",
    "features_test = df_copy_test.drop(\n",
    "    [\n",
    "        \"Div\",\n",
    "        \"Date\",\n",
    "        \"Referee\",\n",
    "        \"HomeTeam\",\n",
    "        \"AwayTeam\",\n",
    "        \"FTHG\",\n",
    "        \"FTAG\",\n",
    "        \"FTR\",\n",
    "        \"HTHG\",\n",
    "        \"HTAG\",\n",
    "        \"HTR\",\n",
    "        \"B365H\",\n",
    "        \"B365D\",\n",
    "        \"B365A\",\n",
    "        \"BWH\",\n",
    "        \"BWD\",\n",
    "        \"BWA\",\n",
    "        \"GBH\",\n",
    "        \"GBD\",\n",
    "        \"GBA\",\n",
    "        \"IWH\",\n",
    "        \"IWD\",\n",
    "        \"IWA\",\n",
    "        \"LBH\",\n",
    "        \"LBD\",\n",
    "        \"LBA\",\n",
    "        \"SBH\",\n",
    "        \"SBD\",\n",
    "        \"SBA\",\n",
    "        \"WHH\",\n",
    "        \"WHD\",\n",
    "        \"WHA\",\n",
    "        \"SJH\",\n",
    "        \"SJD\",\n",
    "        \"SJA\",\n",
    "        \"VCH\",\n",
    "        \"VCD\",\n",
    "        \"VCA\",\n",
    "        \"BSH\",\n",
    "        \"BSD\",\n",
    "        \"BSA\",\n",
    "        \"Bb1X2\",\n",
    "        \"BbMxH\",\n",
    "        \"BbAvH\",\n",
    "        \"BbMxD\",\n",
    "        \"BbAvD\",\n",
    "        \"BbMxA\",\n",
    "        \"BbAvA\",\n",
    "        \"BbOU\",\n",
    "        \"BbMx>2.5\",\n",
    "        \"BbAv>2.5\",\n",
    "        \"BbMx<2.5\",\n",
    "        \"BbAv<2.5\",\n",
    "        \"BbAH\",\n",
    "        \"BbAHh\",\n",
    "        \"BbMxAHH\",\n",
    "        \"BbAvAHH\",\n",
    "        \"BbMxAHA\",\n",
    "        \"BbAvAHA\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took off the team names since we don't care about them in order to predict. The DT could use them if we turn them into a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the DT hyperparameters to regularize the algorithm. In particular, we can choose whether it uses Gini or Entropy to calculate the impurity of a split. Generally, there is no difference, but by definition, Gini may favor the most frequent class more. The advantage is that it is faster.\n",
    "\n",
    "Looking at the other hyperparameters, the options we have in `sklearn` are:\n",
    "\n",
    "`max_depth`: By default, this is `None`; it controls the depth of the tree.\n",
    "`min_samples_split`: Sets the minimum number of samples a node must have to continue splitting it.\n",
    "`min_samples_leaf`: The minimum number of samples a leaf (i.e., the end node) must have.\n",
    "`min_weight_fraction_leaf`: The minimum weighted fraction of samples a leaf must have.\n",
    "`max_leaf_nodes`: Maximum number of leaves.\n",
    "`max_features`: Maximum number of features evaluated in a split.\n",
    "\n",
    "If you raise the minimum values \u200b\u200bor lower the maximum values, you are restricting the tree and regularizing the model.\n",
    "\n",
    "There are other regularization methods, such as pruning, in which you train without restrictions and then remove unnecessary nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "# dt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(features_train, target_train)\n",
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=\"futbol.dot\",\n",
    "    feature_names=features_train.columns,\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng futbol.dot -o futbol.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"futbol.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_leaf=50, max_depth=100)\n",
    "dt.fit(features_train, target_train)\n",
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=\"futbol.dot\",\n",
    "    feature_names=features_train.columns,\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng futbol.dot -o futbol.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"futbol.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_leaf_nodes=6)\n",
    "dt.fit(features_train, target_train)\n",
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=\"futbol.dot\",\n",
    "    feature_names=features_train.columns,\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng futbol.dot -o futbol.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"futbol.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's really optimize things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "params = {\n",
    "    \"max_depth\": [2, 3, 5],\n",
    "    \"min_samples_leaf\": [10, 50],\n",
    "    \"max_leaf_nodes\": [3, 4, 5],\n",
    "}\n",
    "grid = GridSearchCV(dt, params, cv=10, scoring=\"accuracy\")\n",
    "grid.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(\n",
    "    model,\n",
    "    out_file=\"futbol.dot\",\n",
    "    feature_names=features_train.columns,\n",
    "    class_names=[\"Away team wins\", \"Draw\", \"Home team wins\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng futbol.dot -o futbol.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"futbol.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = cross_val_predict(model, features_train, target_train, cv=5)\n",
    "print(confusion_matrix(target_train, predicts))\n",
    "print(\n",
    "    recall_score(\n",
    "        np.where(target_train == -1.0, 1.0, 0.0), np.where(predicts == -1.0, 1.0, 0.0)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    recall_score(\n",
    "        np.where(target_train == 0.0, 1.0, 0.0), np.where(predicts == 0.0, 1.0, 0.0)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    recall_score(\n",
    "        np.where(target_train == 1.0, 1.0, 0.0), np.where(predicts == 1.0, 1.0, 0.0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    accuracy_score(\n",
    "        np.where(target_train == -1.0, 1.0, 0.0), np.where(predicts == -1.0, 1.0, 0.0)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    accuracy_score(\n",
    "        np.where(target_train == 0.0, 1.0, 0.0), np.where(predicts == 0.0, 1.0, 0.0)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    accuracy_score(\n",
    "        np.where(target_train == 1.0, 1.0, 0.0), np.where(predicts == 1.0, 1.0, 0.0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(target_test, model.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba(features_train[:3]))\n",
    "print(np.argmax(model.predict_proba(features_train[:3]), axis=1) - 1)\n",
    "print(model.predict(features_train[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(model.predict(features_train[:3]) == -1.0, 1.0, 0.0))\n",
    "print(np.where(model.predict(features_train[:3]) == 0.0, 1.0, 0.0))\n",
    "print(np.where(model.predict(features_train[:3]) == 1.0, 1.0, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "for threshold in thresholds:\n",
    "    print(\"Threshold \" + str(threshold) + \"\\n\")\n",
    "    y_pred_away = np.where(\n",
    "        model.predict_proba(features_train)[:, 0] >= threshold, 1.0, 0.0\n",
    "    )\n",
    "    y_pred_draw = np.where(\n",
    "        model.predict_proba(features_train)[:, 1] >= threshold, 1.0, 0.0\n",
    "    )\n",
    "    y_pred_home = np.where(\n",
    "        model.predict_proba(features_train)[:, 2] >= threshold, 1.0, 0.0\n",
    "    )\n",
    "    print(accuracy_score(np.where(target_train == -1.0, 1.0, 0.0), y_pred_away))\n",
    "    print(accuracy_score(np.where(target_train == 0.0, 1.0, 0.0), y_pred_draw))\n",
    "    print(accuracy_score(np.where(target_train == 1.0, 1.0, 0.0), y_pred_home))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    recall_score(\n",
    "        np.where(target_train == -1.0, 1.0, 0.0),\n",
    "        np.where(np.argmax(model.predict_proba(features_train), axis=1) == 0, 1.0, 0.0),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    recall_score(\n",
    "        np.where(target_train == 0.0, 1.0, 0.0),\n",
    "        np.where(np.argmax(model.predict_proba(features_train), axis=1) == 1, 1.0, 0.0),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    recall_score(\n",
    "        np.where(target_train == 1.0, 1.0, 0.0),\n",
    "        np.where(np.argmax(model.predict_proba(features_train), axis=1) == 2, 1.0, 0.0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "class_names = [\"Away team wins\", \"Draw\", \"Home team wins\"]\n",
    "for nclass_label, class_label in enumerate([-1.0, 0.0, 1.0]):\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        target_train, model.predict_proba(features_train)[:, 0], pos_label=class_label\n",
    "    )\n",
    "    plt.plot(precision[:-1], recall[:-1])\n",
    "    plt.title(\"Precision-Recall curve for \" + str(class_names[nclass_label]))\n",
    "    plt.xlabel(\"Precision\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    # plt.xlim(0.0,1.0)\n",
    "    # plt.ylim(0.0,1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "plt.title(\"Precision and Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to predict draws!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:"
   },
   "source": [
    "## Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bets as features and optimize the DT. What do you find? Can you assess feature importance? In particular, is it more important to see at \"in-game\" info or \"pre-game\" bets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Regression"
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how DTs can be used for regression with a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "# plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how DTs operate by exploring different depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.axvline(3.133, linestyle=\"dashed\", color=\"black\")\n",
    "plt.axhline(0.571, linestyle=\"dashed\", color=\"black\")\n",
    "plt.axhline(-0.667, linestyle=\"dashed\", color=\"black\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree decides on a predicted value by doing cuts in feature space. `max_depth` controls the number of cuts the algorithm makes. Let's see how the target is assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(np.mean(y) * np.ones(len(y)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(regr_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_first_cut = y[(X[:, 0] <= 3.133)]\n",
    "print(np.mean(y_first_cut), np.mean(y[(X[:, 0] > 3.133)]))\n",
    "print(mean_squared_error(np.mean(y_first_cut) * np.ones(len(y_first_cut)), y_first_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(regr_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(regr_1, out_file=\"reg_tree.dot\", rounded=True, filled=True)\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng reg_tree.dot -o reg_tree.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"reg_tree.png\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(regr_2, out_file=\"reg_tree.dot\", rounded=True, filled=True)\n",
    "\n",
    "# Convierto el dot a png\n",
    "! dot -Tpng reg_tree.dot -o reg_tree.png\n",
    "\n",
    "# Ploteamos el png\n",
    "img = cv2.imread(\"reg_tree.png\")\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the cut, it does not consider neither Gini nor entropy, it uses the MSE! Additionally, it assigns as predicted target the mean of all features before the cut is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise:"
   },
   "source": [
    "## Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the California dataset. Train a DT to predict the house price. Optimize the hyperparameter and report the RMSE and a predicted vs actual house value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_PATH = \"datasets\"\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from Geron\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import tarfile\n",
    "\n",
    "    DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/\"\n",
    "    HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "    !mkdir -p ./datasets/housing\n",
    "\n",
    "    def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "        os.makedirs(housing_path, exist_ok=True)\n",
    "        tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "        # urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "        !wget {HOUSING_URL} -P {housing_path}\n",
    "        housing_tgz = tarfile.open(tgz_path)\n",
    "        housing_tgz.extractall(path=housing_path)\n",
    "        housing_tgz.close()\n",
    "\n",
    "    # Corramos la funci\u00f3n\n",
    "    fetch_housing_data()\n",
    "\n",
    "else:\n",
    "    print(\"Not running on Google Colab. This cell is did not do anything.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_pre = load_housing_data()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing_pre[\"income_cat\"] = pd.cut(\n",
    "    housing_pre[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=445543)\n",
    "for train_index, test_index in split.split(housing_pre, housing_pre[\"income_cat\"]):\n",
    "    california_housing_train = housing_pre.loc[train_index]\n",
    "    california_housing_test = housing_pre.loc[test_index]\n",
    "\n",
    "for set_ in (california_housing_train, california_housing_test):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = california_housing_train.copy()\n",
    "\n",
    "problematic_columns = [\"median_house_value\", \"housing_median_age\", \"median_income\"]\n",
    "max_values = []\n",
    "for col in problematic_columns:\n",
    "    max_value = housing[col].max()\n",
    "    print(\n",
    "        f\"{col}: {sum(housing[col] == max_value)} districts with {col} = {max_value} ({round(sum(housing[col] == max_value)/len(housing)*100,2)}%).\"\n",
    "    )\n",
    "    max_values.append(max_value)\n",
    "\n",
    "housing_clean = housing.copy()\n",
    "for col, max_value in zip(problematic_columns, max_values):\n",
    "    housing_clean = housing_clean[housing_clean[col] != max_value]\n",
    "\n",
    "housing_test = california_housing_test.copy()\n",
    "housing_test_clean = housing_test.copy()\n",
    "for col, max_value in zip(problematic_columns, max_values):\n",
    "    housing_test_clean = housing_test_clean[housing_test_clean[col] != max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_clean[\"rooms_per_household\"] = (\n",
    "    housing_clean[\"total_rooms\"] / housing_clean[\"households\"]\n",
    ")\n",
    "housing_clean[\"bedrooms_per_room\"] = (\n",
    "    housing_clean[\"total_bedrooms\"] / housing_clean[\"total_rooms\"]\n",
    ")\n",
    "housing_clean[\"population_per_household\"] = (\n",
    "    housing_clean[\"population\"] / housing_clean[\"households\"]\n",
    ")\n",
    "\n",
    "housing_test_clean[\"rooms_per_household\"] = (\n",
    "    housing_test_clean[\"total_rooms\"] / housing_test_clean[\"households\"]\n",
    ")\n",
    "housing_test_clean[\"bedrooms_per_room\"] = (\n",
    "    housing_test_clean[\"total_bedrooms\"] / housing_test_clean[\"total_rooms\"]\n",
    ")\n",
    "housing_test_clean[\"population_per_household\"] = (\n",
    "    housing_test_clean[\"population\"] / housing_test_clean[\"households\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels = housing_clean[\"median_house_value\"].copy()\n",
    "housing_clean = housing_clean.drop(\n",
    "    \"median_house_value\", axis=1\n",
    ")  # drop labels for training set\n",
    "housing_num = housing_clean.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "housing_test_labels = housing_test_clean[\"median_house_value\"].copy()\n",
    "housing_test_clean = housing_test_clean.drop(\n",
    "    \"median_house_value\", axis=1\n",
    ")  # drop labels for training set\n",
    "housing_test_num = housing_test_clean.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),  # hay mas opciones aca\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ]\n",
    ")\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing_clean)\n",
    "housing_test_prepared = full_pipeline.transform(housing_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Another_nice_example"
   },
   "source": [
    "## Another nice example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is verbatim from `sklearn` documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.utils.validation import check_random_state\n",
    "\n",
    "# Load the faces datasets\n",
    "data, targets = fetch_olivetti_faces(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to predict the lower half of a face using the upper half:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[targets < 30]\n",
    "test = data[targets >= 30]  # Test on independent people\n",
    "\n",
    "# Test on a subset of people\n",
    "n_faces = 5\n",
    "rng = check_random_state(4)\n",
    "face_ids = rng.randint(test.shape[0], size=(n_faces,))\n",
    "test = test[face_ids, :]\n",
    "\n",
    "n_pixels = data.shape[1]\n",
    "# Upper half of the faces\n",
    "X_train = train[:, : (n_pixels + 1) // 2]\n",
    "# Lower half of the faces\n",
    "y_train = train[:, n_pixels // 2 :]\n",
    "X_test = test[:, : (n_pixels + 1) // 2]\n",
    "y_test = test[:, n_pixels // 2 :]\n",
    "\n",
    "# Fit estimators\n",
    "ESTIMATORS = {\n",
    "    \"Decision Trees\": DecisionTreeRegressor(),\n",
    "    \"Linear regression\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(),\n",
    "}\n",
    "\n",
    "y_test_predict = dict()\n",
    "for name, estimator in ESTIMATORS.items():\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_test_predict[name] = estimator.predict(X_test)\n",
    "\n",
    "# Plot the completed faces\n",
    "image_shape = (64, 64)\n",
    "\n",
    "n_cols = 1 + len(ESTIMATORS)\n",
    "plt.figure(figsize=(2.0 * n_cols, 2.26 * n_faces))\n",
    "plt.suptitle(\"Face completion with multi-output estimators\", size=16)\n",
    "\n",
    "for i in range(n_faces):\n",
    "    true_face = np.hstack((X_test[i], y_test[i]))\n",
    "\n",
    "    if i:\n",
    "        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\n",
    "    else:\n",
    "        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1, title=\"true faces\")\n",
    "\n",
    "    sub.axis(\"off\")\n",
    "    sub.imshow(\n",
    "        true_face.reshape(image_shape), cmap=plt.cm.gray, interpolation=\"nearest\"\n",
    "    )\n",
    "\n",
    "    for j, est in enumerate(sorted(ESTIMATORS)):\n",
    "        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\n",
    "\n",
    "        if i:\n",
    "            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\n",
    "\n",
    "        else:\n",
    "            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j, title=est)\n",
    "\n",
    "        sub.axis(\"off\")\n",
    "        sub.imshow(\n",
    "            completed_face.reshape(image_shape),\n",
    "            cmap=plt.cm.gray,\n",
    "            interpolation=\"nearest\",\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bagging_and_Random_Forests"
   },
   "source": [
    "## Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a particular type of **ensemble** training. Ensemble methods combine different estimators to build a better one, usually reducing the variance and overfitting. In bagging, which originates from **bootstrapping agreggating**, we bootstrap the data and train a model for each bootstrapped dataset. The overall model is then an average of the trained predictors.\n",
    "\n",
    "A **RandomForest** is a bagging model where the base estimator is a Decision Tree and where additionally **feature bagging** is performed. That is, at each decision step for each bootstrapped dataset, only a subset of features chosen at random is considered to select the optimal cut. This further increases the variability of the ensembled models. The added stochasticity can make the decision frontier more irregular, but usually increases performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this using an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a couple of useful functions (if in colab, otherwise, take from utils module)\n",
    "\n",
    "### From Rodrigo Diaz\n",
    "\n",
    "\n",
    "def plot_clasi(\n",
    "    x,\n",
    "    t,\n",
    "    ws,\n",
    "    labels=[],\n",
    "    xp=[-1.0, 1.0],\n",
    "    thr=[\n",
    "        0,\n",
    "    ],\n",
    "    spines=\"zero\",\n",
    "    equal=True,\n",
    "    join_centers=False,\n",
    "    margin=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Figura con el resultado del ajuste lineal\n",
    "    \"\"\"\n",
    "    assert len(labels) == len(ws) or len(labels) == 0\n",
    "    assert len(ws) == len(thr)\n",
    "\n",
    "    if margin is None:\n",
    "        margin = [False] * len(ws)\n",
    "    else:\n",
    "        margin = np.atleast_1d(margin)\n",
    "    assert len(margin) == len(ws)\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        labels = np.arange(len(ws)).astype(\"str\")\n",
    "\n",
    "    # Agregemos el vector al plot\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    xc1 = x[t == np.unique(t).max()]\n",
    "    xc2 = x[t == np.unique(t).min()]\n",
    "\n",
    "    ax.plot(*xc1.T, \"ob\", mfc=\"None\", label=\"C1\")\n",
    "    ax.plot(*xc2.T, \"or\", mfc=\"None\", label=\"C2\")\n",
    "\n",
    "    for i, w in enumerate(ws):\n",
    "        # Compute vector norm\n",
    "        wnorm = np.sqrt(np.sum(w**2))\n",
    "\n",
    "        # Ploteo vector de pesos\n",
    "        x0 = 0.5 * (xp[0] + xp[1])\n",
    "        ax.quiver(\n",
    "            0,\n",
    "            thr[i] / w[1],\n",
    "            w[0] / wnorm,\n",
    "            w[1] / wnorm,\n",
    "            color=\"C{}\".format(i + 2),\n",
    "            scale=10,\n",
    "            label=labels[i],\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "        # ploteo plano perpendicular\n",
    "        xp = np.array(xp)\n",
    "        yp = (thr[i] - w[0] * xp) / w[1]\n",
    "\n",
    "        plt.plot(xp, yp, \"-\", color=\"C{}\".format(i + 2))\n",
    "\n",
    "        # Plot margin\n",
    "        if margin[i]:\n",
    "            for marg in [-1, 1]:\n",
    "                ym = yp + marg / w[1]\n",
    "                plt.plot(xp, ym, \":\", color=\"C{}\".format(i + 2))\n",
    "\n",
    "    if join_centers:\n",
    "        # Ploteo l\u00ednea que une centros de los conjuntos\n",
    "        mu1 = xc1.mean(axis=1)\n",
    "        mu2 = xc2.mean(axis=1)\n",
    "        ax.plot([mu1[0], mu2[0]], [mu1[1], mu2[1]], \"o:k\", mfc=\"None\", ms=10)\n",
    "\n",
    "    ax.legend(loc=0, fontsize=12)\n",
    "    if equal:\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "    if spines is not None:\n",
    "        for a in [\"left\", \"bottom\"]:\n",
    "            ax.spines[a].set_position(\"zero\")\n",
    "        for a in [\"top\", \"right\"]:\n",
    "            ax.spines[a].set_visible(False)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def makew(fitter):\n",
    "    # # Obtengamos los pesos y normalicemos\n",
    "    w = fitter.coef_.copy()\n",
    "\n",
    "    # # Incluye intercept\n",
    "    if fitter.fit_intercept:\n",
    "        w = np.hstack([fitter.intercept_.reshape(1, 1), w])\n",
    "\n",
    "    # # Normalizon\n",
    "    # w /= np.linalg.norm(w)\n",
    "    return w.T\n",
    "\n",
    "\n",
    "# Utility from Geron\n",
    "def plot_decision_regions(\n",
    "    clf,\n",
    "    X,\n",
    "    t,\n",
    "    axes=None,\n",
    "    npointsgrid=500,\n",
    "    legend=False,\n",
    "    plot_training=True,\n",
    "    figkwargs={\"figsize\": [12, 8]},\n",
    "    contourkwargs={\"alpha\": 0.3},\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot decision regions produced by classifier.\n",
    "\n",
    "    :param Classifier clf: sklearn classifier supporting XXX\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(**figkwargs)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if axes is None:\n",
    "        dx = X[:, 0].max() - X[:, 0].min()\n",
    "        dy = X[:, 1].max() - X[:, 1].min()\n",
    "        axes = [\n",
    "            X[:, 0].min() - 0.1 * dx,\n",
    "            X[:, 0].max() + 0.1 * dx,\n",
    "            X[:, 1].min() - 0.1 * dy,\n",
    "            X[:, 1].max() + 0.1 * dy,\n",
    "        ]\n",
    "\n",
    "    # Define grid for regions\n",
    "    x1s = np.linspace(axes[0], axes[1], npointsgrid)\n",
    "    x2s = np.linspace(axes[2], axes[3], npointsgrid)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "    # Make predictions on points of grid; reshape to grid format\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "\n",
    "    # custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    ax.contourf(x1, x2, y_pred, **contourkwargs)\n",
    "\n",
    "    #     custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "    #         plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "\n",
    "    if plot_training:\n",
    "        for label in np.unique(t):\n",
    "            ax.plot(\n",
    "                X[:, 0][t == label], X[:, 1][t == label], \"o\", label=\"C{}\".format(label)\n",
    "            )\n",
    "\n",
    "    # Axis\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Example_with_Moons_dataset"
   },
   "source": [
    "## Example with Moons dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple non-linearly separable dataset to exemplify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, t = make_moons(n_samples=400, noise=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, [], [], [], [], spines=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X, X_test, t, t_test = train_test_split(X, t, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Simple_RF_training"
   },
   "source": [
    "## Simple RF training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2, n_jobs=6)\n",
    "rf.fit(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_decision_regions(\n",
    "    rf,\n",
    "    X,\n",
    "    t,\n",
    "    legend=True,\n",
    "    npointsgrid=500,\n",
    "    figkwargs={\"figsize\": [12, 8]},\n",
    "    contourkwargs={\"alpha\": 0.5, \"levels\": 5, \"cmap\": \"viridis\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will not generalize well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train = rf.predict(X)\n",
    "y_test = rf.predict(X_test)\n",
    "print(\"Accuracy (train): {:.3f}\".format(accuracy_score(t, y_train)))\n",
    "print(\"Accuracy (test): {:.3f}\".format(accuracy_score(t_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can solve this by optimizing the Random Forest using `GridSearchCV`.\n",
    "\n",
    "Another feature of RFs is their interpretability. Since it's based on a white box algorithm, Decision Trees, we can use to study the learned properties. In particular, we can gauge feature importance by inspecting the fitted DTs. For a given DT, the most important features are closer to the root. We can perform statistics on the feature importances by averaging over the fitted DTs.\n",
    "\n",
    "`sklearn` stores this through `feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.feature_importances_)\n",
    "for name, score in zip([\"x_1\", \"x_2\"], rf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an optimize Random Forest by exploring the possible hyperparameters. Compare with a simpler classifier like an optimized polynomial Logistic Regressor or a optimized Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Boosted_and_Boosted_Decision_Trees"
   },
   "source": [
    "## Boosted and Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting methods are another example of **ensemble** methods. They also combine different instances of a base estimator. However, in boosting each successive instance learns both from the data and from the previous estimator. That is, it learns to \"correct\" the previous estimator.\n",
    "\n",
    "* It usually **greatly improves** the performance of **weak predictors**.\n",
    "* It's not easily paralellizable.\n",
    "* It's greedy. Each step seeks to be as good as possible without thinking of global strategies.\n",
    "\n",
    "\n",
    "We'll see two types of boosting: AdaBoosting and GradientBoosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdaBoost"
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AdaBoost, at each step the data points are weighted according to the performance of the previous estimator (they are initiated to 1)\n",
    "\n",
    "That is, for steps $i=1,\\dots,N$\n",
    "1. We train a predictor $h_i$.\n",
    "2. We update the per sample weight $w_{n,i}=f(w_{n,i-1},h_{i})$\n",
    "The final predictor combines all $N$ predictors.\n",
    "\n",
    "The two `sklearn` classes are `AdaBoostClassifier` and `AdaBoostRegressor`, with algorithm specific hyperparameters:\n",
    "\n",
    "The AdaBoost-specific hyperparameters are:\n",
    "\n",
    "* `estimator`: The weak predictor used. By default, it is a `DecisionStump` (a `DecisionTree` with `max_depth=1`).\n",
    "* `n_estimators`: How many estimators to use.\n",
    "* `learning_rate`: The learning rate when taking a new estimator. The lower the learning_rate, the more estimators are needed to fit the data. This is a regularizer for the algorithm.\n",
    "* `loss`: Exclusively for regression. This is the loss function used by the algorithm. The options are `linear`, `square`, and `exponential`.\n",
    "\n",
    "From the fitted class, you can obtain:\n",
    "\n",
    "* `estimators_`: The list of estimators.\n",
    "* `estimators_weights_`: The weights of each estimator. 1 for `SAMME.R` classification, not equal to 1 for regression and classification with `SAMME`.\n",
    "* `estimators_errors_`: The error of each estimator when evaluated on the dataset. This is not the error when applying the ensemble.\n",
    "* `feature_importances_`: The importance of the features.\n",
    "\n",
    "In addition, AdaBoost has the `.staged_` function that allows the ensemble to be evaluated at each step as if it were complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Example"
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True\n",
    "):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap([\"#7d7d58\", \"#4c4c7f\", \"#507d50\"])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV = RidgeCV()\n",
    "ridgeCV.fit(X_train, y_train, sample_weight=np.where(X_train[:, 1] > -0.5, 100.0, 1.0))\n",
    "\n",
    "plot_decision_boundary(ridgeCV, X, y)\n",
    "# plt.axhline(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 300\n",
    "# AdaBoostClassifier(base_estimator=SVC/DT/Perceptron/RL,n_estimator= cuantos voy a considerar, algorithm=que algoritmo uso, learning_rate = ,...)\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=n_estimators,\n",
    "    algorithm=\"SAMME\",\n",
    "    learning_rate=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "plot_decision_boundary(ada_clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "preds = cross_val_predict(ada_clf, X_train, y_train)\n",
    "cm = confusion_matrix(y_train, preds)  # ,ada_clf.predict(X_train))\n",
    "print(cm)\n",
    "print(accuracy_score(y_train, preds))  # ada_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the individual estimators, their weights and loss function, computed as\n",
    "\n",
    "$$ \\text{Loss} = \\sum_{i}w_{i}\\text{Loss}_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(ada_clf.estimators_).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(ada_clf.estimators_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ada_clf.estimator_weights_.shape)\n",
    "plt.plot(ada_clf.estimator_weights_, \"r.\")\n",
    "plt.xlabel(\"Estimator\")\n",
    "plt.ylabel(r\"Weight $\\alpha$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ada_clf.estimator_errors_.shape)\n",
    "plt.plot(ada_clf.estimator_errors_, \"ro\")\n",
    "plt.xlabel(\"Estimator\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore explicitly the evolution as we add estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nest, est_pred in enumerate(ada_clf.staged_predict(X_train[:2])):\n",
    "    print(nest, est_pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss  # counts the misclassified fraction\n",
    "\n",
    "err_train = np.zeros((n_estimators, 2))\n",
    "for i, y_pred in enumerate(ada_clf.staged_predict(X_train)):\n",
    "    err_train[i, 0] = zero_one_loss(y_pred, y_train)\n",
    "    err_train[i, 1] = accuracy_score(y_pred, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ax[0].plot(np.arange(n_estimators) + 1, err_train[:, 0])\n",
    "ax[1].plot(np.arange(n_estimators) + 1, err_train[:, 1])\n",
    "\n",
    "ax[0].set_xlabel(\"# Estimators\")\n",
    "ax[1].set_xlabel(\"# Estimators\")\n",
    "ax[0].set_ylabel(\"Zero One Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Learning_rate_effect_in_convergence"
   },
   "source": [
    "## Learning rate effect in convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice example from Geron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "m = len(X_train)\n",
    "\n",
    "fix, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
    "    sample_weights = np.ones(m)\n",
    "    plt.sca(axes[subplot])\n",
    "    for i in range(5):\n",
    "        svm_clf = SVC(kernel=\"rbf\", C=0.05, gamma=\"scale\", random_state=42)\n",
    "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        y_pred = svm_clf.predict(X_train)\n",
    "        sample_weights[y_pred != y_train] *= 1 + learning_rate\n",
    "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
    "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
    "    if subplot == 0:\n",
    "        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n",
    "        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n",
    "        plt.text(-0.5, 0.10, \"3\", fontsize=14)\n",
    "        plt.text(-0.4, 0.55, \"4\", fontsize=14)\n",
    "        plt.text(-0.3, 0.90, \"5\", fontsize=14)\n",
    "    else:\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for Decision Trees and see the overall evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "\n",
    "learnings = [1.0, 0.5]\n",
    "fix, axes = plt.subplots(\n",
    "    nrows=5, ncols=len(learnings), figsize=(5 * len(learnings), 25), sharey=True\n",
    ")\n",
    "for subplot, learning_rate in enumerate(learnings):\n",
    "    ada_clf = AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=5,\n",
    "        algorithm=\"SAMME\",\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=42,\n",
    "    )\n",
    "    ada_clf.fit(X_train, y_train)\n",
    "    y_pred_train = np.zeros((5, X_train.shape[0]))\n",
    "    for nest_train, est_dec_train in enumerate(ada_clf.staged_predict(X_train)):\n",
    "        y_pred_train[nest_train] = est_dec_train\n",
    "    # axes=[-1.5, 2.45, -1, 1.5]\n",
    "    alpha = 0.5\n",
    "    x1s = np.linspace(-1.5, 2.45, 100)\n",
    "    x2s = np.linspace(-1, 1.5, 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    for nest, est_dec in enumerate(ada_clf.staged_predict(X_new)):\n",
    "        y_pred_estimator_only = (\n",
    "            ada_clf.estimators_[nest].predict(X_new).reshape(x1.shape)\n",
    "        )\n",
    "        y_pred = est_dec.reshape(x1.shape)\n",
    "        custom_cmap2 = ListedColormap([\"#7d7d58\", \"#4c4c7f\", \"#507d50\"])\n",
    "        axes[nest, subplot].plot(\n",
    "            X_train[:, 0][y_train == 0], X_train[:, 1][y_train == 0], \"yo\", alpha=alpha\n",
    "        )\n",
    "        axes[nest, subplot].plot(\n",
    "            X_train[:, 0][y_train == 1], X_train[:, 1][y_train == 1], \"bs\", alpha=alpha\n",
    "        )\n",
    "        axes[nest, subplot].plot(\n",
    "            X_train[:, 0][y_pred_train[nest] != y_train],\n",
    "            X_train[:, 1][y_pred_train[nest] != y_train],\n",
    "            \"rx\",\n",
    "            alpha=1.0,\n",
    "        )\n",
    "        axes[nest, 0].set_ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "        axes[nest, subplot].contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "        # axes[nest,subplot].contour(x1, x2, y_pred_estimator_only, cmap='plasma', alpha=0.8)\n",
    "        axes[nest, subplot].set_title(\n",
    "            \"learning_rate = {}, estimator ={}\".format(learning_rate, nest + 1),\n",
    "            fontsize=16,\n",
    "        )\n",
    "    #      plt.show()\n",
    "    axes[-1, subplot].set_xlabel(r\"$x_1$\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the individual cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "\n",
    "learnings = [1.0, 0.5]\n",
    "fix, axes = plt.subplots(\n",
    "    nrows=5, ncols=len(learnings), figsize=(5 * len(learnings), 25), sharey=True\n",
    ")\n",
    "for subplot, learning_rate in enumerate(learnings):\n",
    "    ada_clf = AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=5,\n",
    "        algorithm=\"SAMME\",\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=42,\n",
    "    )\n",
    "    ada_clf.fit(X_train, y_train)\n",
    "    y_pred_train = np.zeros((5, X_train.shape[0]))\n",
    "    for nest_train, est_dec_train in enumerate(ada_clf.staged_predict(X_train)):\n",
    "        y_pred_train[nest_train] = est_dec_train\n",
    "    # axes=[-1.5, 2.45, -1, 1.5]\n",
    "    alpha = 0.5\n",
    "    x1s = np.linspace(-1.5, 2.45, 100)\n",
    "    x2s = np.linspace(-1, 1.5, 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    for nest, est_dec in enumerate(ada_clf.estimators_):\n",
    "        y_pred = est_dec.predict(X_new).reshape(x1.shape)\n",
    "        custom_cmap2 = ListedColormap([\"#7d7d58\", \"#4c4c7f\", \"#507d50\"])\n",
    "        axes[nest, subplot].plot(\n",
    "            X_train[:, 0][y_train == 0], X_train[:, 1][y_train == 0], \"yo\", alpha=alpha\n",
    "        )\n",
    "        axes[nest, subplot].plot(\n",
    "            X_train[:, 0][y_train == 1], X_train[:, 1][y_train == 1], \"bs\", alpha=alpha\n",
    "        )\n",
    "        axes[nest, subplot].plot(\n",
    "            X_train[:, 0][y_pred_train[nest] != y_train],\n",
    "            X_train[:, 1][y_pred_train[nest] != y_train],\n",
    "            \"rx\",\n",
    "            alpha=1.0,\n",
    "        )\n",
    "        axes[nest, 0].set_ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "        axes[nest, subplot].contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "        axes[nest, subplot].set_title(\n",
    "            \"learning_rate = {}, estimator ={}\".format(learning_rate, nest + 1),\n",
    "            fontsize=16,\n",
    "        )\n",
    "    #      plt.show()\n",
    "    axes[-1, subplot].set_xlabel(r\"$x_1$\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Regression_example"
   },
   "source": [
    "## Regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how to use `AdaBoostRegressor`. It's very similar, you only need to specify the `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(max_depth=4),\n",
    "    loss=\"square\",\n",
    "    n_estimators=300,\n",
    "    random_state=rng,\n",
    ")\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
    "plt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
    "plt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Boosted Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr_2.estimator_weights_.shape)\n",
    "plt.plot(regr_2.estimator_weights_, \"ro\")\n",
    "plt.xlabel(\"Iteracion\")\n",
    "plt.ylabel(\"Peso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr_2.estimator_errors_.shape)\n",
    "plt.plot(regr_2.estimator_errors_, \"ro\")\n",
    "plt.xlabel(\"Iteracion\")\n",
    "plt.ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GradientBoosting"
   },
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting follows a different iterative procedure than AdaBoost.\n",
    "\n",
    "Instead of correcting based on weights, Gradient Boosting improves by effectively training each estimator on the residuals of the previous estimators. For predictors  $h_{m}$ with $m=1,\\dots,M$\n",
    "\n",
    "$$\\hat{y}^{m}_{n} = F_{m}(x_{n})$$\n",
    "\n",
    "with $F$ built from the collection of all $m$ estimators in an iterative way\n",
    "\n",
    "$$F_{m}(x)=F_{m-1}(x)+h_{m}(x)=\\sum_{m'=1}^{m}h_{m'}(x)$$\n",
    "\n",
    "Thus, $h_{m}$ is learned by optimizing\n",
    "\n",
    "$$h_{m}=\\text{arg }\\min_{h} \\mathcal{L}_{m}(h) = \\text{arg }\\min_{h} \\sum_{n=1}^{N}\\mathcal{l}(y_{n},F_{m-1}(x_{n})+h(x_{n}))$$\n",
    "\n",
    "which can be efficiently approximated via a linear expansion on $h$\n",
    "\n",
    "$$\\mathcal{l}(y_{n},F_{m-1}(x_{n})+h(x_{n})) \\approx \\mathcal{L}(y_{n},F_{m-1}(x_{n}))+h(x_{n})\\frac{\\partial \\mathcal{l}(y_{n},F(x_{n}))}{\\partial F}|_{F=F_{m-1}}$$\n",
    "\n",
    "and we have that\n",
    "\n",
    "$$h_{m}=\\text{arg }\\min_{h}\\sum_{n=1}^{N}h(x_{n})\\frac{\\partial \\mathcal{l}(y_{n},F(x_{n}))}{\\partial F}|_{F=F_{m-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An_example"
   },
   "source": [
    "## An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Geron, we can get a nice qualitative picture of how GradientBoosting works (it's not exactly this, but it's similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)  # first estimator residuals\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)  # second estimator residuals\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can predict by aggregating the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(\n",
    "    tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)\n",
    ")  # sum of predictions from all trees\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None\n",
    "):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_predictions(\n",
    "    [tree_reg1],\n",
    "    X,\n",
    "    y,\n",
    "    axes=[-0.5, 0.5, -0.1, 0.8],\n",
    "    label=\"$h_1(x_1)$\",\n",
    "    style=\"g-\",\n",
    "    data_label=\"Training set\",\n",
    ")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_predictions(\n",
    "    [tree_reg1],\n",
    "    X,\n",
    "    y,\n",
    "    axes=[-0.5, 0.5, -0.1, 0.8],\n",
    "    label=\"$h(x_1) = h_1(x_1)$\",\n",
    "    data_label=\"Training set\",\n",
    ")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Ensemble predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_predictions(\n",
    "    [tree_reg2],\n",
    "    X,\n",
    "    y2,\n",
    "    axes=[-0.5, 0.5, -0.5, 0.5],\n",
    "    label=\"$h_2(x_1)$\",\n",
    "    style=\"g-\",\n",
    "    data_style=\"k+\",\n",
    "    data_label=\"Residuals\",\n",
    ")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_predictions(\n",
    "    [tree_reg1, tree_reg2],\n",
    "    X,\n",
    "    y,\n",
    "    axes=[-0.5, 0.5, -0.1, 0.8],\n",
    "    label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\",\n",
    ")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_predictions(\n",
    "    [tree_reg3],\n",
    "    X,\n",
    "    y3,\n",
    "    axes=[-0.5, 0.5, -0.5, 0.5],\n",
    "    label=\"$h_3(x_1)$\",\n",
    "    style=\"g-\",\n",
    "    data_style=\"k+\",\n",
    ")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_predictions(\n",
    "    [tree_reg1, tree_reg2, tree_reg3],\n",
    "    X,\n",
    "    y,\n",
    "    axes=[-0.5, 0.5, -0.1, 0.8],\n",
    "    label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\",\n",
    ")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "`sklearn`_implementation"
   },
   "source": [
    "## `sklearn` implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes are GradientBoostingClassifier y GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=50, learning_rate=1.0, random_state=42\n",
    ")\n",
    "gbrt.fit(X, y)\n",
    "\n",
    "gbrt_slow = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=50, learning_rate=0.1, random_state=42\n",
    ")\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(\n",
    "    [gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\"\n",
    ")\n",
    "plt.title(\n",
    "    \"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators),\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\n",
    "    \"learning_rate={}, n_estimators={}\".format(\n",
    "        gbrt_slow.learning_rate, gbrt_slow.n_estimators\n",
    "    ),\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Optimal_number_of_trees"
   },
   "source": [
    "## Optimal number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of estimators needs to be optimized. Too few, we underfit. Too many, we overfit. A nice way to find the optimal number of trees is by implementing an **early stopping** algorithm, which evaluates the predictor on a validation dataset to assess performance. Once the validation metric worsens, we can stop and get back to the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [\n",
    "    np.sqrt(mean_squared_error(y_val, y_pred)) for y_pred in gbrt.staged_predict(X_val)\n",
    "]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=bst_n_estimators, random_state=42\n",
    ")\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bst_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = np.min(errors)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error * 1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([40, 120, 0, 0.1])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Error\", fontsize=16)\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, **early stopping** avoids overfitting (to a certain degree). However, in the code above we're still training all possible estimators. A realistic implementation usually stops once the metric worsens to avoid wasteful compute. We can do this through the `warm_start` option, which stores all trees trained during `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbrt.n_estimators)\n",
    "print(\"Minimo MSE en el conjunto de validacion:\", min_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    n_estimators=120,\n",
    "    warm_start=True,\n",
    "    random_state=42,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=5,\n",
    ")\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt.n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stochastic_gradient_boosting"
   },
   "source": [
    "## Stochastic gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an additional parameter called `subsample` which is very useful. It defines whether we use all possible data or if we consider a randomly chosen subset at each step, which usually accelerates training by lowering the variance of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_all = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=100, learning_rate=1.0, random_state=42\n",
    ")\n",
    "gbrt_all.fit(X, y)\n",
    "\n",
    "gbrt_stochastic = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=100, learning_rate=1.0, subsample=0.5, random_state=42\n",
    ")\n",
    "gbrt_stochastic.fit(X, y)\n",
    "\n",
    "fix, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(\n",
    "    [gbrt_all], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\"\n",
    ")\n",
    "plt.title(\n",
    "    \"subsample={}, n_estimators={}\".format(gbrt_all.subsample, gbrt_all.n_estimators),\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions([gbrt_stochastic], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\n",
    "    \"subsample={}, n_estimators={}\".format(\n",
    "        gbrt_stochastic.subsample, gbrt_stochastic.n_estimators\n",
    "    ),\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGBoost"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although useful, the `sklearn` implementation is not the most powerful available.\n",
    "\n",
    "One possible choice is to use **Extreme Gradient Boosting**, or `XGBoost`, which is an optimized implementation that prioritizes speed, scalability and portability. It is hugely popular (as can be seen in Kaggle) and can be used in a similar manner as `sklearn` (by design, they can be used together fairly easily). In particular, the `XGBRegressor` and `XGBClassifier` classes are built to be equivalent to `sklearn` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the relevant document [here](https://xgboost.readthedocs.io/en/latest/).\n",
    "\n",
    "The relevant hyperparameters for us are:\n",
    "\n",
    "- `learning rate` (1 by default)\n",
    "- `gamma` / `min_split_loss` (0 by default): the minimum loss reduction for the tree to continue splitting a leaf\n",
    "- `max_depth` (6 by default)\n",
    "- `min_child_weight` (1 by default): the minimum number of weighted measurements that must remain in a child when splitting a leaf node\n",
    "- `subsample` (1 by default)\n",
    "- `colsample_bytree`, `colsample_bylevel`, `colsample_bynode` (1 by default for all three): the fraction of features considered per tree, per level, and per node.\n",
    "- `reg_lambda` (1 by default): L2 penalty factor in the weights\n",
    "- `reg_alpha` (0 by default): L1 penalty factor in the weights\n",
    "- `objective`: specifies the task to be performed. 'reg:squarederror' is the least squares loss. 'binary:logistic' or 'multi:softmax' are useful for classification with probabilistic outputs. There are several other options to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "print(X.head())\n",
    "print(X.info())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train_2, X_val, y_train_2, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.5,\n",
    "    reg_lambda=0.0,\n",
    "    reg_alpha=0.0,\n",
    "    gamma=0.0,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=5,\n",
    "    objective=\"reg:squarederror\",\n",
    "    max_depth=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train using `fit`, with some hyperparameters set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(\n",
    "    X_train_2,\n",
    "    y_train_2,\n",
    "    eval_set=[(X_train_2, y_train_2), (X_val, y_val)],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(regressor.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.evals_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(iris.feature_names)):\n",
    "    print((iris.feature_names[i], regressor.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's so fast, we can even do `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scores = cross_val_score(\n",
    "#    regressor, X_train, y_train, scoring=\"neg_root_mean_squared_error\"\n",
    "# )\n",
    "# print(-scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.save_model(\"xbg_modelo_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = regressor.get_xgb_params()\n",
    "regressor_2 = XGBRegressor(**params)\n",
    "regressor_2.get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_2.load_model(\"xbg_modelo_1.json\")\n",
    "regressor_2.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.predict(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_2.predict(X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exercise"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the LHC we can look for new particles. One possibility are $W^\\prime$, which may decay to different final states. For example, a proton-proton collision may produce a very massive particle that decays to two jets, which we rank by transverse momentum $P_{T}$ and call *leading* and *submladling* jets. Each of these jets is characterized by seven parameters: its invariant mass ($M_j$), its transverse momentum ($P_T$), its relativistic rapidity ($Y$), its azimuthal angle ($\\phi$), and three variables ($\\tau_{21}, \\tau_{31}, \\tau_{32}$), which measure the substructure of each jet.\n",
    "\n",
    "We have a dataset of 10000 simulated collisions where this new particle $W^\\prime$, which we call *signal*, is actually produced, and another 10000 whose collisions does not result in the creation of this particle but instead originate from many SM model processes which constitute the irreducible *background* of the search.\n",
    "\n",
    "The goal is to train a classifer based on the jets features to differentiate signal and background events. This classifier can be used as a tagger to select interesting events or even be used to build an optimal observable for statistical inference (based on the Neyman-Pearson lemma).\n",
    "\n",
    "The following cells import the data and visualize them. Explore the dataset and train an optimized tagger using cross-validation. First train a classifier using either the leading or the sub-leading jets, then both. Get the feature importances and report all relevant metrics. Compare to a simpler classifier and decide whether BDT were worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -N https://gitlab.com/mcgen-ct/tutorials/-/raw/2025-cteq/.full/ml/datasets/np_background.dat\n",
    "!wget -q -N https://gitlab.com/mcgen-ct/tutorials/-/raw/2025-cteq/.full/ml/datasets/np_signal.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "background = []\n",
    "# reads background events\n",
    "with open(\"np_background.dat\") as backgroundfile:\n",
    "    for nline, line in enumerate(backgroundfile):\n",
    "        if nline < 10000:\n",
    "            Line = line.split(\";\")\n",
    "            # separates the leading jet data from the sub-leading jet data, transforms them into float\n",
    "            # and constructs an array of dimensions [10000, 2, 7] (event, jet, feature)\n",
    "            background_1 = list(map(lambda x: float(x), Line[0].split(\",\")))\n",
    "            background_2 = list(map(lambda x: float(x), Line[1].split(\",\")))\n",
    "            background.append([background_1, background_2])\n",
    "\n",
    "background = np.asarray(background)\n",
    "\n",
    "# Does the same for the signal data.\n",
    "signal = []\n",
    "with open(\"np_signal.dat\") as signalfile:\n",
    "    for nline, line in enumerate(signalfile):\n",
    "        if nline < 10000:\n",
    "            Line = line.split(\";\")\n",
    "            signal_1 = list(map(lambda x: float(x), Line[0].split(\",\")))\n",
    "            signal_2 = list(map(lambda x: float(x), Line[1].split(\",\")))\n",
    "            signal.append([signal_1, signal_2])\n",
    "signal = np.asarray(signal)\n",
    "\n",
    "print(\"Shape of background and signal:\", background.shape, signal.shape)\n",
    "\n",
    "# group both datasets and assign labels, 0 for background and 1 for signal\n",
    "X = np.vstack((background, signal))\n",
    "Y = np.hstack((np.zeros(len(background)), np.ones(len(signal))))\n",
    "\n",
    "print(\"Shapes of data and labels:\", X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vars = [\"$M_j$\", \"$P_T$\", \"Y\", \"$\\phi$\", r\"$\\tau_{21}$\", r\"$\\tau_{31}$\", r\"$\\tau_{32}$\"]\n",
    "\n",
    "# Let's plot the distributions of the variables for both leading and sub-leading jets and for each process.\n",
    "for i in range(7):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    axs[0].hist(background[:, 0, i], histtype=\"step\", color=\"blue\", label=\"Background\")\n",
    "    axs[0].hist(signal[:, 0, i], histtype=\"step\", color=\"red\", label=\"Signal\")\n",
    "    axs[0].legend(loc=\"upper right\")\n",
    "    axs[0].set_title(vars[i] + \" Leading Jet\")\n",
    "    axs[1].hist(background[:, 1, i], histtype=\"step\", color=\"blue\", label=\"Background\")\n",
    "    axs[1].hist(signal[:, 1, i], histtype=\"step\", color=\"red\", label=\"Signal\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(vars[i] + \" Sub-Leading Jet\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's study the correlations between all other variables and the jet mass for both leading and sub-leading jets.\n",
    "for i in range(6):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 3))\n",
    "    f1 = axs[0].hist2d(background[:, 0, 0], background[:, 0, 1 + i], cmap=\"gist_heat_r\")\n",
    "    fig.colorbar(f1[3], ax=axs[0])\n",
    "    axs[0].set_xlabel(vars[0])\n",
    "    axs[0].set_ylabel(vars[1 + i])\n",
    "    axs[0].set_title(\"Background Leading Jet\")\n",
    "    f2 = axs[1].hist2d(signal[:, 0, 0], signal[:, 0, 1 + i], cmap=\"gist_heat_r\")\n",
    "    fig.colorbar(f2[3], ax=axs[1])\n",
    "    axs[1].set_xlabel(vars[0])\n",
    "    # axs[1].set_ylabel(vars[1+i])\n",
    "    axs[1].set_title(\"Signal Leading Jet\")\n",
    "    f3 = axs[2].hist2d(background[:, 1, 0], background[:, 1, 1 + i], cmap=\"gist_heat_r\")\n",
    "    fig.colorbar(f3[3], ax=axs[2])\n",
    "    axs[2].set_xlabel(vars[0])\n",
    "    axs[2].set_title(\"Background Sub-Leading Jet\")\n",
    "    f4 = axs[3].hist2d(signal[:, 1, 0], signal[:, 1, 1 + i], cmap=\"gist_heat_r\")\n",
    "    axs[3].set_xlabel(vars[0])\n",
    "    axs[3].set_title(\"Signal Sub-Leading Jet\")\n",
    "    fig.colorbar(f4[3], ax=axs[3])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Theory",
    "Classification_with_Decision_Trees",
    "Exercise:",
    "Regression",
    "Exercise:",
    "Another_nice_example",
    "Bagging_and_Random_Forests",
    "Example_with_Moons_dataset",
    "Simple_RF_training",
    "Exercise",
    "Boosted_and_Boosted_Decision_Trees",
    "AdaBoost",
    "Example",
    "Learning_rate_effect_in_convergence",
    "Regression_example",
    "GradientBoosting",
    "An_example",
    "`sklearn`_implementation",
    "Optimal_number_of_trees",
    "Stochastic_gradient_boosting",
    "XGBoost",
    "Exercise"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}